[TOC]



## 1. 소프트웨어 세상의 세 가지 혁명



1. 클라우드 혁명
2. 데브옵스의 탄생
3. 컨테이너



* => 3가지 혁명의 결과로, *클라우드 네이티브* 소프트웨어 환경이 만들어짐



### 1. 클라우드 혁명

* "클라우드"

  1. 물리적인 컴퓨터 자원을 구매하는 것 대신, *컴퓨팅 리소스*를 구매하는 방식

  *  => 원격 컴퓨팅 파워가 중요해진 시대

  2. 구축과 업그레이드가 필요하지 않은 IaaS( Infra as a Service )

  * 클라우드 인프라의 상품화
  * => 개발과 운영의 통합성을 불러옴



### 2. 데브옵스

> 소프트웨어의 개발(Development)과 운영(Operations)의 합성어
>
> * 소프트웨어 개발자와 운영조직 간의 소통, 협업 및 통합을 강조하는 개발 환경이나 문화 ( 상호 의존적 대응 )
> * 소프트웨어 제품과 서비스를 빠른 시간에 개발 및 배포하는 것을 목적으로 한다.



* 과거
  * 개발자는 소프트웨어 작성
  * 운영자는 상용환경에서 소프트웨어를 실행 및 관리
  * => 중복되는 업무가 거의 없었다!
  * '시스템' = 개인이 개발한 소프트웨어



* 클라우드 혁명 이후..?
  * 복잡한 분산 시스템
  * 상호의존적인 클라우드 시스템
  * '시스템' = 밀접하게 연결되고, 상호의존적인 다양한 소프트웨어..
    * 예 : 사내 소프트웨어, 클라우드 서비스, 네트워크 리소스, 로드 밸런서, 모니터링...



* 데브옵스에 대한 정의는 제각각
  * 중요한 포인트는, 데브옵스는 기술적인 문제 보다는 주로 조직적인 문화와 더 관련이 깊다는 것이다.



* 비즈니스 이점
  * 데브옵스의 장점?
    * 클라우드 자동화와 실행으로 릴리스 주기를 단축
    * => 소프트 웨어의 품질 향상 + 소프트웨어가 상용 환경에서 상시 운영!
  * 데브옵스의 핵심 목표
    * 속도, 민첩성, 협업, 자동화, 소프트웨어 품질



* 코드형 인프라

  * IAC ( Infastructure As Code )
    * 클라우드 환경에서 하드웨어는 '클라우드'에 존재
    * => 어떤 의미로 본다면, 모든 것이 소프트웨어
    * => 소프트웨어 형식으로 인프라를 자동 공급

  

* 정리
  * 클라우드 + 협업의 필요성 + 인프라의 소프트웨어화
  * => 소프트웨어의 운영 문제 
    * 소프트웨어를 배포하는 방법 중 이식성이 높은 방법은 무엇이 있을까?



### 3. 컨테이너 등장

* 소프트웨어를 배포하려면?
  * 소프트웨어 + 의존성( 소프트웨어와 관련된 모든 것 ) + 구성( 소프트웨어를 사용 가능한 서비스로 만들어주는 것 )
    * 의존성 : 라이브러리, 인터프리터, 컴파일러,,
    * 구성 : 설정, 라이선스 키, 데이터베이스 패스워드



* 배포 과정의 변화

  1. 코드 구성 관리 시스템

  * 퍼핏, 앤서블 ...
  * 소프트웨어를 설치, 실행, 구성, 업데이트
    * 단점 : 언어에 종속되며, 의존성 문제 완전히 해결 불가

  2. 옴니버스 패키지

  * 애플리케이션에 필요한 모든 것을 *단일 파일*로 구성
    * 소프트웨어, 구성, 소프트웨어 컴포넌트, 컴포넌트의 구성, 컴포넌트의 의존성..

  3. 가상 머신 이미지

  * 애플리케이션 실행을 위해 필요한 전체 컴퓨터 시스템
    * 느린 다운로드와 배포, 비효율적인 성능과 리소스

  4. 컨테이너

  * 애플리케이션의 실행에 필요한 것은 모두 *이미지파일*에 저장
    * 레이어 방식의 파일 구성
  * 가상화의 오버헤드 없이, 실제 CPU에서 실행





* 컨테이너의 특징
  * **플러그 앤 플레이 애플리케이션**
  * 배포 및 패키징, 
  * 생성된 컨테이너 이미지는 재사용이 가능
    * 이것을 이용하여 스케일링, 리소스 할당이 가능
  * 컨테이너의 의존성은 운영체제 커널뿐..



* 컨테이너 오케스트레이션
  * 오케스트레이터
    * 다양한 머신을 하나의 클러스터로 결합하도록 설계된 소프트웨어의 한 종류
      * 클러스터 => 사용자의 입장에서, 컨테이너를 실행할 수 있는 일종의 컴퓨터
    * 일반적으로 스케줄링, 오케스트레이션, 클러스터 관리를 담당
    * 오케스트레이션 != 스케줄링
      * 오케스트레이션 :
        * 서비스의 공통적인 목표를 위해 서로 다른 역할을 조정하고 나열
      * 스케줄링 : 
        * 사용 가능한 리소스를 관리하고 가장 효율적으로 실행할 수 있는 워크로드를 할당하는 것을 의미
          * 지정된 시간에 예약된 작업을 실행하는 스케줄링 의미와 혼동하면 **안된다**





* 쿠버네티스
  * 구글이 개발한 컨테이너 오케스트레이터
  * '보그'에서 시작하여 '쿠버네티스' 오픈 소스 프로젝트로 발전
  * 쿠버네티스가 특별한 이유?
    * 시스템 관리자의 역할
      * 자동화, 장애 조치, 모니터링..
      * => 전통적인 시스템 관리 작업을 대신해주어서, 팀이 보다 핵심적인 작업에 집중할 수 있도록 도움
    * 다양한 기능과 API, 확장하는 생태계
    * 간편한 배포 작업
      * 롤링 업데이트로 무중단 배포
      * 오토 스케일링 지원
        * 클라우드 서비스 업체와 관계없이 사용 가능
        * 서비스 업체가 제공하는 기능에 리소스를 적절하게 매핑
          * 각 클라우드 업체의 세부 사항을 추상화하여 각각에 맞는 기능을 제공
          * => 소프트웨어 **실행 방식**에 대한 이식 가능성
  * 다만, 만능은 아니다 : 
    * 부적합한 애플리케이션 존재
      * 예 :  데이터 베이스
      * 데이터 베이스 레플리카는 서로 호환되지 않는다..
    * 서버리스 플랫폼에서 실행할 수 있는 애플리케이션들..





* 클라우드 함수와 펀테이너
  * 클라우드 함수 : 
    * FaaS( Function as a Service )
    * 클라우드 위에 함수를 등록하고 트리거를 걸어 사용
      * 즉, 이벤트가 발생하면 등록해놓은 함수가 동작
      * 함수 내용 작성을 제외한 모든 것을 클라우스 서비스 업체가 처리!
  * 펀테이너
    * 클라우드 함수와 컨테이너의 혼용
    * 예 : 쿠버네티스 클러스터에서 클라우드 함수를 실행하는 것
  * Knative
    * 컨테이너와 클라우드 함수를 모두 포함하는 쿠버네티스 기반 소프트웨어 제공 플랫폼



* 클라우드 네이티브

  * 클라우드 환경 전체에 지속적인 개발과 자동화된 관리 환경을 제공하기 위해 특별히 설계된 애플리케이션

  * 클라우드 네이티브 환경의 특징

    * 자동화

      * 일반적인 표준, 형식, 인터페이스를 다라 자동으로 애플리케이션을 배포

    * 유비쿼터스와 유연성

      * 컨테이너화된 마이크로서비스는 디스크와 같은 물리적 자원이나, 실행되는 컴퓨팅 노드에 대한 특정 지식과 분리!
      * => 클러스터 간의, 노드 간의 이동을 자유롭게 할 수 있다.

    * 탄력성과 확장성

      * 분산된 환경을 기반으로 고가용성을 보장

    * 역동성

      * 사용 가능한 리소스를 최대한 활용, 컨테이너 스케줄링

    * 관측가능성

      * 분산 시스템의 핵심 요구사항
      * 모니터링, 로깅 등으로 시스템의 작동 상태와 장애 상태를 이해하는 데 도움을 줌

    * 분산

      * 분산 마이크로 서비스
      * 합치면 애플리케이션

      



* 운영의 미래
  * 데브옵스로 인해 구분되지 않는 개발과 운영 업무
  * DPE( developer productivity engineering )
    * 운영 팀을 대신
    * 개발자가 더 나은 환경에서 개발 업무를 할 수 있도록 도움을 제공







## 2. 쿠버네티스 첫걸음



### 2-1. 첫번째 컨테이너 실행하기

* 클라우드 네이티브 개발에서 중요한 개념, *컨테이너*
* 컨테이너를 빌드 및 실행하는 데 필요한 핵심 도구, *Docker 도커*



* 실습으로 진행되는 Docker

  * `docker version`

    * 사용자가 이용하는 도커의 버전이 출력
    * 정상적으로 설치 되었을 때 확인 가능

  * `docker container run -p 9999:8888 --name hello cloudnatived/demo:hello`

    * 책에서 제공하는 github 데모 파일로 컨테이너 생성 테스트
    * 실행 후, `http://localhost:9999/`에 접속하면 명령어 실행 결과를 확인할 수 있다.

  * `git clone https://github.com/cloudnativedevops/demo.git`

    * 전체 소스코드를 git에서 다운받아 살펴본다.
    * Dockerfile, main.go, go.mod로 이루어짐
      * go lang에 대한 간단한 소개
        * 클라우드 네이티브 개발에 유용한 프로그래밍 언어
        * Go로 개발된 다양한 오픈소스 프로젝트 ( 도커, 쿠버네티스, 테라폼.. )
      * 데모 애플리케이션의 동작 분석
        * go의 handler() 함수가 핵심
        * HTTP 요청을 처리하여 HTTP 서버가 8888포트에서 서비스 되도록 실행

  * `docker image build`

    * 컨테이너의 이미지 빌드
      * Dockerfile이란 텍스트 파일을 이용하여 구현 내용을 입력 받는다.
      * 보통 dockerfile은 **베이스 이미지**라고 불리는 시작 이미지를 가져와 변환, 새로운 이미지로 저장한다.

  * Dockerfile 이해하기

    * 테스트용으로 제공된 Dockerfile은 main.go 파일을 go lang 컨테이너에서 컴파일 한 뒤, **scratch**라는 **비어있는 컨테이너 이미지**에 컴파일한 내용만 복사하여 컨테이너 이미지를 최소한의 크기로 작성한다.

    * ```dockerfile
      From golang:1.11-alpine AS build
      
      WORKDIR /src/
      COPY main.go go.* /src/
      RUN CGO_ENABLED=0 go build -o /bin/demo
      
      FROM scratch
      COPY --from=build /bin/demo /bin/demo
      ENTRYPOINT ["/bin/demo"]
      ```

    * 이미지를 최소한의 크기로 작성하는 이유?

      * 배포에 편리함
      * 컨테이너에 올라간 프로그램이 적을수록 보안문제를 일으킬 수 있는 공격 지점이 줄어듦

  * `image build`명령어 추가 옵션

    * `-t` : 이미지에 이름을 지정
      * ex ) `docker image build -t myflasktest .`

  * `docker run` 명령어 추가 옵션

    * `-p` : 실행하는 컨테이너 내부 포트를 호스트 컴퓨터 포트와 연결
      * ex ) `docker container run -p HOST_PORT:CONTAINER_PORT`
      * HOST_PORT에 대한 모든 요청은 CONTAINER_PORT로 자동 전달되게 됨

  * 컨테이너 registry

    * git과 같은 저장소 개념으로, pull push로 이미지를 다운로드 및 업로드 할 수 있다.
    * 기본 registry는 Docker hub이고, 회원가입해서 자유롭게 쓸 수 있다. 
      * 로컬에서 사용하려면 docker login 필요
    * `push` 
      * 로컬 이미지를 registry에 push 하려면 DOCKER_ID/IMAGE_NAME 형식으로 이름을 정해야함
      * --tag 옵션으로 이름 뒤에 version을 명시할 수 있다.





### 2-2. 쿠버네티스 실행하기

* 앞서 빌드한 docker 이미지를 이용하여 쿠버네티스 환경에서 실행
* 이미지는 `--imag=cloudnatived/demo:hello` 이미지를 이용하여도 좋다.



* kubectl과 minikube를 설치하여 진행되는 실습

  * `kubectl run demo --image=cloudnatived/demo:hello --port=9999 --labels app=demo`

  *  결과 : `deployment.apps "demo" created`

    * `run` 대신에 `kubectl run --generator=run-pod/v1  `, 혹은 `kubectl create ~` 명령어를 이용할 것.

  * 위의 명령어에서는 서비스가 노출될 로컬의 포트만 설정해주었다. (`--port=9999`)

    * 따라서, 별도의 포트 포워딩이 필요

    * `kubectl port-forward deploy/demo 9999:8888`

    * 결과 : `Forwarding from 127.0.0.1:9999 -> 8888`

      `Forwarding from [::1]:9999 -> 8888`

    * 이후, 새로운 터미널 창에서 `curl http://localhost:9999/`로 HTTP 요청을 보내면, 이에 대한 응답이 출력된다.







## 3. 쿠버네티스 구축하기



### 3-1. 클러스터 아키텍쳐



* 클러스터 구성요소

  * 컨트롤 플레인

    * 클러스터의 두뇌 역할
    * 컨테이너 스케줄링, 서비스관리, API 요청 처리 등의 작업 수행
    * **마스터 노드**에서 수행
    * 구성요소
      * `kube-apiserver`
        * 컨트롤 플레인의 프론트 엔드 서버로, API 요청을 처리
      * `etcd`
        * 쿠버네티스와 관련된 모든 정보를 저장하는 데이터 베이스
        * 정보의 예시 :어떤 노드가 존재하고, 클러스터에 어떤 리소스가 존재하는지
      * `kube-scheduler`
        * 새로 생성된 Pod를 실행할 노드를 결정
        * Pod의 생성 및 
      * `kube-controller-manager`
        * deployment와 같은 리소스 컨트롤러를 관리
      * `cloud-controller-manager`
        * 클라우드 리소스와 관련한 컨트롤러
        * 로드 밸런서나 디스크 볼륨과 같은 리소스를 관리

  * 노드 ( 워커 노드 )

    * 클러스터 내에서 사용자의 워크로드 실행
    * 구성요소
      * `kubelet`
        * 노드에 예약된 워크로드를 실행하기 위해 컨테이너 런타임을 관리
        * 상태를 모니터링
      * kube-proxy
        * 네트워크 트래픽을 라우팅
          * Pod-Pod 통신, 혹은 Pod와 외부 네트워크 통신
      * 컨테이너 런타임
        * 컨테이너의 시작과 중지를 맡음
        * 컨테이너 간 통신을 처리
        * 일반적으로는 *Docker*가 사용된다.

  * 서로 다른 컴포넌트를 실행하는 것 이외에, 마스터 노드와 워커 노드의 본질적인 차이점은 없다.

  * 클러스터의 구성 이점

    * 고가용성( HA - high availabilty )
      * 여러 개의 마스터 노드로 구성된 컨트롤 플레인
        * 일부 마스터 노드에 문제가 생겨도, 다른 마스터 노드가 역할을 대신
          * 네트워크 파티션 상황에서도 컨트롤 플레인의 특성으로 처리 가능
          * split-brain 개념에 대해 추가 이해
            * https://bryan.wiki/290
      * etcd 데이터 베이스 복제
        * 여러 노드에 걸쳐 복제하여 마찬가지로 고가용성을 보장

  * 컨트롤 플레인 장애 

    * 클러스터의 오작동을 일으킬 수 있다.

    * 장애가 발생했을 때, 클러스터가 정족수( 쿼럼 - quorum )을 유지할 수 있도록 충분한 **수**의 마스터 노드 필요

      * 예 : 상용 클러스터의 경우, 최소 3개의 마스터 노드 필요

      > 정족수 ? 
      >
      > * 정의 : 여러 사람의 합의로 운영되는 의사기관에서 의결을 하는데 필요한 최소한의 참석자 수
      >
      > * 클러스터에서 정족수의 개념 : 앞서 언급된 **split-brain** 발생을 방지하기 위한 알고리즘의 개념
      >
      >   * 마스터 노드에 장애가 발생 시, 각 노드로부터 투표를 받아 핵심 마스터 노드를 결정
      >
      >   * 쿼럼은 쿼럼 서버 - 쿼럼 클라이언트로 구성
      >
      >     * 쿼럼 서버 : 전체 노드와 통신하며 노드에 장애가 발생 시 중재하여 클러스터의 데이터 손실을 방지
      >
      >     * 쿼럼 클라이언트 : 클러스터의 각 노드에 설치되는 에이전트
      >
      >       * 쿼럼 서버와 통신하며 노드의 상태를 판단하는데 사용된다.
      >
      >       http://www.mantech.co.kr/quorum/

  * 워커 노드 장애

    * 워커 노드 장애는 그다지 중요한 문제는 아님
      * 이유 ? 
        * 컨트롤 플레인이 정상 작동한다면, 워커 노드의 장애를 감지하고 재조정할 것 이기 때문
    * 드물게 가용성 영역에 장애가 발생하기도 한다.
      * 따라서 워커 노드를 가용 영역 여러 곳에 분산시키는 것이 좋다.





### 3-2. 쿠버네티스 자체 호스팅



* 구축과 구입의 갈림길
  * 자체 호스팅은 인력, 기술, 유지보수 등등 많은 자원을 요구
  * 구축의 개념은 단순 설치 및 실행과는 다른 개념이다.
  * 호스팅하게 된다면 고려해야하는 개념
    * 고가용성
    * 안전성
    * 형상 관리
    * 보안성
    * 유지 보수의 체계성
* 자체 호스팅은 지속적인 관리가 필요
  * 모니터링
  * 장애 발생 시 해결 조치
  * 클러스터 최신 버전으로 업데이트
* 도구가 모든 것을 해결하지는 못한다.
  * 대부분의 도구는 간단한 문제만 해결 가능
  * 강력한 도구는 비싸거나 공개되지 않는다.
* 쿠버네티스는 어렵다
  * 간결한 설계로 매우 복잡한 상황을 처리하기 때문에, 이해 하고자 한다면 어려운 소프트웨어가 될 수 밖에 없다.
* 관리 오버헤드



* 따라서, 자체 호스팅 보다는 관리형 서비스가 더 효율적인 관리 방안이 될 수 있다.





### 3-3. 관리형 쿠버네티스 서비스



1. 구글 쿠버네티스 엔진

   * 구글은 GKE( Google Kubernetes Engine )을 구글 클라우드 플랫폼에 통합하여 제공

   * 장점

     * 고가용성
       * 고가용성을 보장해주는 서비스들이 다수 있고, 구글 클라우드의 다른 서비스들과 완벽하게 통합되어있다.
       * 예 : 멀티존 클러스터, 지역 클러스터
     * 클러스터 오토 스케일링

     

2. 아마존 일래스틱 쿠버네티스 서비스

   * 클러스터 인프라 뿐만 아니라 마스터 노드에 대해서도 비용을 청구

   

3. 애저 쿠버네티스 서비스

   * 구글의 GKE와 같은 경쟁사의 기능을 대부분 제공한다.
   * 웹 인터페이스나 애저 az 명령줄 도구를 사용하여 클러스터를 생성 가능
   * 워커 노드 수를 기준으로 비용이 청구



4. 오픈시프트
   * 서비스형 플랫폼(Paas) 제품으로, 관리형 쿠버네티스 서비스 이상의 역할
   * 전체 소프트웨어 개발 수명 주기를 관리하는 것을 목표로 함
   * 모든 환경에서 단일 쿠버네티스 클러스터를 생성 가능
     * 베어 메탈 서버, 가상 머신, 사설 클라우드, 퍼블릭 클라우드..
     * 다양한 인프라를 보유한 기업에게 유리



5. IBM 클라우드 쿠버네티스 서비스
   * 바닐라 쿠버네티스 클러스터 구축 가능
   * 간단하고 직관적
   * 딱히 차별화되는 기능은 없음



6. VM웨어 PKS
   * 헵티오를 인수하여 VM웨어 패키지에 통합 제공하고 있다.
     * 헵티오?
       * 쿠버네티스 관리 스타트업으로, 쿠버네티스 초기 개발자 2명이 공동 창업
       * 헵티오 인수를 통해, VM웨어는 자체 쿠버네티스 배포판을 출시







### 3-4. 턴키형 쿠버네티스 솔루션

> 턴키란?
>
> * turnkey 방식 또는 Design -Build
> * 구매자가 제품을 바로 사용할 수 있도록 생산자가 인도하는 방식
>
> 목적 : 
>
> * 턴키형 솔루션은 웹 브라우저에서 버튼만 클릭하면 바로 사용 가능한 상용 쿠버네티스 클러스터 제공을 목표로 하고 있다.



1. 스택포인트
   * 자체 호스팅과 관리 서비스 사이의 절충안
   * 웹 기반에서 쿠버네티스를 프로비저닝 및 관리 가능
   * 사용자의 퍼블릭 클라우드 인프라에서 워커 노드를 실행하기 좋음
   * 장점 :
     * 무제한 노드 클러스터
     * 멀티 클라우드에 걸친 클러스터 페더레이션
     * => 페더레이션 기능은 현재 불안정하다고 판단, 가이드 상에서 deprecated(사용 중단) 기능으로 분류



2. 컨테이너십 쿠버네티스 엔진
   * 퍼블릭 클라우드에서 쿠버네티스를 프로비저닝하기 위한 또 다른 웹 기반 인터페이스
   * 클러스터에 요구되는 거의 모든 사항을 사용자가 지정 가능







### 3-5. 쿠버네티스 설치 프로그램



1. kops
   * 쿠버네티스 클러스터의 자동 프로비저닝을 위한 명령줄 도구
   * 쿠버네티스 프로젝트의 일부로, AWS 전용 도구로 오랫동안 사용되어 왔다.
   * 상용 쿠버네티스 배포에 적합
     * 고가용성 클러스터 구축을 지원
     * 선언적 구성을 사용
     * 클라우드 자원 프로 비저닝, 스케일링, 노드 재조정, 업그레이드...



2. Kubespray
   * 이전 이름 : Kargo
   * 쿠버네티스 산하 프로젝트
   * 상용 클러스터를 쉽게 배포하는 도구
   * 온프레미스 베어 메탈 서버에 쿠버네티스를 설치하는 것에 중점



3. TK8
   * 테라폼( 클라우드 서버 생성용 ) + kubespray( 쿠버네티스 설치용 )을 모두 활용하는 명령줄 도구
   * 쿠버네티스 클러스터 프로비저닝을 위해 사용
     * 쿠버네티스 구축 뿐만 아니라, 추가 기능 설치를 제공
     * 추가 기능의 예 : 제이미 클러스터( 부하 테스트 ), 프로메테우스( 모니터링 ), 예거( 추적 ), 헬름( 쿠버네티스 패키징 ), 젠킨스 X( CI/CD를 위한 도구 ) ...
   * Go로 작성



4. 켈시 하이타워의 튜토리얼
   * 켈시 하이타워의 [ Kubernetes The Hard Way ] 튜토리얼
     * 클러스터 수동 설치 설명 자료
     * 구축 과정의 복잡한 상태를 잘 보여주는 것으로 유명
     * 내부적으로 쿠버네티스가 어떻게 작동하는지 이해하고자 할 때 따라하기 좋은 예제



5. kubeadm
   * 쿠버네티스에서 제공하는 도구
   * 쿠버네티스 클러스터를 최적으로 설치하고 관리할 수 있도록 도움
   * 클러스터에 인프라를 프로비저닝 하지 않는다!
     * => 베어 메탈 서버나 클라우드 인스턴스에 쿠버네티스를 설치할 때 적합



6. Tarmak
   * 쿠버네티스 클러스터의 수명 주기를 관리하는 도구
   * 클러스터 노드의 수정, 업그레이드 작업을 도와 안정성 확보
     * 노드 수정 시, 대부분의 도구는 노드를 **교체**
     * Tarmak은 교체 과정 없이 노드 그대로 수정, 업그레이드
   * 내부적으로 테라폼 이용
     * 클러스터 노드 프로 비저닝
   * 퍼핏
     * 노드의 설정 관리



7. 랜처 쿠버네티스 엔진
   * 간단하고 빠른 쿠버네티스 설치 프로그램
   * 노드 프로비저닝 제공 X
     * 클러스터 구축 전 직접 도커를 설치해야함
     * 컨트롤 플레인의 고가용성 지원해준다.



8. 퍼핏 쿠버네티스 모듈
   * 쿠버네티스와 관련하여 컨트롤 플레인과 etcd의 고가용성 지원을 포함한 쿠버네티스 설치 및 설정 기능 제공



9. kubeformatioin
   * 온라인 쿠버네티스 구성 도구
   * 웹 인터페이스를 통해 클러스터 옵션을 설정할 수 있다.





### 3-6. 구입할 것이냐, 구축할 것이냐



* 적게 실행하는 소프트웨어 철학
  * 표준 기술 이용하기
  * 필요할 땐 아웃소싱
  * 지속적인 경쟁 우위 창출하기



* 클라우드 기반이 아닌, 쿠버네티스 자체를 대상으로 애플리케이션과 자동화를 고려하여 설계한다면 업체 종속에서 자유로울 수 있다.



* 구축하게 되었을 때
  * 표준 쿠버네티스 설치 도구를 이용하는 것이 좋다
  * 반드시 퍼블릭 클라우드 업체에 인프라를 적용할 필요는 없다.





### 3-7. 클러스터가 없는 컨테이너 서비스

* 클러스터리스
  * 컨테이너 워크로드 실행의 오버헤드를 최소화
  * 내부적으로 클러스터가 동작하지만, kubectl과 같은 도구로 접근할 수 없다.
  * 몇 가지 매개변수를 지정하면, 서비스가 나머지 작업을 수행
    * 매개변수 : 컨테이너 이미지, 애플리케이션의 CPU, 메모리 요구사항 ...
    * 서비스 : 애저 컨테이너 인스턴스, 아마존 파게이트



1. 아마존 파게이트
   * VM 대신 컨테이너를 제공
   * 아마존의 ECS와 달리, 클러스터 노드를 프로비저닝하고 컨트롤 플레인에 연결하지 않아도 된다.
     * 컨테이너 이미지를 실행하기 위한 명령어만 정의, 실행하면 된다.
   * 적합한 용도
     * 단순하고 독립적이며, 장기간 실행되는 계산 작업
     * 다른 서비스와의 통합이 필요하지 않은 배치 작접
     * 수명이 짧은 컨테이너를 빌드할 때
     * 워커 노드 관리의 오버헤드가 클 때



2. 애저 컨테이너 인스턴스
   * 파게이트와 비슷
   * 애저 쿠버네티스 서비스와의 통합도 제공
     * 예 :  트래픽이 급증한 경우, 임시로 Pod를 프로비저닝 하여 제공







## 4. 쿠버네티스 오브젝트 다루기



### 4-1. Deployment



* 관리와 스케줄링

  * 디플로이먼트 오브젝트

    * 각 프로그램을 관리하기 위해 생성

    * 프로그램에 대한 정보를 기록

      * 컨테이너 이미지 이름, 실행할 레플리카 수, 컨테이너 실행에 필요한 정보

    * 컨트롤러에 의해 관리된다.

      * 디플로이먼트 리소스에 정의된 'Spec' 내용에 맞게 리소스가 작동하는지 확인
      * 지정된 레플리카 수를 기준으로, 레플리카 수가 그 이상이거나 이하면 즉시 정해진 기준 숫자에 맞게 리소스를 관리한다.
        * deployment를 정의하는 YAML 파일에 레플리카 수가 정의되어 있다면,
        * 레플리카를 종료하거나 제거하더라도 계속 재시작을 하게 된다.
        * =>  쿠버네티스는 *재시작*을 기본 작동으로 수행

    * 관리 순서도 : 

      * `컨트롤러 => 디플로이먼트 오브젝트 => 레플리카셋( ReplicaSet ) => 레플리카 객체( Pod )`

      

* 디플로이먼트 조회하기
  * `kubectl get deployments`
    * 현재 네임스페이스에서 활성화된 모든 deployment 를 확인할 수 있다.
  * `kubectl describe deploymets/<deployment의 NAME>`
    * 특정 deployment의 상세 정보를 확인하기 위한 명령어
    * deployment에는 Pod를 생성하기 위한 Pod Template 정보가 함께 담겨있다.





### 4-2. Pod



* Pod란?
  * 하나 이상의 컨테이너 그룹으로 구성된 쿠버네티스 오브젝트
  * Pod의 Spec에는 컨테이너 목록이 존재



* Deployment에서 컨테이너를 개별적으로 관리하지 않는 이유?
  * 컨테이너 집합으로 스케줄링 되는 경우가 있기 때문
    * 이때, 동일 노드에서 실행되며, 자원 공유가 필요
  * Pod를 실행할 때,
    * Pod 스펙을 참고하여, Pod는 특정 컨테이너와 함께 실행되어야 함을 **선언**





### 4-3. ReplicaSet



* ReplicaSet

  * 동일한 Pod 집합이나 레플리카들을 관리하는 오브젝트
  * 관리의 기준은 Spec에 상세된 레플리카 수를 기준으로 한다.

  * 디플로이먼트 오브젝트에 의해 컨트롤된다.
    * 사용자가 직접 replicaset을 다루는 경우는 드물다.







### 4-4. 조정 루프



* 조정 루프
  * 쿠버네티스 컨트롤러는 각 리소스에서 지정한 "상태( Spec )"와 지속적으로 비교, 동기화하기 위한 작업을 수행
  * 의도한 상태( Spec )과 실제 상태를 일치하기 위한 조정 작업을 영원히 반복



* 작동 예시 
  * 디플로이먼트에 Pod가 "항상 실행"되어야 한다고 정의했다면, 
  * => Pod를 직접 종료, 제거하더라도 이것을 운영자의 실수라고 판단하고 새로운 Pod를 실행한다.
  * => 실제로 이것을 감지, 교체 작업을 진행하는 것은 노드 내의 kubelet
    * 종료된 Pod가 재 스케줄링되는 것





### 4-5. 쿠버네티스 스케줄러



* 스케줄러란?
  * 디플로이먼트가 Pod를 생성하고, 쿠버네티스가 요청된 Pod를 실행하는 **과정을 책임지는 컴포넌트**



* 역할 : 
  * 대기열에서 스케줄링 되지 않은 Pod를 탐색
  * 탐색된 Pod를 배치 및 실행하기 적합한 Node를 탐색
  * => 실행은 언제..?
    * Pod가 노드에 스케줄링되어 할당되면, 노드 내의 kubelet이 컨테이너를 실행





### 4-6. YAML 형식의 리소스 매니페스트



* 쿠버네티스는 근본적으로 **선언형** 시스템
  * Spec을 선언 후, 수정사항이 생겼을 때 수정하면 쿠버네티스가 변경 사항을 처리





* 리소스는 데이터다
  * 쿠버네티스의 리소스는 모두 내부 데이터 베이스에 기록
  * => 조정 루프는 이러한 데이터 베이스 기록 내의 변경사항을 감시, 이에 따라 동작
  * 명령어 이용보다, 리소스 **매니페스트**를 생성, 수정을 권장
    * 매니페스트? 
      * 리소스에 대해 '의도한 상태'의 스펙
    * kubectl apply 명령어를 이용하여 매니페스트를 적용하게 됨
      * apply : 선언형 명령어
        * 쿠버네티스는 선언형 인프라 코드 시스템이므로 보다 활용하기 좋은 명령어



* 디플로이먼트 매니페스트
  * 일반적으로 YAML 파일로 정의
    
    * JSON 형식도 이용 가능
    
  * 예시 : 
  
    ```yaml
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
    	name: demo
    	labels:
    		app: demo
    	spec:
    		replicas: 1
    		selector: 
    			matchLabels:
    				app: demo
    			template:
    				metadata:
    					labels:
    						app: demo
    				spec:
    					containers:
    						- name: demo
    						  image: cloudnatived/demo:hello
    			
    ```





* **Service( 서비스 리소스 )**

  * 서비스 리소스란?

    * 네트워크 연결이 필요한 Pod에 영구적인 IP 주소와 DNS 주소를 제공
    * 트래픽 요청을 받으면, 백엔드 Pod Set에 요청을 전달
    * 웹포트와 YAML 파일의 Spec에 명시된 포트로 트래픽 전달 
      * label과 selector를 이용하여 label 값이 일치하는 Pod 중 무작위로 선택한 Pod로 각 요청을 전달
      * 전통적인 로드 밸런서와 유사
    * *즉, Service는 Pod에게 요청을 전달하는 단일 엔트리 포인트를 제공*

  * 서비스 매니페스트 예시 : 

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
    	name:demo
    	labels:
    		app: demo
    spec:
    	ports:
    	-	port: 9999
    		protocol: TCP
    		targetPort:8888
    	selector:
    		app:demo
    	type: ClusterIP
    ```





* kubectl로 클러스터 조회하기
  * 리소스 관리 도구
    * 리소스 설정 적용
    * 리소스 생성, 수정, 제거
    * 리소스 상태에 대해 조회
  * `kubectl get all`
    * 모든 타입의 리소스를 확인
  * `kubectl describe <리소스 종류>/<리소스 이름>`
    * 개별 리소스에 대한 포괄적인 정보 확인





### 4-7. Helm ( 헬름 ) : 쿠버네티스 패키지 매니저



* 헬름이란?
  * 쿠버네티스 패키지 매니저
  * 헬름 CLI로 애플리케이션 설치 및 설정 가능
    * Helm chart
      * 패키지
      * 애플리케이션을 실행하는데 필요한 리소스, 의존성, 구성 가능한 변수를 지정 가능
      * 다른 바이너리 패키지와의 차이점 : 
        * 실제 실행되는 컨테이너 이미지 자체는 포함하지 않음
        * 이미지를 찾을 수 있는 메타 데이터만 포함
  * 클라우드 네이티브 컴퓨팅 파운데이션(CNCF)  프로젝트의 일부





* 헬름 설치
  * https://helm.sh/ko/docs/intro/using_helm/
  * helm install은 *헬름 릴리스*라는 쿠버네티스 오브젝트를 생성하여 이를 수행





* chart, repository, relelase
  * chart: 
    * 쿠버네티스에서 애플리케이션을 실행하는 데 필요한 모든 리소스를 정의하여 포함
  * repository :
    * 차트가 모여 공유할 수 있는 공간
  * release:
    * 쿠버네티스 클러스터에서 실행되는 차트의 특정 인스턴스
    * `-name` 플래그로 이름을 지정할 수 있다.
      * 지정 안하면 임의로 지어진다.





* `helm list`
  * 실행 중인 릴리스 확인
  * `helm status <릴리즈 이름>`
    * 특정 릴리스의 정확한 상태 확인







## 5. 리소스 관리하기



### 5-1. 리소스 이해하기

> 쿠버네티스 스케줄러 관점에서 리소스를 가장 효율적으로 사용할 수 있는 방법을 생각해본다.
>
> 각 Pod마다 리소스 요청 / 상한이 존재한다.
>
> CPU와 메모리 두 리소스를 어떻게 관리하면 좋을까?





* 리소스 단위
  * Pod의 CPU 사용량
    * CPU 단위로 표시
    * 1CPU = 일반적인 CPU 단위
    * 대부분의 Pod는 CPU 전체를 필요로 하지 않는다.
      * 요청과 상한은 millicpus로 표시
      * 메모리는 바이트 / mebibyte( MiB )로 측정



* 리소스 요청
  * Pod를 실행하기 위한 최소 리소스 양을 지정
  * 쿠버네티스 스케줄러는 Pod template에 요구된 리소스를 기준으로 충분한 리소스 용량을 가진 노드를 탐색
    * 요구된 리소스보다 적은 리소스를 가진 노드에는 Pod가 할당되지 않는다.



* 리소스 상한
  * Pod가 사용할 수 있는 최대 리소스 양을 지정
    * 만일, Pod가 지정된 상한을 초과하여 CPU를 사용하려고 하면, 해당 Pod는 곧바로 제지된다.
    * 상한을 초과한 Pod는 종료 후 재 스케줄링된다.
    * 상한을 지정함으로써, Pod가 클러스터 용량을 과다하게 점유하는 것을 막을 수 있다.
  * 오버커밋( overcommit )
    * 노드 내 컨테이너의 모든 리소스 상한의 합계가 해당 노드의 전체 리소스 양을 초과할 수 있음을 의미
      * 컨테이너 모든 리소스 상한의 합 > 노드 전체 리소스 양
    * 스케줄러는.. 이런 상황이 발생하지 않을 것이라고 가정하고 오버 커밋을 허용
    * 발생 시
      * 리소스 부족으로 인해 컨테이너를 종료
        * 상한 리소스를 초과하지 않은 컨테이너도 종료될 수 있다.
      * 리소스 요청을 가장 많이 한 Pod부터 종료한다. 





* 컨테이너를 작게 유지하라
  * 컨테이너 이미지는 가능한 작은 것이 좋다.
    * 더 빠른 빌드
    * 저장공간 덜 차지
    * 더 빠른 풀링
    * 더 적은 보안 취약점





### 5-2. 컨테이너 생명 주기 관리하기



* Stuck state
  * 프로세스가 실행 중이지만 요청을 처리하지 못하는 상태
  * 쿠버네티스는 이러한 상황을 감지하고 재시작하여 문제 해결이 가능해야 한다.





* 활성 프로브

  * 컨테이너의 작동여부를 확인하는 헬스 체크

    * 컨테이너 스펙으로 지정할 수 있다.

    * Spec 작성 예시

      ```yaml
      livenessProbe:
      	httpGet:
      		path: /healthz
      		port: 8888
      	initialDelaySeconds: 3
      	periodSeconds: 3
      ```

    * 작동 설명

      * httpGet 프로브가 지정한 URI와 포트에 HTTP 요청
        * /healthz의 8888포트로 요청
        * `/healthz` : 일반적으로 헬스 체크를 위한 엔드포인트로 쓰인다.
      * 애플리케이션이 `2xx`, `3xx` 상태 코드로 HTTP 응답 시, 활성 상태로 판단
        * 다른 값으로 응답 시, 컨테이너 죽은 것으로 판단, 재실행



* 프로브 딜레이와 주기
  * 쿠버네티스는 언제 활성 프로브를 검사해야 하는가?
    * `initalDelaySeconds`
      * 첫 번째 활성 프로브를 실행하기 전에 얼마나 기다려야하는지 값을 설정
      * 시작하자마자 활성 프로브를 검사하는 경우를 막아준다.
    * `periodSeconds`
      * 활성 프로브를 검사하는 주기를 지정
      * 지나치게 많은 헬스 체크 요청은 좋지 않다.



* 그 외의 프로브

  * HTTP 외에 tcpSocket도 이용할 수 있다.

  * 작성 예시

    ```yaml
    livenessProbe:
    	tcpSocket:
    	port: 8888
    ```

  * exec 프로브 작성 예시

    ```yaml
    redinessProbe:
    	exec:
    		command:
    		- cat
    		- /tmp/healthy
    ```

    * exec 프로브
      * 컨테이너에 지정된 명령어를 실행한다.
      * 명령어가 실행 후, 제로 상태 코드로 종료하면 프로브는 성공
      * 일반적으로 준비성 프로브로 유용하게 사용



* gPRC 프로브
  * **마이크로서비스**에서 가장 인기있는 네트워크 프로토콜
  * 효율적이고 간편한 바이너리 네트워크 프로토콜
  * 구글이 개발
  * httpGet 프로브는 작동하지 않으므로, tcpSocket 프로브를 사용
    * 서버 연결 불가능
    * 소켓 연결 가능
  * `grpc-health-probe`
    * 쿠버네티스 활성 프로브로 헬스체크를 활용하기 위한 도구
    * 컨테이너에 도구 추가 시, exec 프로브를 사용하여 상태 확인 가능



* 준비성 프로브 ( rediness probe )

  * 애플리케이션이 일시적으로 요청을 처리할 수 없는 상태일때, 쿠버네티스에 신호를 보내는 기능

  * 작성 예시 

    ```yaml
    readinessProbe:
    	httpGet:
    		path: /healthz
    		port: 8888
    	initialDelaySeconds: 3
    	periodSeconds: 3
    ```

    * 애플리케이션이 준비를 완료할 때까지 HTTP를 수신하지 않는 경우의 작성 예시

  * 준비성 프로브가 준비를 실패하면, Pod는 제거된 뒤, 준비를 성공할 때까지 재시작된다.

    * => 준비되지 않은 컨테이너 때문에 발생하는 사용자 에러 예방
    * 준비 완료 반환 값은 오로지 `HTTP 200 ok` 
      * => 쿠버네티스는 200~ 300 사이의 HTTP 응답값을 모두 "준비" 상태로 판단하지만, 클라우드 로드 밸런서는 300 이상의 코드를 비정상으로 판단할 수도 있기 때문.



* 파일 기반 준비성 프로브
  * exec 준비성 프로브로 컨테이너 내에 파일 존재 여부를 알 수 있다.
  * 컨테이너 내 프로그램 디버그 시 유용하다.
    * 준비성 프로브 기준을 컨테이너 내 파일로 설정한 뒤, 
    * 디버그할 컨테이너에 접속하여 기준 파일을 삭제한다.
    * 접속한 컨테이너는 준비성 프로브에서 실패하였으므로 서비스에서 제외된다.
    * 서비스 중단 상태에서 디버그를 진행한다.



* `minReadySeconds`
  * 컨테이너가 준비 완료 상태이지만, 안정성을 좀 더 확인하고자 할 때 이용하는 필드값
  * 준비성 프로브 성공 후, minReadySeconds 지정 값이 지날 때까지 컨테이너는 준비 상태로 판단되지 않는다.



* PodDisruptionBudgets

  * 주어진 시간에 제거할 수 있는 Pod의 양을 제한하는 리소스

  * `minAvailable`

    * 최소한 실행해야 하는 Pod의 갯수를 지정하는 필드값

    * 작성 예시

      ```yaml
      apiVersion: policy/v1beta1
      kind: PodDisruptionBudget
      metadata:
      	name: demo-pdb
      spec:
      	minAvailable: 3
      	selector:
      		matchLabels:
      			app: demo
      ```

      * `app: demo` 레이블과 일치하는 Pod가 적어도 3개는 실행 중이어야 한다는 것을 의미

  * `maxUnavailable`

    * 한 번에 퇴출시킬 수 있는 Pod의 총 갯수 / 비율을 제한하는 필드값

    * 작성 예시

      ```yaml
      apiVersion: policy/v1beta1
      kind: PodDisruptionBudget
      metadata:
      	name: demo-pdb
      spec:
      	maxUnavailable: 10%
      	selector:
      		matchLabels:
      			app: demo
      ```

      * `app: demo` 레이블과 일치하는 Pod를 10% 이상 한번에 퇴출시킬 수 없음을 의미
      * 자발적 퇴출의 경우에만 제한값이 적용
        * 자발적 퇴출?
          * 쿠버네티스가 인위적으로 Pod를 종료하는 경우를 뜻함
      * 하드웨어 장애 및 비자발적 퇴출로 인한 것은 제한 값에 포함되지 않고 별도로 취급된다.

  * 쿠버네티스는 Pod를 분산시키려는 경향이 있다.

  * 분산 운영 시, Pod 축출이 과하게 이뤄졌을 경우 문제가 발생할수 있으므로 위와 같은 옵션값을 리소스에서 잘 지정하자!

    * 서비스에 영향이 가지 않도록 충분한 수의 레플리카를 유지할 수 있도록 하자는 뜻!



### 5-3. 네임스페이스 사용하기

> 리소스 관리의 또다른 방법 중에는 **네임스페이스**를 활용하는 것이 있다.
>
> * 예 : 상용 애플리케이션을 위한 prod 네임스페이스 / 테스트를 위한 test 네임스페이스
> * 네임스페이스 안의 Name은 다른 네임스페이스와 격리
> * 컴퓨터 하드디스크의 폴더와 비슷하다고 볼 수 있다.
>   * 즉, 관련된 리소스를 그룹화하여 작업하기 쉽게 만들어준다.
>   * 단, 폴더와는 다르게 네임스페이스는 중첩될 수 없다.



* 네임스페이스 다루기
  * 네임스페이스를 지정하지 않으면 `default` 기본 네임스페이스에 명령이 실행
    * `--namespace` 플래그로 네임스페이스를 지정
    * `kubectl get pods --namespace <namespace 이름>`
      * 네임스페이스에 해당하는 모든 Pod를 나열
  * `kube-system` 네임 스페이스
    * 내부 시스템 컴포넌트가 실행되는 공간
    * 애플리케이션과는 격리되어있다.



* 어떤 네임스페이스를 사용해야 하는가?

  * 직관적인 방법

    * 하나의 애플리케이션 당 하나의 네임스페이스
    * 예 : demo 애플리케이션을 demo 네임스페이스에서 실행

  * 네임스페이스 생성

    * 작성 예시

      ```yaml
      apiVersion: v1
      kind: Namespace
      metadata:
      	name: demo
      ```

    * 네임스페이스를 삭제하면 네임스페이스 내 모든 리소스가 삭제된다.

  * 쿠버네티스 네트워크 정책

    * 특정 네임스페이스의 모든 트래픽을 차단하는 경우에 사용



* 서비스 주소
  * 서비스의 DNS 양식
    * `SERVICE.NAMESPACE.svc.cluster.local`
    * `svc.cluster.local`은 선택 사항
    * `NAMESPACE`도 선택사항
    * 작성 예시 :  prod 네임스페이스에 있는 demo 서비스와 통신하고 싶다
      * `demo.prod`
      * 선택사항을 입력하여 원하는 서비스를 정확하게 지정 가능



* 리소스 쿼터

  * 네임스페이스도 리소스 제한이 가능하다.

  * 리소스 종류 : `ResourceQuota`

  * 작성 예시

    ```yaml
    apiVersion: v1
    kind: ResourceQuota
    metadata:
    	name: demo-resourcequota
    spec:
    	hard:
    		pods: "100"
    ```

    * 위의 매니페스트가 적용된 네임스페이스에서는 실행 가능한 Pod의 갯수가 100개로 제한

  * 적용 예시

    `kubectl apply --namespace <namespace 이름> -f <작성한 매니페스트 이름>.yaml`

  * 장점

    * 특정 네임스페이스 내 애플리케이션이 과도한 리소스를 독점하는 것을 막을 수 있다.

  * 네임 스페이스의 CPU와 메모리 사용량 제한은 **권장하지 않는다**

    * 낮게 설정 시
      * 워크로드가 한계에 가까워졌을 때, 예상 밖의 문제가 발생할 가능성이 크다
    * 높게 설정 시
      * 제한 설정의 의미가 없다.

  * `kubectl get resourcequotas`

    * 특정 네임스페이스에서 리소스 쿼터가 활성화인지 체크



* 기본 리소스 요청과 상한

  * `LimitRange`

    * 네임스페이스 내 **모든** 컨테이너의 기본 리소스 요청과 상한 설정 가능

  * 리소스 매니페스트 작성 예시

    ```yaml
    apiVersion: v1
    	kind: LimitRange
    	metadata:
    		name: demo-limitrange
    	spec:
    		limits:
    		-	default:
    				cpu: "500m"
    				memory: "256Mi"
    			defaultRequest:
    				cpu: "200m"
    				memory: "128Mi"
    			type: Container
    ```

    * 리소스 요청과 상한을 지정하지 않은 네임스페이스의 모든 컨테이너는 LimitRange에서 기본값 상속

  * LimitRange 리소스 이용시, 개별 컨테이너의 요청과 상한 지정을 신경쓰지 않아도 된다.

    * **하지만 권장하지 않는다**
    * 이 리소스 적용 여부와 상관없이, 컨테이너 Spec을 보고 요청과 상한을 알 수 있어야 함
    * 컨테이너 요청과 상한 지정을 놓쳤을 때를 대비한 *안전장치*로만 사용할 것





### 5-4. 클러스터 비용 최적화하기



* deployment 최적화
  * 레플리카의 갯수는 많을 수록 좋을까?
    * 롤링 업그레이드 중에도 서비스 품질이 저하되지 않는다.
    * 더 많은 트래픽 처리 가능
    * => **그러나** 클러스터는 한정된 리소스를 갖고있으므로, Pod는 최대의 효율성을 추구하는 만큼의 갯수만 실행되어야한다.
    * => 무중단 서비스를 제공하는데 있어서, 생각보다 많은 수의 레플리카가 필요하진 않다.
    * 결론 : deployment에서 Pod의 갯수는 최소한으로 두는 것이 좋다.





* Pod 최적화
  * 컨테이너 리소스 상한 설정
    * 워크로드 유형에 따라 상한 설정 방법이 달라진다.
    * 실제 작업보다 약간 높게 설정하되, 지나치게 높은 상한은 설정하지 않는다.
    * 필요성 : 
      * 컨테이너 메모리 누수를 방지한다.
      * 다른 컨테이너의 리소스 침해를 방지한다.



* Vertical Pod Autoscaler
  * 쿠버네티스 Addon
  * 리소스 요청에 대한 이상적인 값 설정을 돕는다.
  * 디플로이먼트를 관찰, 실제 사용하는 양에 따라 자동으로 Pod의 리소스 요청으로 조정
    * dry-run-mode
      * 실행 중인 Pod를 수정하지 않고 제안만 하는 모드



* Node 최적화
  * 모든 노드에는 운영체제가 존재
    * 디스크, 메모리, CPU 자원을 소모
    * 노드 크기가 작을 때 
      * 오버헤드가 차지하는 전체 리소스 비율이 더 높아진다.
      * 버려지는 리소스( stranded resources ) 비율이 높다
        * 사용 가능한 메모리 공간과 CPU 시간이 너무 작아서 Pod에 할당되지 않기 때문
    * 노드 크기가 클 때
      * 워크로드에 사용할 수 있는 리소스 비율이 더 높다.
      * 비용 면에서 효율적
      * 단점 : 개별 노드에 장애가 발생할 경우 클러스터 가용 용량에 미치는 영향이 더 크다.
  * 노드 권장 설정값
    * 노드의 크기
      * 보통의 Pod를 적어도 5개 실행 가능하게 한다.
      * 버려지는 리소스의 비율을 10% 이하로 유지한다.
        * 만약 Pod를 10개 이상으로 실행할 경우, 버려지는 리소스는 5%미만으로 유지해야한다.
    * 노드 당 기본 Pod 상한값은 110개
      * `--max-pods`로 Pod 상한을 조정할 수 있지만, 관리형 서비스마다 지원여부가 다르다.
      * 특별한 사유가 없다면 변경을 권장하지 않는다.
      * 클라우드 업체에서 가장 큰 인스턴스를 사용할 경우 이점이 없다.
        * 이런 경우, 작은 크기의 노드를 다수 운영하는 것이 효율적
  * `kubectl top nodes`
    * 각 노드의 리소스 사용률을 확인한느 명령어
    * CPU 사용률이 높을수록 잘 활용되고 있는 것
    * 큰 노드의 사용률이 높으면 작은 노드를 제거하고 큰 노드로 교체하는 것이 좋다.





* 스토리지 최적화
  * 디스크 스토리지는 간과하기 쉬운 클라우드 비용 중 하나!
  * 다른 리소스들과 달리, 조정이 불가능한 리소스
  * 실제 사용하는 처리량과 공간을 기준으로 할당할 디스크 볼륨의 크기를 정해야 한다.





* 사용하지 않는 리소스 정리

  * 정기적으로 리소스를 정리하여서 비용과 리소스를 효율적으로 처리하는 것이 좋다.

  * 정리 방법

    1. 소유자 메타데이터 이용

       * 각 리소스에 소유자 정보를 태그하여 관리

         * 매니페스트 파일의 annotation 필드값으로 관리

         * 필드값 작성 예시

           ```yaml
           apiVersion: extendsions/v1beta1
           kind: Deployment
           metadata:
           	name: my-brilliant-app
           	annotations:
           		example.com/owner: "Customer Apps Team"
           		...
           		
           ```

    2. 활용도가 낮은 리소스 파악

       * Pod의 활용도를 **메트릭**을 이용하여 파악한다.
         * 모든 Pod는 수신한 요청의 수를 메트릭으로 기록
       * 웹 콘솔에서 각 Pod의 CPU와 메모리 사용류률을 확인
       * 활용도가 낮지만 유지가 필요한 Pod는 annotation 필드값에 추가하여 식별할 수 있도록 해주는 것이 좋다.

    3. 완료된 잡 정리하기

       * 쿠버네티스 Job?
         * 한번만 실행되고 완료된 이후에는 다시 시작하지 않는 Pod
         * 완료된 잡은 삭제되지 않고 쿠버네티스 데이터베이스에 잔류
         * => 누적되면 API 성능에 영향을 줄 수 있다.
       * `kube-job-cleaner`
         * 완료된 잡을 정리하는 도구





* 여유 용량 파악하기
  * 현재 여유 용량 확인 방법
    * 운영 중인 노드에서 가장 큰 노드를 비우면 됨
    * 즉, 해당 노드가 종료되는 상황을 만든 후, 종료된 노드에 있던 Pod들이 다른 노드로 이동해 실행되는 과정이 잘 이루어지는지 확인
    * 잘 작동하지 않는다면, 장애 상황에 대처하지 못하는 경우이므로 용량 추가가 필요하다.





* 예약 인스턴스 사용하기
  * AWS 예약 인스턴스
    * On-demand 인스턴스 (기본 형태)의 절반 가격
    * 1, 3년 등 다양한 기간으로 인스턴스 예약 가능
    * 중간에 크기 변경은 불가능
  * 구글 클라우드 약정할인 인스턴스
    * 특정 양의 vCPU와 메모리 사용 계약 가능
    * 약정 범위를 넘어서는 사용량은 일반 가격으로 추가 지불
      * 초기 예약량보다 필요에 따라 리소스를 더 사용할 수 있으므로, AWS보다 유연
  * 두 서비스 모두 지불 후에 환불이 불가능
  * 요구 사항이 일정할 것으로 예상될 경우에만 인스턴스를 이용하는 것이 낫다.





* 선점형(스폿) 인스턴스 사용

  * 선점형 인스턴스란?

    * 최대 24시간까지만 유지
    * 24시간 내에 인스턴스 활동이 없다면 자동으로 꺼지는 옵션
    * AWS 스폿 인스턴스
      - 시간당 요금이 수요에 비례
    * 구글 클라우스 선점형 VM
      - 고정 비율 청구
      - 일반적으로 5 ~ 15% 
      - 인스턴스 유형에 따라 다르지만, 보통 기본 인스턴스 유형보다 최대 80%까지 저렴해질 수 있음

  * 클러스터를 위한 비용 효율적 선택이 될 수 있음

    * 그러나, 최소한의 워크 로드를 처리할 수 있는 **비선점형 노드**가 충분해야 한다.
    * 불시에 중단되는 선점형 노드를 감당할 수 있는 범위 내에서 사용해야 한다.
      * 클러스터 오토스케일링 기능
        * 가능한 빠르게 중단되는 선점형 노드를 교체
      * 이론상 모든 선점형 노드가 동시에 사라질 가능성 존재
        * 선점형 노드는 클러스터의 2/3 이하로 제한하는 것을 권장

  * 노드 affinity 설정값 이용

    * hard affinity

      * selector의 표현식과 일치하는 노드에 절대 스케줄 되지 않는다.

        ```yaml
        affinity:
        	nodeAffinity:
        		requiredDuringSchedulingIgnoredDuringExecution:
        			nodeSelectorTerms:
        			-	matchExpressions:
        				-	key: cloud.google.com/gke-preemptible
        					operator: DoesNotExist
        ```

        

    * soft affinity

      * selector의 표현식과 일치하는 노드에 우선적으로 스케줄링되도록 지정

        ```yaml
        affinity:
        	nodeAffinity:
        		preferredDuringSchedulingIgnoredDuringExecution:
        		-	preference:
        				matchExpressions:
        				-	key: cloud.google.com/gke-preemptible
        					operator: Exists
        			weight: 100
        ```

         



* 워크로드를 균형있게 유지

  * 스케줄러

    * 스케줄러는 재시작하지 않는 한, 이미 실행된 Pod를 다른 노드로 이동시키지 않는다.

    * => 노드에 Pod를 고르게 분산하여 고가용성을 유지하려는 목적에 부합하지 않는 경우가 발생

    * 예 :  다음과 같이 A와 B Pod가 고르게 분산되어 있는 노드 A, B가 있을 때,

      ![스크린샷, 2020-09-14 11-17-35](https://user-images.githubusercontent.com/58680504/93036898-13fdd000-f67c-11ea-9a8a-a3b1ba296c47.png)

    * Node B에 장애가 생긴 경우, Node A에 그림과 같이 Pod가 몰리게 될 것이다.

      ![스크린샷, 2020-09-14 11-17-52](https://user-images.githubusercontent.com/58680504/93036900-14966680-f67c-11ea-8c56-0ba6cb229466.png)

    * 이런 경우, 불균형 노드가 되었음에도 불구하고 스케줄러는 Pod를 재배치하지 않는다.

      * 고가용성이 보장될 수 없는 상황

    * 이런 상황에서 특정 Pod의 롤링 업데이트가 일어날 경우, 상황은 더욱 악화된다.

      ![스크린샷, 2020-09-14 11-17-59](https://user-images.githubusercontent.com/58680504/93036904-152efd00-f67c-11ea-849e-81987695ab13.png)

    * Pod B가 롤링업데이트를 시도할 경우, Node A는 할당 여유 공간이 없으므로 Node B에서 롤링 업데이트 된 Pod B가 재실행 되게 된다.

      * 이 상황에서도 마찬가지로 고가용성은 보장될 수 없다.
      * 노드 중 하나에 장애가 생기면,  무조건 서비스 중단 

  * 해결 방안

    * **디스케줄러 ( deschedular ) 이용 **
      * 쿠버네티스 Job으로 실행 가능
      * 사용자가 설정할 수 있는 다양한 정책 제공
        * 예 : 
          * 사용률이 낮은 노드를 찾고, 다른 노드에서 실행 중인 Pod를 강제 종료 후 다른 노드로 재스케줄링
          * 동일한 노드에서 중복되는 Pod가 실행중이면, 퇴출 후 재스케줄링
      * 두 개의 예시만으로도, 위에서 언급된 상황을 해결하고, 고가용성을 보장할 수 있음을 확인할 수 있다.











## 6. 클러스터 운영하기



### 6-1. 클러스터 사이징과 스케일링

* 클러스터의 비용은 노드의 크기와 갯수에 비례
  * 적정 크기를 정하고, 확장 가능성을 두는 것이 좋다.



* 클러스터 용량 계획 세우기
  * 동일한 애플리케이션을 실행하는 데 필요한 기존 서버의 수를 계산
  * 가장 작은 클러스터
    * 단일 노드 구성
    * 단일 노드도 워크로드 실행 가능
    * *그러나* 장애가 하나라도 발생 시, 복구가 불가능
    * 워크 로드가 쿠버네티스 컨트롤 플레인과 리소스 경쟁을 하게될 수도 있다.
  * 가장 큰 클러스터
    * 쿠버네티스에서는 5000개의 노드 제한
      * 클러스터링에서 필요한 노드 간 통신 경로 수, 데이터 베이스의 누적 부하 발생
      * 구성은 가능하지만, 응답 속도 보장 불가
    * 쿠버네티스에서 권장하는 지원가능 클러스터 최대
      * 노드 5000개 이하, 
        * 노드당 Pod 갯수는 100개 이하
      * Pod 15만개 이하,
      * 전체 컨테이너 30만개 이하,
      * => 클러스터가 클수록 마스터 노드의 부하도 커짐
      * => 이 이상의 크기를 원한다면, 다중 클러스터 사용
  * 클러스터 페더레이션
    * 대규모 클러스터를 운영할 경우
    * 워크로드의 요청이 많을 경우
      * => 워크로드를 여러 *클러스터*에 분산 가능
    * 페더레이션이란?
      * 두 개 이상의 클러스터를 동기화하여 동일한 워크로드를 실행
      * 용도 : 
        * 클러스터 운영 시 멀티클라우드를 이용하거나,
        * 사용자 지연 시간( latency )를 줄이기 위해 여러 지역에 클러스터가 분포한 경우
      * 장점 : 
        * 개별 클러스터에 장애가 발생하더라도, 서비스에 영향을 주지 않는다.
  * 멀티 클러스터가 필요한가?
    * 대규모 서비스가 아닌 경우, 대부분 두 개 이상의 클러스터가 필요하지 않을 것
    * 멀티 클러스터의 리소스 관리
      * 네임스페이스 이용
        * 클러스터를 논리적 영역으로 분리하여 리소스 관리
      * 멀티 클러스터 운용 시, 필연적으로 오버헤드가 발생하므로, 네임스페이스를 이용해 클러스터를 분리하는 것을 권장





* 노드와 인스턴스
  * 적절한 노드 크기 선정
    * 클라우드 서비스, 하드웨어 업체 혹은 워크로드의 특성에 따라 달라짐
    * 노드 인스턴스
      * 비용 효율적인 면에서 선택하는 것이 좋다
    * 노드 갯수
      * 노드 크기에 따라 달라짐
      * 고가용성을 위한 최소한의 갯수와 크기를 맞출 것을 권장
  * 클라우드 인스턴스 유형
    * 마스터 노드 최소 스펙
      * vCPU 1개
      * 메모리 3~ 4GiB
    * 워커 노드 권장 스펙
      * 단일 CPU
      * 메모리 4GiB
      * kubelet도 마찬가지로 리소스를 사용하기 때문에, 원활한 워크로드 실행을 위해서는 여유 용량이 필요하다.
      * 워커 노드에 지나치게 작은 크기의 인스턴스를 사용하지 말것.
  * 다양한 종류의 노드
    * 예 : GPU와 같은 특수속성 노드
      * 머신러닝, 데이터 분석 작업 등에 활용
    * 대부분의 노드는 리눅스 기반에서, 리눅스용 컨테이너를 빌드
    * GPU나 윈도우와 같은 특정 요구사항
      * 특별한 유형의 노드를 추가하여 프로비저닝 후, 빌드하면 된다.
  * 베어 메탈 서버
    * 베어 메탈 서버를 쿠버네티스 클러스터로 활용 가능
    * 기존의 서버를 컴퓨팅 리소스로 전환하는 대신,
    * 클러스터로 활용하면 된다. 







* 클러스터 스케일링
  * 클러스터 확장 및 축소 가능성
  * 인스턴스 그룹
    * 활용 도구 예시:
      * kops
        * 클러스터 관리 도구
        * 인스턴스 그룹
          * 주어진 인스턴스 유형의 노드 집합 개념
      * 구글 쿠버네티스 엔진
        * 노드 풀
          * 인스턴스 그룹의 크기 변경
          * 특정 인스턴스 유형을 변경
  * 다운스케일링
    * 쿠버네티스에게 제거할 노드 비우도록 지시 => 쿠버네티스에서 해당 노드에 실행 중인 Pod를 다른 노드로 이동 => 제거할 노드를 종료
    * 나머지 클러스터 노드에 충분한 여유 용량이 없다면?
    * 또는, 특정 서비스 Spec에 명시된 Pod 최소 실행 값등의 제한요소가 있다면?
      * 제한값이 변경되거나, 클러스터가 리소스를 확보할 때까지 노드를 비우는 작업이 중단
  * 오토스케일링
    * 대부분의 클라우드 서비스 업체가 지원
    * 일부 메트릭과 스케줄에 따라 그룹 내 인스턴스를 자동으로 증감
      * 인스턴스 최대 최소 갯수를 정해놓고 증감 가능
      * 시간에 따른 서비스 수요량에 맞춰 증감 가능





### 6-2. 적합성 검사

* 쿠버네티스가 보편적인 플랫폼이 되려면?
  * 어떤 클러스터에서도 워크로드 실행이 가능해야 한다.
  * => 유연하고 이식성이 뛰어나야함
  * => 쿠버네티스가 올바르게 작동하는지, 도구로 확인해볼 필요가 있다.



* CNCF 인증
  * CNCF?
    * Cloud Native Computing foundation의 약자
    * 쿠버네티스 프로젝트와 상표 소유
    * 쿠버네티스와 관련된 업체에 인증 제공
  * 공인 쿠버네티스
    * 쿠버네티스 인증 마크가 붙는 것은, 서비스 업체가 CNCF에서 지정한 쿠버네티스 표준을 따른 다는 것을 의미
    * 인증받은 업체 간은 쿠버네티스를 통한 호환이 가능하다.
    * Sonobuoy
      * CNCF 인증 이외에 업체에서 자체인증 할 수 있는 도구
  * 공인 쿠버네티스 관리자
    * CKA 시험을 통과하여, 상용환경에서 클러스터를 관리하는 역량을 보유한 관리자
  * 쿠버네티스 공인 서비스 업체
    * CNCF 회원이고, 기업 지원을 제공하는 업체는 쿠버네티스 공인 서비스 업체 프로그램에 지원 가능
    * 이외에도, CKA 공인 엔지니어 3명 이상 고용하는 등의 조건이 포함되어 있다.



* Sonobuoy를 사용한 적합성 테스트
  * 헵티오의 표준 도구
  * https://github.com/vmware-tanzu/sonobuoy
  * kubectl apply 명령어로 클러스터에서 실행,
  * 적합성 검사를 진행하면 헵티오로 결과가 전송된다.
  * 스캐너 웹 인터페이스에서 적합성 검사 결과를 확인할 수 있다. 





### 6-3. 검증과 검사

* 도구를 이용한 적합성 검사에서는 검출되지 않는 문제점들도 고려해야한다.
* 예 : 
  * 지나치게 큰 용량의 컨테이너 이미지를 이용해서, 클러스터 리소스를 낭비하는 문제
  * 단일 Pod 레플리카만 지정하여서 고가용성을 보장하지 못하는 디플로이먼트
  * 잠재적인 보안 위험을 가져오는 루트 권한 컨테이너 실행 등등..



* K8Guard
  * target에서 개발한 도구
  * 쿠버네티스 클러스터의 일반적인 문제를 확인, 잘못된 부분을 수정 및 알람 전송이 가능하다.
    * 문제라고 판단하는 일정한 기준을 설정할 수 있다.
    * 예 : 컨테이너 이미지가 1GiB보다 클 경우 문제라고 판단 후 경고 알림이 발생
  * 모니터링 시스템에서 수집할 수 있는 메트릭 제공
    * 모니터링을 통해 빠른 문제 탐색 및 해결이 가능하도록 한다.



* Cooper

  * 배포 전, 쿠버네티스 매니페스트를 검사하는 도구

  * 사용자가 지정한 정책을 이용하거나, 문제 사항을 플래그

  * DSL ( domain-specific language )

    * 검증 규칙과 정책을 표현하기 위한 도메인 특화 언어

    * 예 : latest 태그를 사용하는 모든 컨테이너를 차단하는 규칙 표현

      ```
      rule NoLatest ensure {
      	fetch("$.spec.template.spec.containers..image")
      		.as(:image)
      		.pick(:tag)
      		.contains("latest") == false
      }
      ```

    * 위의 규칙을 정의한 후, latest 태그가 포함된 컨테어너 이미지를 Spec에 정의한 쿠버네티스 매니페스트 파일을 검사하면 실패 메세지가 출력

  * 태그 검사 및 소스 제어에 Cooper 이용을 권장





* kube-bench
  * 쿠버네티스 클러스터가 인터넷 보안센터( CIS )에서 지정한 기준을 따르는지 감사
  * YAML 형식으로 테스트 항목을 추가 가능



* 쿠버네티스 감사 로깅
  * 활성화 시 기록 내용
    * 클러스터 API로의 모든 요청이 타임스탬프로 기록
    * 리소스와 같은 요청 세부 내용
    * 응답 기록





### 6-4. 카오스 테스팅

* 고가용성을 실제로 검증하는 방법
  * 클러스터 노드 제거 후 확인하는 것
  * 자동화된 테스트 프로세스 필요

* **카오스 몽키( chaos monkey )**
  * 상용 서비스에서 자동화된 무작위 테스트
  * 이외 비슷한 카오스 엔지니어링 도구
    * 레이턴시 몽키( Latency Monkey )
      * 네트워크 문제를 시뮬레이션 하기 위해 통신 지연 발생
    * 시큐리티 몽키( Security Monkey )
      * 알려진 보안 취약점 파악
    * 카오스 고릴라( Chaos Gorilla )
      * AWS 가용성 영역 전체를 중단



* 상용과 스테이징 환경은 다르다
  * 보통은 상용 서비스에 영향을 주지 않기위해 테스트 서버에서만 테스트 실행 
    * => 그러나 테스트 결과가 상용환경에서 동일한 결과를 보장하진 않는다!
    * => 한 번의 테스트로 상용환경을 신뢰해서는 안된다.
  * 카오스 테스트 자동화의 요점
    * 지속, 반복 실행을 통해 시스템에 대한 믿음과 신뢰를 쌓는 것
    * 새로운 취약점을 발견하고, 초기 보완한 취약점을 지속적으로 점검하는 것



* chaoskube
  * 클러스터의 Pod를 무작위 종료
    * 기본적으로는 dry-run-mode
      * 종료할 Pod만 보여주고, 실제로 종료하지는 않음
      * 비활성화하여 실제 작동시킬 수도 있다.
    * 쿠버네티스 리소스에만 포함되면, 종료를 가리지 않고 무작위 종료
  * 종료할 대상 한정
    * Pod의 label, annotationi, namspace와 날짜, 기간 등을 기준으로 삼아 종료할 Pod와 종료하지 않을 Pod를 구별할 수 있다.
  * 설치와 설정이 간단하여 카오스 엔지니어링 입문자에게 유용한 도구



* kube-monkey

  * 설정된 시각에 작동

  * 나머지 시간에 디플로이먼트 스케줄을 빌드

  * 다른 도구와 다르게 `opt-in`기반으로 작동

    * opt-in?

    * 당사자가 개인의 데이터 수집을 동의하기 전까지 데이터를 금지하는 제도

      * annotation 사용

        * 지정한 Pod를 대상으로만 kube-monkey 활성화

      * annotation 작성 예시

        ```yaml
        # 평균 무고장 시간( mean time between failure )을 이틀로 지정
        kube-monkey/mtbf: 2
        
        # 대상 디플로이먼트의 Pod를 최대 50%까지 제거 가능
        kube-monkey/kill-mode: "random-max-percent"
        kube-monkey/kill-value: 50
        ```



* PowerfulSeal
  * 오픈소스
  * 쿠버네티스 엔지니어링 도구
  * 대화형 ( interactive ) / 자율형 ( autonomous ) 두 가지 모드로 작동
    * 대화형 모드
      * 클러스터 탐색 및 수동 테스트
      * 노드, 네임스페이스, 디플로이먼트, 개별 Pod 제거 가능
    * 자율형 모드
      * 사용자 정책을 기반으로 작동
        * 테스트 대상과 제외할 대상 지정
        * 실행 시간 설정
        * 테스트 공격성 조절
      * 거의 모든 카오스 엔지니어링 시나리오 설정 가능







## 7. 유용한 쿠버네티스 도구



### 7-1. kubectl 마스터하기

* shell alias 등록

  * `.bash_profile` 파일에 별칭을 추가

    * 작성 예시 :`alias k=kubectl`
    * 작성 후 kubectl 이용 : `k get pods`

  * 이외에 다양한 명령어들을 별칭으로 저장하여 이용할 수 있다.

    * 예시 :

      ```shell
      alias kg=kubectl get
      alias kgdep=kubectl get deployment
      alias ksys=kubectl --namespace=kube-system
      alias kd=kubectl describe
      ```



* 짧은 플래그 사용하기
  * 예 :  `--namespace` 대신에 `-n` 옵션 사용



* 리소스 단축형 사용하기

  * kubectl에서 제공하는 리소스 단축형 이용

    ```
    Pod
    => po
    
    deployment
    => deploy
    
    service
    => svc
    
    namespace
    => ns
    
    node
    => no
    
    comfigmaps
    => cm
    
    serviceaccount
    => sa
    
    daemonset
    => ds
    
    persistentvolumes
    => pv
    ```





* kubectl 명령어 자동완성
  * `bash` 혹은 `zsh` 쉘을 이용하면 kubectl 명령어의 자동완성 사용 가능
  * 자동완성 활성화 명령어
    * `kubectl completion -h`
    * 이후 명령어를 Tab키로 자동완성 시킬 수 있다.
    * Tab을 두번 누르면 사용 가능한 모든 명령어를 확인
      * 플래그 목록도 동일한 방법으로 확인 가능
      * 쿠버네티스 리소스 이름도 동일한 방법으로 확인 가능



* 도움말 보기
  * `kubectl -h`
    * 사용 가능한 전체 명령어 개요 확인
  * `kubectl <command> -h`
    * 명령어 -h 옵션 시, 각 명령어에 대한 자세한 설명과 옵션, 예제 확인 가능





* 쿠버네티스 리소스 도움말 보기
  * 명령어 외에 쿠버네티스 객체에 대한 도움말도 확인 가능
  * `kubectl explain <리소스 유형>`
    * 지정한 리소스 유형에 대한 도움말
  * `kubectl explain --recursive`
    * 필드 내 필드를 재귀적으로 확인





* 더 자세한 출력결과 확인 하기
  * `-o wide`
    * Pod가 실행 중인 노드와 같은 추가 정보 확인 가능





* jq를 사용하여 JSON 데이터 다루기
  * `kubectl get` 명령어의 기본 출력 형식은 평문 ( plain text )
    * JSON 형식으로도 정보를 출력 가능
    * 명령어 이용 : `kubectl get pods -n kube-system -o json`
      * jq를 이용하여 필요한 정보만 필터링 가능
      * `sudo apt install jq` 로 설치 후 이용 가능
      * jq로 JSON 쿼리 작성 예시 :
      *  `kubectl get pods -n kube-system -o json | jq '.items[].metadata.name'`
        * 모든 Pod의 이름 출력
      * ` kubectl get pods -o json | jq '.items | group_by(.spec.nodeName) | map({"nodeName": .[0].spec.nodeName, "count": length}) | sort_by(.count) | reverse'`
        * 노드 별 실행중인 Pod 갯수 출력, 실행중인 Pod가 많은 순서대로 출력
    * jq 외에도 `jsonpath`를 이용할 수 있다.
      * `kubectl get pods -o=jsonpath={.items[0].metadata.name}`





* 오브젝트 감시하기
  * `--watch` 플래그 ( `-w` )
    * 플래그 이용 시, 리소스 상태가 변경될 때마다 업데이트 된 상태를 터미널에서 확인 가능





* 오브젝트 정보 확인하기
  * `kubectl describe`
    * 오브젝트의 더 자세한 정보를 확인하고 싶을 때 명령어 사용
    * Event 섹션의 정보
      * 컨테이너 생명 주기 각 단계에서 발생하는 에러를 기록
      * 컨테이너 문제 해결에 유용





### 7-2. 리소스 다루기

* 선언형 이외에 명령형 커맨드로도 리소스 관리가 가능하다.



* kubectl 명령형 커맨드
  * `kubectl run`
    * 지정한 컨테이너를 생성하는 deployment를 암시적으로 생성
  * `kubectl create`
    * 대부분의 리소스를 명시적으로 생성
    * 상용 클러스터에서 권장하지 않음
  * `kubectl delete`
    * 리소스 삭제
  * `kubectl edit`
    * 리소스 확인 후 수정 가능
    * 시스템에 설정된 기본 편집기에서 YAML 매니페스트 파일이 열린다.
    * 파일 수정 및 저장 후 편집기 종료 시, `apply` 명령어를 적용한 것처럼 파일 수정이 적용
    * 상용 클러스터에서 권장하지 않음



* 명령형 커맨드를 사용하지 않을 때
  * 명령형 커맨드 단점
    * 단일 소스 저장소가 없음
      * 명령을 실행한 관리자, 결과를 확인할 수 없다.
      * 명령형 커맨드의 내용은 클러스터 매니페스트에 동기화되지 않는다.
      * => 누군가 매니페스트를 적용한다면 명령형 커맨드 설정에 덮어씌어진다.





* 리소스 매니페스트 생성하기

  * 명령형 커맨드 장점

    * YAML 파일을 처음부터 생성하지 않아도 된다.

    * YAML 기본 파일 생성 명령어

      `kubectl run demo --image=cloudnatived/demo:hello --dry-run -o yaml >deployment.yaml`

      * `--dry-run` 
        * 실제 리소스를 생성하지는 않고, 생성 시 발생할 수 있는 메세지만 출력
      * `-o yaml`
        * 플래그 추가 시 리소스 매니페스트를 YAML 형식으로 출력





* 리소스 내보내기

  * `--export`플래그

    * 명령형 커맨드로 작성한 내용을 YAML 매니페스트 파일로 생성

    * 명령어 작성 예시

      1. 명령형 커맨드로 리소스 생성

         `kubectl run newdemo --image=cloudnatived/demo:hello --port=8888 --labels app=demo`

      2. YAML 매니페스트 파일로 추출

         `kubectl get deployments newdemo -o yaml --export >deployment.yaml`





* 리소스 비교하기
  * `kubectl diff`
    * 쿠버네티스 매니페스트를 적용하기 전에, 클러스터에 변경될 사항을 정확하게 명시해주는 명령어
  * 명령어 작성 예시
    * `kubectl diff -f deployments.yaml`





### 7-3. 컨테이너 다루기



* 컨테이너 로그 보기
  * 쿠버네티스 로그에는 표준 출력 / 표준 오류 스트림 포함
  * `kubectl logs`
    * 명령어로 로그 확인
    * `--tail` 플래그 사용
      * 가장 최근의 로그만 출력하도록 제한
      * 명령어 이용 예시
        * `kubectl logs -n kube-system --tail=20 <Pod 이름>`
    * `--follow` ( `-f` ) 플래그 사용
      * 실행 중인 컨테이너를 감시
      * 로그 출력이 터미널로 스트림
    * `--container`( `-c` ) 플래그 사용
      * Pod 내의 특정 컨테이너의 로그를 확인
      * 명령어 이용 예시
        * `kubectl logs metric-server -c metrics-server-nanny`





* 컨테이너 연결하기
  * 로컬 터미널을 컨테이너에 연결하여 컨테이너의 출력을 직접 확인
  * 사용 명령어 : 
    * `kubectl attach <컨테이너 이름>`



* kubespy로 쿠버네티스 리소스 검사
  * 클러스터 내 개별 리소스 상태를 관찰, 시간이 지남에 따라 어떤 일이 일어나는지 사용자에게 보여줌



* 컨테이너 포트 포워딩
  * `kubectl port-forward <pod 이름> host_port:container_port`



* 컨테이너에 명령어 실행하기
  * `kubectl exec -it <Pod 이름> /bin/sh`
  * 컨테이너에 접속하여 프로그램이 제대로 실행되고 있는지 확인
    * `exec` 명령어는 기본적으로 Pod 내의 첫번째 컨테이너에 명령어를 실행
    * 특정 컨테이너에 접속하고 싶다면 `-c` 옵션 이용



* 문제 해결을 위해 컨테이너 실행하기

  ```
  $ kubectl run demo --image cloudnatived/demo:hello --expose --port 8888
  service/demo created
  pod/demo created
  
  $ kubectl run nslookup --image=busybox:1.28 --rm -it --restart=Never --command -- nslookup demo
  Server:    10.0.0.10
  Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local
  
  Name:      demo
  Address 1: 10.0.6.169 demo.default.svc.cluster.local
  pod "nslookup" deleted
  
  $ kubectl run wget --image=busybox:1.28 --rm -it --restart=Never --command -- wget -qO- http://demo:8888
  Hello, 世界
  pod "wget" deleted
  ```

  * 예제 명령어에 쓰인 플래그 분석
  * `--rm`
    * 실행 완료 후 컨테이너 이미지가 자동 삭제
  * `-it`
    * 컨테이너를 터미널(t) 대화형(i) 옵션으로 실행
  * `--restart=Never`
    * 컨테이너 종료시, 재실행이 기본값
      * 기본값을 비활성화
  * `--command --`
    * 컨테이너가 지정한 명령어를 실행하도록 함
    * `--` 다음의 모든 값은 컨테이너 명령으로 전달



* 비지박스 명령어 사용하기

  * Busybox?

    * 하나의 실행 파일 안에 스트립 다운된 일부 유닉스 도구들을 제공하는 소프트웨어
      * 즉, 일종의 명령어 압축 모음집
      * 리눅스에서 사용되는 각각 명령어를 실행하기 위한 함수들을 최소 사이즈로 구현해놓았다고 한다.
    * 쉘 스크립트와 호환되는 가벼운 쉘인 `ash`를 사용

    



* 비지박스를 컨테이너에 추가

  * 컨테이너가 alpine과 같은 리눅스 기반 이미지로 빌드하여 쉘이 포함되어 있다면?

    * `exec` 명령어를 이용하여 쉘로 접근 가능

  * 컨테이너에 /bin/sh가 없다면?

    * 컨테이너를 빌드할 때, busybox. 파일을 복사하여 빌드하는 방법 이용

    * Dockerfile 작성 예시

      ```dockerfile
      FROM golang:1.11-alpine AS build
      WORKDIR /src/
      COPY main.go go.* /src/
      RUN CGO_ENABLED=0 go build -o /bin/demo
      
      FROM SCRATCH
      COPY --from=nuild /bin/demo /bin/demo
      COPY --from=busybox:1.28 /bin/busybox /bin/busybox
      ENTRYPOINT ["/bin/demo"]
      ```



* 컨테이너에 프로그램 설치하기
  * busybox에 없는 프로그램을 이용해야 한다면?
    * 리눅스 이미지를 실행하여 필요한 프로그램을 직접 설치하는 방안도 있다.





* kubesquash를 사용한 디버깅
  * 컨테이너 디버깅 이외에, 실제 디버거를 컨테이너 프로세스에 연결하기 위한 방안
  * 컨테이너로 디버거를 연결할 수 있도록 도와주는 도구
    * **golang** 디버거인 `dlv`를 연결하여 활용할 수 있도록 해준다.
  * http://github.com/solo-io/kubesquash
  * 주의할 점
    * kubesquash는 대상 컨테이너에서 사용하는 ls 명령어에 의존
    * 따라서, 컨테이너가 ls 명령어를 이용할 수 없는 이미지를 기반한다면, 위의 예시로 작성된 Dockerfile에서처럼 busybox 실행 파일을 복사해주어야 한다.
      * busybox 실행파일을 /bin/busybox가 아닌 /bin/ls로 복사해주면 문제없이 작동시킬 수 있다.







### 7-4. 컨텍스트와 네임스페이스

* 멀티 클러스터를 다루는 경우, kubectl  명령어는 어디에 적용되는가?
* kubectl은 명령어를 실행하는 대상 클러스터를 어떻게 구별할까?



* Context?
  * 클러스터, 사용자, 네임스페이스의 조합
    * 일종의 북마크와 같은 개념
    * 특정 클러스터와 네임스페이스로 쉽게 전환할 수 있도록 돕는다.
  * kubectl 명령어는 컨텍스트를 기준으로 실행된다.
  * `kubectl config get-contexts`
    * kubectl이 현재 알고있는 컨텍스터 목록이 출력
      * 클러스터에 인증하는 사용자 이름
      * 클러스터의 네임스페이스 정보 포함
    * 현재 kubectl이 위치한 컨텍스트 옆에는 `*` 로 표시된다.
  * `kubectl config use-context <전환할 context 이름>`
    * 다른 컨텍스트로 전환하는 명령어
  * `kubectl config current-context`
    * 현재 컨텍스트 조회
  * `kubectl config set-context `
    * 새로운 컨텍스트 생성





* kubectx와 kubens
  * kubectl의 빠른 컨텍스트 전환을 돕는 도구
  * kubectx
    * 컨텍스트 간의 빠른 스위칭이 가능
    * 저장된 모든 컨텍스트 목록 확인 가능
      * 위치한 컨텍스트는 하이라이트 표시
  * kubens
    * 네임스페이스 간의 빠른 스위칭을 도움





* kube-ps1
  * bash나 zsh 쉘을 사용할 경우
    * 유틸리티를 이용하여 이용중인 쿠버네티스 컨텍스트를 프롬프트에 표기할 수 있다.







### 7-5. 쿠버네티스 쉘과 도구



* kube-shell

  * kubectl용 래퍼
  * 각 명령어의 자동완성을 팝업메뉴로 제공
  * 대화형 쿠버네티스 클라이언트
  * https://github.com/cloudnativelabs/kube-shell

  ![img](https://camo.githubusercontent.com/6dd81f81976c3abf550dddbed8dcc1fa93d86595/687474703a2f2f692e696d6775722e636f6d2f6466656c6b4b722e676966)





* Click

  * kubectl의 대화형 버전
  * 현재 작업 중인 객체를 **기억**
  * Rust로 작성됨
  * https://github.com/databricks/click

  ![A demo gif that shows a few features](https://camo.githubusercontent.com/5530b1aad49711e3899be326c169ca16cff726fd/68747470733a2f2f696d6775722e636f6d2f6674345748634c2e676966)





* kubed-sh
  * kube-shell과 Click
    * 쿠버네티스에 대한 로컬 쉘 제공
  * kubed-sh
    * 클러스터 자체에서 쉘이 실행
  * 로컬에서 생성한 자바스크립트, 루비, 파이썬 프로그램을 쿠버네티스 deploy로 스크립트를 실행한다.
  * https://github.com/mhausenblas/kubed-sh





* stern
  * kubectl logs 명령어의 단점을 보완한 도구
  * 정규 표현식과 일치하는 모든 Pod를 스트림
    * 여러 개의 Pod를 스트림할 경우, 각 로그 메세지 앞단에 컨테이너의 이름이 함께 출력
  * `--since`
    * 플래그 사용으로 출력 결과를 최근으로 제한할 수 있음
  * kubectl과 같이 쿠버네티스 레이블-셀렉터 표현식 사용 가능





### 7-6. 쿠버네티스 도구 직접 만들기



* Go lang
  * 상용 워크플로 자동화를 위해서는 실제 시스템의 프로그래밍 언어를 사용할 것을 권장
  * client-go
    * golang에서 제공하는 클라이언트 라이브러리
    * 쿠버네티스 API에 대한 모든 권한 제공
      * pod, deployment와 같은 모든 리소스를 생성 및 삭제 가능
    * 없는 기능은 라이브러리를 사용하여 구현할 수 있다
    * https://github.com/kubernetes/client-go
* 이외에 루비, 파이썬, PHP와 같은 다른 프로그래밍 언어에서도 마찬가지로 라이브러리가 제공된다.





## 8. 컨테이너 실행하기



* 기술적인 수준에서 컨테이너의 작동 원리를 파악한다.
* 컨테이너의 관계 및 컨테이너 이미지를 쿠버네티스에 배포하는 방법을 살펴본다.
* 쿠버네티스 보안 기능을 사용해 안전한 애플리케이션 배포 방법을 살표본다.



### 8-1. 컨테이너와 Pod

* Pod란?
  * 쿠버네티스의 스케줄링 단위
    * 쿠버네티스의 모든 작업은 Pod로 실행된다.
  * 단일 / 여러 컨테이너의 그룹
    * 동일한 환경에서 실행되는 애플리케이션 컨테이너 + 볼륨으로 구성된 집합체



* 컨테이너란?
  * 실행에 필요한 모든 것을 공유하는 표준화된 소프트웨어 패키지
  * 운영체제의 관점 : 
    * 분리된 네임스페이스에 존재하는 **격리**된 프로세스
  * 컨테이너의 프로세스
    * 단일 프로세스
    * 자신의 컨테이너 자원에 완전하게 접근할 수 있도록 실행



* 컨테이너의 구성 요소
  * 일반적으로 컨테이너는 *하나의 작업*만 수행하는 것을 권장
    * 즉, 하나의 메인 프로세스만 실행
    * 대규모 프로세스의 실행은 여러 개의 컨테이너를 조합하여 통신하도록 하는 것을 권장
  * Entrypoint
    * 스크립트로 기록하여 지정
    * CMD와 비슷한 역할을 하지만, 변경 여부에 따라 달라진다.
      * Entrypoint : 지정된 내용은 컨테이너 실행 시의 명령어로 변경될 수 없음	 
      * CMD : 지정한 내용 대신, 컨테이너 실행 시 받은 인자로 대체하여 실행될 수 있다.





* Pod의 구성요소
  * Pod란?
    * 서로 통신하며 데이터를 공유하는 컨테이너의 그룹
      * Pod 내의 컨테이너는 그룹단위 스케줄 필요
      * 동일한 물리머신에서 실행 필요
  * 여러 개의 컨테이너로 구동되는 Pod 예시
    * 멤캐시드 애플리케이션
      * 같은 로컬 캐시에 데이터를 저장
      * 서비스를 위한 애플리케이션  + 데이터 저장과 검색으로 처리하는 멤캐시드 서버
    * github 블로그 애플리케이션
      * nginx와 같은 웹서버 + git 저장소에 보관된 블로그 데이터
  * Pod 내 컨테이너
    * 한 가지 목적을 위해 함께 동작하는 컨테이너





### 8-2. 컨테이너 매니페스트



* 컨테이너 매니페스트 작성법

  * 여러 개의 컨테이너 

    ```yaml
    # spec 작성 예시
    spec:
    	containers:
    	-	name: container1
    		image: example/container1
    	- 	name: container2
    		image: example/container2
    ```

    * 각 컨테이너 스펙에 name과 image 필드를 지정해주어야 한다. 



* 이미지 식별자

  * 구분 항목

    1. registry host name
       * docker.io가 기본 도커 이미지 레지스트리
       * gcr.io는 구글 컨테이너 레지스트리
       * 이외 다른 레지스트리에 저장된 이미지를 불러오려면 호스트 네임을 적어주어야 한다.
    2. repository namespace
       * default는 library 기본 네임스페이스
         * 도커에서 승인, 관리하는 공식 이미지들의 집합
    3. image repository
       * 레지스트리와 네임스페이스에서 특정 컨테이너 이미지를 식별
    4. tag
       * 동일한 이미지의 다양한 버전 식별
         * SHA 태그 : 컨테이너를 빌드하는데 사용한 소스 저장소의 특정 커밋을 식별
         * staging, production과 같은 환경을 의미

  * 이미지 이름을 제외한 나머지는 선택사항

  * 모든 구분 항목을 포함한 예시

    `docker.io/cloudnatived/demo:hello`

    * registry host name : docker.io
    * repository namespace: cloudnatived
    * image repository :  demo
    * tag : hello





* latest 태그
  * latest
    * 별도의 태그를 지정하지 않으면 지정되는 기본 태그
    * 명시적으로 태그하지 않은 이미지 중 가장 최신 이미지
    * 식별자로써 별 도움은 되지 않는다.
  * 상용 컨테이너 활용 시에는 태그 지정이 매우 중요





* 컨테이너 다이제스트
  * 태그 => 비결정론적
    * 태그 이름이 동일하더라도, 태깅된 이미지 내용은 달라질 수 있기 때문
  * 다이제스트
    * 이미지 내용을 암호화된 해시로 계산
    * 태그와 달리 유일한 값을 가짐





* 베이스 이미지 태그
  * 컨테이너 빌드를 위한 베이스 이미지 참조 시, 태그를 지정하지 않으면 latest 태그 이미지가 참조
  * 항상 태그를 참조하여 빌드하기를 권장



* 포트
  * ports 필드는 애플리케이션, 즉 컨테이너가 수신하는 네트워크 포트를 지정



* 리소스 요청과 상한
  * 각 컨테이너는 Spec의 일부로 다음과 같은 필드 지원
    * `resources.requests.cpu`
    * `resources.requests.memory`
    * `resources.limits.cpu`
    * `resources.limits.memory`
  * 개별 컨테이너별로 지정은 가능하나, 일반적으로는 Pod를 기준으로 정함
  * Pod의 리소스 요청 = Pod 내 전체 컨테이너의 리소스 요청의 합





* 이미지 풀링 정책
  * `imagePullPolicy`
    * 이미지 풀링에 대한 쿠버네티스 정책을 지정
    * `Always`
      * 컨테이너를 실행할 때마다 항상 이미지를 Pull
      * 시간과 대역폭 낭비의 가능성
    * `IfNotPresent`
      * 이미지가 노드에 없는 경우에만 이미지를 다운로드
    *  `Never`
      * 이미지를 업데이트하지 않는다.
      * 레지스트리에서 이미지를 가져오지 않는다.



* 환경변수

  * 실행 중인 컨테이너에 정보를 전달하는 것이 제한적인 경우 사용하는 방법

  * 리눅스 시스템은 모두 환경 변수에 접근 가능

  * 환경변수

    * 문자열만 지원
      * 배열, 키/값, 구조형 데이터는 지원하지 않는다.
    * 프로세스 환경변수의 전체 크기는 32KiB로 제한된다.

  * 환경변수 작성 예시

    ```yaml
    containers:
    -	name: demo
    	image: cloudnatived/demo:hello
    	env:
    	-	name: GREETING
    		value: "Hello from the environment"
    ```

    * `env` 필드에 원하는 환경변수 목록을 설정
    * env 설정은 컨테이너 이미지의 환경변수 설정값을 덮어쓴다.





### 8-3. 컨테이너 보안



* 리눅스 시스템 중, 일부 프로세스는 root 실행이 필요
* 하지만 컨테이너에서는 root 실행이 필요하지 않다.
  * root 권한이 필요하지 않을 때 root 권한으로 실행하는 행위는 **최소 권한의 법칙** 위배
    * 프로그램의 악의적인 해킹, 이용을 방지하기 위한 법칙



* root가 아닌 사용자로 컨테이너 실행하기

  * 특정 사용자로 컨테이너를 실행하도록 하는 Spec 작성 예제

    ```yaml
    containers:
    -	name: demo
    	image: cloudnatived/demo:hello
    	securityContext:
    		runAsUser: 1000
    ```

    * runAsUser 필드값
      * UID ( 사용자 식별 번호 )
      * 대부분의 리눅스 시스템에서 첫번째 일반 사용자 생성 UID 값은 1000
      * 컨테이너 이미지와, Spec의 runAsUser 필드값을 모두 지정하지 않은 채 컨테이너를 실행하면 root 권한으로 실행이 된다.
    * 두 개 이상의 컨테이너가 동일한 데이터에 접근하려면?
      * 각각의 컨테이너가 동일한 UID를 가져야 한다.





* root 컨테이너 차단하기

  * `runAsNonRoot`

    * 컨테이너가 root 사용자로 실행되는 경우를 방지하기 위한 필드값

    * True, False 값으로 구분

    * 용도

      * root가 아닌 사용자로 컨테이너를 실행하는 것을 잊은 경우
      * root로 실행하도록 설정된 third party가 실행되는 것을 방지

    * 작성 예제

      ```yaml
      containers:
      -	name: demo
      	image: cloudnatived/demo:hello
      	securityContext:
      		runAsNonRoot: true
      ```

      * 위와 같은 매니페스트 파일을 가진 컨테이너가 실행이 차단될 경우
        * Pod의 status : CreateContainerConfigError





* 읽기 전용 파일 시스템 설정하기

  * `readOnlyRootFilesystem`

    * 컨테이너의 파일 시스템 쓰기 권한 제한 필드값
    * 설정 값을 true로 한 뒤, 컨테이너에서 쓰기 활동이 이루어지면 I/O 에러 발생

  * 항상 값을 설정해 둘 것을 권장

  * 작성 예제

    ```yaml
    containers:
    -	name: demo
    	image: cloudnatived/demo:hello
    	securityContext:
    		readOnlyRootFilesystem: true
    ```

    

* 권한 상승 비활성화
  * `allowPrivilegeEscalation`
    * setuid 메커니즘 이용 시, 일시적으로 root 권한 행사 가능
    * 이를 방지하기 위한 필드값





* 캐퍼빌리티

  * 보통 유닉스, 리눅스 프로그램

    * 일반 / 슈퍼유저 두 가지 수준 권한

  * 도커 컨테이너

    * 이러한 캐퍼빌리티 설정이 관대

    * 컨테이너 캐퍼빌리티 설정 예제

      ```yaml
      containers:
      -	name: demo
      	image: cloudnatived/demo:hello
      	securityContext:
      		capabilities:
      			drop: ["CHOWN", "NET_RAW", "SETPCAP"]
      			add: ["NET_ADMIN"]
      ```

      * `drop` : 삭제되는 캐퍼빌리티
      * `add` : 추가되는 캐퍼빌리티
      * 최대한의 보안을 위해서는 다음과 같은 설정을 권장
        * `drop` : `["all"]`
        * `add` : `["필요한 캐퍼빌리티"]`

  * 캐퍼빌리티 매커니즘은 컨테이너가 root 권한으로 실행이 되더라도, 내부 프로세스 작업을 할 수 있는 수준을 제한한다.

    * => 컨테이너 수준에서 캐퍼빌리티 제거 시, 악성 프로세스가 최대 권한을 갖게 되더라도 복구하여 악용하는 것이 불가능!



* Pod 보안 컨텍스트

  * 개별 컨테이너 수준의 보안 컨텍스트 중 일부는 Pod 수준에서도 가능

  * 다음과 같은 매니페스트 파일에, Spec을 정의하면 Pod내 모든 컨테이너에 설정이 적용

    ```yaml
    apiVersion: v1
    kind: Pod
    ...
    spec:
    	securityContext:
    		runAsUser: 1000
    		runAsNonRoot: false
    		allowPrivilegeEscalation: false
    ```

    * 이러한 설정값은 컨테이너 자체 설정값을 덮어쓴다.





* Pod 보안 정책

  * `PodSecurityPolicy`

    * 개별 컨테이너, Pod 보안 설정이 아닌, **클러스터** 수준의 보안을 설정할 수 있는 리소스

    * 작성 예제

      ```yaml
      apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      metadata:
      	name: example
      spec:
      	privileged: false
      	seLinux:
      		rule: RunAsAny
      	supplementalGroups:
      		rule: RunAsAny
      	runAsUser:
      		rule: RunAsAny
      	fsGroup:
      		rule: RunAsAny
      	volumes:
      	- *
      ```

      * 이와 같은 보안 정책을 이용하면, 특수 권한을 가진 컨테이너를 차단할 수 있다.

    * 개별 Pod의 보안 설정을 직접 제어하기 어려운 대규모 인프라에서 사용하는 것을 권장





* Pod 서비스 계정

  * 서비스 계정 default : 네임스페이스에 대한 기본 서비스 계정

  * 서비스 계정 설정 방법

    * 애플리케이션에 대한 전용 서비스 계정 생성

    * 요청한 역할로 바인딩

    * 생성된 서비스 계정을 Pod에 설정

    * Pod의 spec에 작성 예시

      ```yaml
      apiVersion: v1
      kind: Pod
      ...
      spec:
      	serviceAccountName: deploy-tool
      ```





### 8-4. 볼륨



* 컨테이너의 파일 시스템
  * 각 컨테이너에서만 접근 가능한 비영구적인 자체 파일 시스템

* 쿠버네티스 볼륨 오브젝트
  * 동일 Pod 내에 있는 다른 컨테이너와 데이터 공유 기능
  * 컨테이너가 재시작 하더라도 데이터를 유지할 수 있는 기능





* emptyDir 볼륨

  * 가장 간단한 유형의 볼륨

  * 비영구적 스토리지

    * 노드에서 Pod가 실행되는 동안 유지

  * 비어있는 상태로 시작하여 데이터를 노드에 저장

  * 용도

    * 컨테이너에 추가 스토리지 프로비저닝이 필요하지만, 영구적으로 유지할 필요는 없을 때
    * 컨테이너가 다른 노드에 스케줄되어도 상관없는 경우
    * Pod 내 컨테이너 간 파일 공유가 필요하지만 데이터를 장기간 보관할 필요는 없을 때

  * volume 필드 작성 예제

    ```yaml
    apiVersion: v1
    kind: Pod
    ...
    spec:
    	volume:
    	-	name: cache-volume
    		emptyDir: {}
    	containers:
    	-	name: demo
    		image: cloudnatived/demo:hello
    		volumeMounts:
    		-	mountPath: /cache
    			name: cache-volume
    ```

    * volume 내 name과 emptyDir 정의
      * Pod 내 모든 컨테이너에 cache-volume을 마운트하여 사용 가능
    * `/cache` 경로의 모든 것은 볼륨에 기록되며, 동일한 볼륨을 마운트한 다른 컨테이너에서도 확인할 수 있게 된다.

  * 주의할 점

    * 쿠버네티스는 디스크 쓰기 작업에 Lock을 걸지 않는다.
      * 쓰기 작업이 중복될 경우 데이터가 손상될 수 있음
      * 자체적은 Lock을 구현하거나, Lock을 지원하는 볼륨 유형을 사용해야 한다.





* 퍼시스턴트 볼륨

  * 데이터 베이스와 같은 영구적인 데이터 보관이 필요한 애플리케이션에 적합

    * 데이터 베이스는 쿠버네티스에서 실행은 권장하지 않음
    * 클라우드 서비스에서 제공하는 데이터 베이스 서비스 이용을 추천..

  * `PersistentVolume`

    * 영구적인 볼륨 리소스
    * 클라우드 서비스 업체마다 사용방법이 다르다.

  * 퍼시스턴트 볼륨을 사용하는 가장 유연한 방법

    * `PersistentVolumeClaim` 객체 사용

      * 볼륨 유형과 퍼시스턴트 볼륨의 크기를 요청
      * 예 : 쓰기가 가능한 10GiB의 고속 볼륨

    * 리소스 매니페스트 작성 예시

      ```yaml
      volumes:
      -	name: data-volume
      	persistentVolumeClaim:
      		claimName: data-pvc
      ```

      * 이러한 매니페스트 작성을 통해  Pod에서 클러스터에 있는 PersistentVolume 풀을 생성
      * 동적 프로비저닝을 이용, 
        * 리소스 객체가 마운트 될때 적절한 스토리지 공간을 자동으로 프로비저닝, Pod에 연결





### 8-5. 재시작 정책



* 종료된 Pod가 재시작되는 설정값

  * Always

  * OnFailure

    * 컨테이너가 비정상 종료된 경우에 재시작

  * Never

  * Pod가 작업 후 재시작하기를 원치 않으면 Job으로 실행하는 것을 권장

  * 리소스 매니페스트 작성 예시

    ```yaml
    apiVersion: v1
    kind: Pod
    ...
    spec:
    	restartPolicy: OnFailure
    ```





### 8-6. 이미지 풀 시크릿



* 이미지 풀링을 할 때, 사설 레지스트리를 이용하고자 한다면?
  * 인증할 수 있는 자격 증명을 쿠버네티스에게 어떻게 부여하는가?



* `imagePullSecrets`  필드값 이용

  1. 레지스트리 자격 증명을 시크릿 오브젝트에 저장

  2. 매니페스트 파일에 시크릿 객체를 Spec에 정의

     ```
     apiVersion: v1
     kind: Pod
     ...
     spec:
     	imagePullSecrets:
     	-	name: registry-creds
     ```

  * 매니페스트 이외에, 서비스 계정에 추가할 수도 있다. 
    * 서비스 계정을 사용하여 생성한 모든 Pod에는 레지스트리 자격 증명이 자동으로 추가









## 9. Pod 관리하기









