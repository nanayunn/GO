[TOC]



## 1. 소프트웨어 세상의 세 가지 혁명



1. 클라우드 혁명
2. 데브옵스의 탄생
3. 컨테이너



* => 3가지 혁명의 결과로, *클라우드 네이티브* 소프트웨어 환경이 만들어짐



### 1. 클라우드 혁명

* "클라우드"

  1. 물리적인 컴퓨터 자원을 구매하는 것 대신, *컴퓨팅 리소스*를 구매하는 방식

  *  => 원격 컴퓨팅 파워가 중요해진 시대

  2. 구축과 업그레이드가 필요하지 않은 IaaS( Infra as a Service )

  * 클라우드 인프라의 상품화
  * => 개발과 운영의 통합성을 불러옴



### 2. 데브옵스

> 소프트웨어의 개발(Development)과 운영(Operations)의 합성어
>
> * 소프트웨어 개발자와 운영조직 간의 소통, 협업 및 통합을 강조하는 개발 환경이나 문화 ( 상호 의존적 대응 )
> * 소프트웨어 제품과 서비스를 빠른 시간에 개발 및 배포하는 것을 목적으로 한다.



* 과거
  * 개발자는 소프트웨어 작성
  * 운영자는 상용환경에서 소프트웨어를 실행 및 관리
  * => 중복되는 업무가 거의 없었다!
  * '시스템' = 개인이 개발한 소프트웨어



* 클라우드 혁명 이후..?
  * 복잡한 분산 시스템
  * 상호의존적인 클라우드 시스템
  * '시스템' = 밀접하게 연결되고, 상호의존적인 다양한 소프트웨어..
    * 예 : 사내 소프트웨어, 클라우드 서비스, 네트워크 리소스, 로드 밸런서, 모니터링...



* 데브옵스에 대한 정의는 제각각
  * 중요한 포인트는, 데브옵스는 기술적인 문제 보다는 주로 조직적인 문화와 더 관련이 깊다는 것이다.



* 비즈니스 이점
  * 데브옵스의 장점?
    * 클라우드 자동화와 실행으로 릴리스 주기를 단축
    * => 소프트 웨어의 품질 향상 + 소프트웨어가 상용 환경에서 상시 운영!
  * 데브옵스의 핵심 목표
    * 속도, 민첩성, 협업, 자동화, 소프트웨어 품질



* 코드형 인프라

  * IAC ( Infastructure As Code )
    * 클라우드 환경에서 하드웨어는 '클라우드'에 존재
    * => 어떤 의미로 본다면, 모든 것이 소프트웨어
    * => 소프트웨어 형식으로 인프라를 자동 공급

  

* 정리
  * 클라우드 + 협업의 필요성 + 인프라의 소프트웨어화
  * => 소프트웨어의 운영 문제 
    * 소프트웨어를 배포하는 방법 중 이식성이 높은 방법은 무엇이 있을까?



### 3. 컨테이너 등장

* 소프트웨어를 배포하려면?
  * 소프트웨어 + 의존성( 소프트웨어와 관련된 모든 것 ) + 구성( 소프트웨어를 사용 가능한 서비스로 만들어주는 것 )
    * 의존성 : 라이브러리, 인터프리터, 컴파일러,,
    * 구성 : 설정, 라이선스 키, 데이터베이스 패스워드



* 배포 과정의 변화

  1. 코드 구성 관리 시스템

  * 퍼핏, 앤서블 ...
  * 소프트웨어를 설치, 실행, 구성, 업데이트
    * 단점 : 언어에 종속되며, 의존성 문제 완전히 해결 불가

  2. 옴니버스 패키지

  * 애플리케이션에 필요한 모든 것을 *단일 파일*로 구성
    * 소프트웨어, 구성, 소프트웨어 컴포넌트, 컴포넌트의 구성, 컴포넌트의 의존성..

  3. 가상 머신 이미지

  * 애플리케이션 실행을 위해 필요한 전체 컴퓨터 시스템
    * 느린 다운로드와 배포, 비효율적인 성능과 리소스

  4. 컨테이너

  * 애플리케이션의 실행에 필요한 것은 모두 *이미지파일*에 저장
    * 레이어 방식의 파일 구성
  * 가상화의 오버헤드 없이, 실제 CPU에서 실행





* 컨테이너의 특징
  * **플러그 앤 플레이 애플리케이션**
  * 배포 및 패키징, 
  * 생성된 컨테이너 이미지는 재사용이 가능
    * 이것을 이용하여 스케일링, 리소스 할당이 가능
  * 컨테이너의 의존성은 운영체제 커널뿐..



* 컨테이너 오케스트레이션
  * 오케스트레이터
    * 다양한 머신을 하나의 클러스터로 결합하도록 설계된 소프트웨어의 한 종류
      * 클러스터 => 사용자의 입장에서, 컨테이너를 실행할 수 있는 일종의 컴퓨터
    * 일반적으로 스케줄링, 오케스트레이션, 클러스터 관리를 담당
    * 오케스트레이션 != 스케줄링
      * 오케스트레이션 :
        * 서비스의 공통적인 목표를 위해 서로 다른 역할을 조정하고 나열
      * 스케줄링 : 
        * 사용 가능한 리소스를 관리하고 가장 효율적으로 실행할 수 있는 워크로드를 할당하는 것을 의미
          * 지정된 시간에 예약된 작업을 실행하는 스케줄링 의미와 혼동하면 **안된다**





* 쿠버네티스
  * 구글이 개발한 컨테이너 오케스트레이터
  * '보그'에서 시작하여 '쿠버네티스' 오픈 소스 프로젝트로 발전
  * 쿠버네티스가 특별한 이유?
    * 시스템 관리자의 역할
      * 자동화, 장애 조치, 모니터링..
      * => 전통적인 시스템 관리 작업을 대신해주어서, 팀이 보다 핵심적인 작업에 집중할 수 있도록 도움
    * 다양한 기능과 API, 확장하는 생태계
    * 간편한 배포 작업
      * 롤링 업데이트로 무중단 배포
      * 오토 스케일링 지원
        * 클라우드 서비스 업체와 관계없이 사용 가능
        * 서비스 업체가 제공하는 기능에 리소스를 적절하게 매핑
          * 각 클라우드 업체의 세부 사항을 추상화하여 각각에 맞는 기능을 제공
          * => 소프트웨어 **실행 방식**에 대한 이식 가능성
  * 다만, 만능은 아니다 : 
    * 부적합한 애플리케이션 존재
      * 예 :  데이터 베이스
      * 데이터 베이스 레플리카는 서로 호환되지 않는다..
    * 서버리스 플랫폼에서 실행할 수 있는 애플리케이션들..





* 클라우드 함수와 펀테이너
  * 클라우드 함수 : 
    * FaaS( Function as a Service )
    * 클라우드 위에 함수를 등록하고 트리거를 걸어 사용
      * 즉, 이벤트가 발생하면 등록해놓은 함수가 동작
      * 함수 내용 작성을 제외한 모든 것을 클라우스 서비스 업체가 처리!
  * 펀테이너
    * 클라우드 함수와 컨테이너의 혼용
    * 예 : 쿠버네티스 클러스터에서 클라우드 함수를 실행하는 것
  * Knative
    * 컨테이너와 클라우드 함수를 모두 포함하는 쿠버네티스 기반 소프트웨어 제공 플랫폼



* 클라우드 네이티브

  * 클라우드 환경 전체에 지속적인 개발과 자동화된 관리 환경을 제공하기 위해 특별히 설계된 애플리케이션

  * 클라우드 네이티브 환경의 특징

    * 자동화

      * 일반적인 표준, 형식, 인터페이스를 다라 자동으로 애플리케이션을 배포

    * 유비쿼터스와 유연성

      * 컨테이너화된 마이크로서비스는 디스크와 같은 물리적 자원이나, 실행되는 컴퓨팅 노드에 대한 특정 지식과 분리!
      * => 클러스터 간의, 노드 간의 이동을 자유롭게 할 수 있다.

    * 탄력성과 확장성

      * 분산된 환경을 기반으로 고가용성을 보장

    * 역동성

      * 사용 가능한 리소스를 최대한 활용, 컨테이너 스케줄링

    * 관측가능성

      * 분산 시스템의 핵심 요구사항
      * 모니터링, 로깅 등으로 시스템의 작동 상태와 장애 상태를 이해하는 데 도움을 줌

    * 분산

      * 분산 마이크로 서비스
      * 합치면 애플리케이션

      



* 운영의 미래
  * 데브옵스로 인해 구분되지 않는 개발과 운영 업무
  * DPE( developer productivity engineering )
    * 운영 팀을 대신
    * 개발자가 더 나은 환경에서 개발 업무를 할 수 있도록 도움을 제공







## 2. 쿠버네티스 첫걸음



### 2-1. 첫번째 컨테이너 실행하기

* 클라우드 네이티브 개발에서 중요한 개념, *컨테이너*
* 컨테이너를 빌드 및 실행하는 데 필요한 핵심 도구, *Docker 도커*



* 실습으로 진행되는 Docker

  * `docker version`

    * 사용자가 이용하는 도커의 버전이 출력
    * 정상적으로 설치 되었을 때 확인 가능

  * `docker container run -p 9999:8888 --name hello cloudnatived/demo:hello`

    * 책에서 제공하는 github 데모 파일로 컨테이너 생성 테스트
    * 실행 후, `http://localhost:9999/`에 접속하면 명령어 실행 결과를 확인할 수 있다.

  * `git clone https://github.com/cloudnativedevops/demo.git`

    * 전체 소스코드를 git에서 다운받아 살펴본다.
    * Dockerfile, main.go, go.mod로 이루어짐
      * go lang에 대한 간단한 소개
        * 클라우드 네이티브 개발에 유용한 프로그래밍 언어
        * Go로 개발된 다양한 오픈소스 프로젝트 ( 도커, 쿠버네티스, 테라폼.. )
      * 데모 애플리케이션의 동작 분석
        * go의 handler() 함수가 핵심
        * HTTP 요청을 처리하여 HTTP 서버가 8888포트에서 서비스 되도록 실행

  * `docker image build`

    * 컨테이너의 이미지 빌드
      * Dockerfile이란 텍스트 파일을 이용하여 구현 내용을 입력 받는다.
      * 보통 dockerfile은 **베이스 이미지**라고 불리는 시작 이미지를 가져와 변환, 새로운 이미지로 저장한다.

  * Dockerfile 이해하기

    * 테스트용으로 제공된 Dockerfile은 main.go 파일을 go lang 컨테이너에서 컴파일 한 뒤, **scratch**라는 **비어있는 컨테이너 이미지**에 컴파일한 내용만 복사하여 컨테이너 이미지를 최소한의 크기로 작성한다.

    * ```dockerfile
      From golang:1.11-alpine AS build
      
      WORKDIR /src/
      COPY main.go go.* /src/
      RUN CGO_ENABLED=0 go build -o /bin/demo
      
      FROM scratch
      COPY --from=build /bin/demo /bin/demo
      ENTRYPOINT ["/bin/demo"]
      ```

    * 이미지를 최소한의 크기로 작성하는 이유?

      * 배포에 편리함
      * 컨테이너에 올라간 프로그램이 적을수록 보안문제를 일으킬 수 있는 공격 지점이 줄어듦

  * `image build`명령어 추가 옵션

    * `-t` : 이미지에 이름을 지정
      * ex ) `docker image build -t myflasktest .`

  * `docker run` 명령어 추가 옵션

    * `-p` : 실행하는 컨테이너 내부 포트를 호스트 컴퓨터 포트와 연결
      * ex ) `docker container run -p HOST_PORT:CONTAINER_PORT`
      * HOST_PORT에 대한 모든 요청은 CONTAINER_PORT로 자동 전달되게 됨

  * 컨테이너 registry

    * git과 같은 저장소 개념으로, pull push로 이미지를 다운로드 및 업로드 할 수 있다.
    * 기본 registry는 Docker hub이고, 회원가입해서 자유롭게 쓸 수 있다. 
      * 로컬에서 사용하려면 docker login 필요
    * `push` 
      * 로컬 이미지를 registry에 push 하려면 DOCKER_ID/IMAGE_NAME 형식으로 이름을 정해야함
      * --tag 옵션으로 이름 뒤에 version을 명시할 수 있다.





### 2-2. 쿠버네티스 실행하기

* 앞서 빌드한 docker 이미지를 이용하여 쿠버네티스 환경에서 실행
* 이미지는 `--imag=cloudnatived/demo:hello` 이미지를 이용하여도 좋다.



* kubectl과 minikube를 설치하여 진행되는 실습

  * `kubectl run demo --image=cloudnatived/demo:hello --port=9999 --labels app=demo`

  *  결과 : `deployment.apps "demo" created`

    * `run` 대신에 `kubectl run --generator=run-pod/v1  `, 혹은 `kubectl create ~` 명령어를 이용할 것.

  * 위의 명령어에서는 서비스가 노출될 로컬의 포트만 설정해주었다. (`--port=9999`)

    * 따라서, 별도의 포트 포워딩이 필요

    * `kubectl port-forward deploy/demo 9999:8888`

    * 결과 : `Forwarding from 127.0.0.1:9999 -> 8888`

      `Forwarding from [::1]:9999 -> 8888`

    * 이후, 새로운 터미널 창에서 `curl http://localhost:9999/`로 HTTP 요청을 보내면, 이에 대한 응답이 출력된다.







## 3. 쿠버네티스 구축하기



### 3-1. 클러스터 아키텍쳐



* 클러스터 구성요소

  * 컨트롤 플레인

    * 클러스터의 두뇌 역할
    * 컨테이너 스케줄링, 서비스관리, API 요청 처리 등의 작업 수행
    * **마스터 노드**에서 수행
    * 구성요소
      * `kube-apiserver`
        * 컨트롤 플레인의 프론트 엔드 서버로, API 요청을 처리
      * `etcd`
        * 쿠버네티스와 관련된 모든 정보를 저장하는 데이터 베이스
        * 정보의 예시 :어떤 노드가 존재하고, 클러스터에 어떤 리소스가 존재하는지
      * `kube-scheduler`
        * 새로 생성된 Pod를 실행할 노드를 결정
        * Pod의 생성 및 
      * `kube-controller-manager`
        * deployment와 같은 리소스 컨트롤러를 관리
      * `cloud-controller-manager`
        * 클라우드 리소스와 관련한 컨트롤러
        * 로드 밸런서나 디스크 볼륨과 같은 리소스를 관리

  * 노드 ( 워커 노드 )

    * 클러스터 내에서 사용자의 워크로드 실행
    * 구성요소
      * `kubelet`
        * 노드에 예약된 워크로드를 실행하기 위해 컨테이너 런타임을 관리
        * 상태를 모니터링
      * kube-proxy
        * 네트워크 트래픽을 라우팅
          * Pod-Pod 통신, 혹은 Pod와 외부 네트워크 통신
      * 컨테이너 런타임
        * 컨테이너의 시작과 중지를 맡음
        * 컨테이너 간 통신을 처리
        * 일반적으로는 *Docker*가 사용된다.

  * 서로 다른 컴포넌트를 실행하는 것 이외에, 마스터 노드와 워커 노드의 본질적인 차이점은 없다.

  * 클러스터의 구성 이점

    * 고가용성( HA - high availabilty )
      * 여러 개의 마스터 노드로 구성된 컨트롤 플레인
        * 일부 마스터 노드에 문제가 생겨도, 다른 마스터 노드가 역할을 대신
          * 네트워크 파티션 상황에서도 컨트롤 플레인의 특성으로 처리 가능
          * split-brain 개념에 대해 추가 이해
            * https://bryan.wiki/290
      * etcd 데이터 베이스 복제
        * 여러 노드에 걸쳐 복제하여 마찬가지로 고가용성을 보장

  * 컨트롤 플레인 장애 

    * 클러스터의 오작동을 일으킬 수 있다.

    * 장애가 발생했을 때, 클러스터가 정족수( 쿼럼 - quorum )을 유지할 수 있도록 충분한 **수**의 마스터 노드 필요

      * 예 : 상용 클러스터의 경우, 최소 3개의 마스터 노드 필요

      > 정족수 ? 
      >
      > * 정의 : 여러 사람의 합의로 운영되는 의사기관에서 의결을 하는데 필요한 최소한의 참석자 수
      >
      > * 클러스터에서 정족수의 개념 : 앞서 언급된 **split-brain** 발생을 방지하기 위한 알고리즘의 개념
      >
      >   * 마스터 노드에 장애가 발생 시, 각 노드로부터 투표를 받아 핵심 마스터 노드를 결정
      >
      >   * 쿼럼은 쿼럼 서버 - 쿼럼 클라이언트로 구성
      >
      >     * 쿼럼 서버 : 전체 노드와 통신하며 노드에 장애가 발생 시 중재하여 클러스터의 데이터 손실을 방지
      >
      >     * 쿼럼 클라이언트 : 클러스터의 각 노드에 설치되는 에이전트
      >
      >       * 쿼럼 서버와 통신하며 노드의 상태를 판단하는데 사용된다.
      >
      >       http://www.mantech.co.kr/quorum/

  * 워커 노드 장애

    * 워커 노드 장애는 그다지 중요한 문제는 아님
      * 이유 ? 
        * 컨트롤 플레인이 정상 작동한다면, 워커 노드의 장애를 감지하고 재조정할 것 이기 때문
    * 드물게 가용성 영역에 장애가 발생하기도 한다.
      * 따라서 워커 노드를 가용 영역 여러 곳에 분산시키는 것이 좋다.





### 3-2. 쿠버네티스 자체 호스팅



* 구축과 구입의 갈림길
  * 자체 호스팅은 인력, 기술, 유지보수 등등 많은 자원을 요구
  * 구축의 개념은 단순 설치 및 실행과는 다른 개념이다.
  * 호스팅하게 된다면 고려해야하는 개념
    * 고가용성
    * 안전성
    * 형상 관리
    * 보안성
    * 유지 보수의 체계성
* 자체 호스팅은 지속적인 관리가 필요
  * 모니터링
  * 장애 발생 시 해결 조치
  * 클러스터 최신 버전으로 업데이트
* 도구가 모든 것을 해결하지는 못한다.
  * 대부분의 도구는 간단한 문제만 해결 가능
  * 강력한 도구는 비싸거나 공개되지 않는다.
* 쿠버네티스는 어렵다
  * 간결한 설계로 매우 복잡한 상황을 처리하기 때문에, 이해 하고자 한다면 어려운 소프트웨어가 될 수 밖에 없다.
* 관리 오버헤드



* 따라서, 자체 호스팅 보다는 관리형 서비스가 더 효율적인 관리 방안이 될 수 있다.





### 3-3. 관리형 쿠버네티스 서비스



1. 구글 쿠버네티스 엔진

   * 구글은 GKE( Google Kubernetes Engine )을 구글 클라우드 플랫폼에 통합하여 제공

   * 장점

     * 고가용성
       * 고가용성을 보장해주는 서비스들이 다수 있고, 구글 클라우드의 다른 서비스들과 완벽하게 통합되어있다.
       * 예 : 멀티존 클러스터, 지역 클러스터
     * 클러스터 오토 스케일링

     

2. 아마존 일래스틱 쿠버네티스 서비스

   * 클러스터 인프라 뿐만 아니라 마스터 노드에 대해서도 비용을 청구

   

3. 애저 쿠버네티스 서비스

   * 구글의 GKE와 같은 경쟁사의 기능을 대부분 제공한다.
   * 웹 인터페이스나 애저 az 명령줄 도구를 사용하여 클러스터를 생성 가능
   * 워커 노드 수를 기준으로 비용이 청구



4. 오픈시프트
   * 서비스형 플랫폼(Paas) 제품으로, 관리형 쿠버네티스 서비스 이상의 역할
   * 전체 소프트웨어 개발 수명 주기를 관리하는 것을 목표로 함
   * 모든 환경에서 단일 쿠버네티스 클러스터를 생성 가능
     * 베어 메탈 서버, 가상 머신, 사설 클라우드, 퍼블릭 클라우드..
     * 다양한 인프라를 보유한 기업에게 유리



5. IBM 클라우드 쿠버네티스 서비스
   * 바닐라 쿠버네티스 클러스터 구축 가능
   * 간단하고 직관적
   * 딱히 차별화되는 기능은 없음



6. VM웨어 PKS
   * 헵티오를 인수하여 VM웨어 패키지에 통합 제공하고 있다.
     * 헵티오?
       * 쿠버네티스 관리 스타트업으로, 쿠버네티스 초기 개발자 2명이 공동 창업
       * 헵티오 인수를 통해, VM웨어는 자체 쿠버네티스 배포판을 출시







### 3-4. 턴키형 쿠버네티스 솔루션

> 턴키란?
>
> * turnkey 방식 또는 Design -Build
> * 구매자가 제품을 바로 사용할 수 있도록 생산자가 인도하는 방식
>
> 목적 : 
>
> * 턴키형 솔루션은 웹 브라우저에서 버튼만 클릭하면 바로 사용 가능한 상용 쿠버네티스 클러스터 제공을 목표로 하고 있다.



1. 스택포인트
   * 자체 호스팅과 관리 서비스 사이의 절충안
   * 웹 기반에서 쿠버네티스를 프로비저닝 및 관리 가능
   * 사용자의 퍼블릭 클라우드 인프라에서 워커 노드를 실행하기 좋음
   * 장점 :
     * 무제한 노드 클러스터
     * 멀티 클라우드에 걸친 클러스터 페더레이션
     * => 페더레이션 기능은 현재 불안정하다고 판단, 가이드 상에서 deprecated(사용 중단) 기능으로 분류



2. 컨테이너십 쿠버네티스 엔진
   * 퍼블릭 클라우드에서 쿠버네티스를 프로비저닝하기 위한 또 다른 웹 기반 인터페이스
   * 클러스터에 요구되는 거의 모든 사항을 사용자가 지정 가능







### 3-5. 쿠버네티스 설치 프로그램



1. kops
   * 쿠버네티스 클러스터의 자동 프로비저닝을 위한 명령줄 도구
   * 쿠버네티스 프로젝트의 일부로, AWS 전용 도구로 오랫동안 사용되어 왔다.
   * 상용 쿠버네티스 배포에 적합
     * 고가용성 클러스터 구축을 지원
     * 선언적 구성을 사용
     * 클라우드 자원 프로 비저닝, 스케일링, 노드 재조정, 업그레이드...



2. Kubespray
   * 이전 이름 : Kargo
   * 쿠버네티스 산하 프로젝트
   * 상용 클러스터를 쉽게 배포하는 도구
   * 온프레미스 베어 메탈 서버에 쿠버네티스를 설치하는 것에 중점



3. TK8
   * 테라폼( 클라우드 서버 생성용 ) + kubespray( 쿠버네티스 설치용 )을 모두 활용하는 명령줄 도구
   * 쿠버네티스 클러스터 프로비저닝을 위해 사용
     * 쿠버네티스 구축 뿐만 아니라, 추가 기능 설치를 제공
     * 추가 기능의 예 : 제이미 클러스터( 부하 테스트 ), 프로메테우스( 모니터링 ), 예거( 추적 ), 헬름( 쿠버네티스 패키징 ), 젠킨스 X( CI/CD를 위한 도구 ) ...
   * Go로 작성



4. 켈시 하이타워의 튜토리얼
   * 켈시 하이타워의 [ Kubernetes The Hard Way ] 튜토리얼
     * 클러스터 수동 설치 설명 자료
     * 구축 과정의 복잡한 상태를 잘 보여주는 것으로 유명
     * 내부적으로 쿠버네티스가 어떻게 작동하는지 이해하고자 할 때 따라하기 좋은 예제



5. kubeadm
   * 쿠버네티스에서 제공하는 도구
   * 쿠버네티스 클러스터를 최적으로 설치하고 관리할 수 있도록 도움
   * 클러스터에 인프라를 프로비저닝 하지 않는다!
     * => 베어 메탈 서버나 클라우드 인스턴스에 쿠버네티스를 설치할 때 적합



6. Tarmak
   * 쿠버네티스 클러스터의 수명 주기를 관리하는 도구
   * 클러스터 노드의 수정, 업그레이드 작업을 도와 안정성 확보
     * 노드 수정 시, 대부분의 도구는 노드를 **교체**
     * Tarmak은 교체 과정 없이 노드 그대로 수정, 업그레이드
   * 내부적으로 테라폼 이용
     * 클러스터 노드 프로 비저닝
   * 퍼핏
     * 노드의 설정 관리



7. 랜처 쿠버네티스 엔진
   * 간단하고 빠른 쿠버네티스 설치 프로그램
   * 노드 프로비저닝 제공 X
     * 클러스터 구축 전 직접 도커를 설치해야함
     * 컨트롤 플레인의 고가용성 지원해준다.



8. 퍼핏 쿠버네티스 모듈
   * 쿠버네티스와 관련하여 컨트롤 플레인과 etcd의 고가용성 지원을 포함한 쿠버네티스 설치 및 설정 기능 제공



9. kubeformatioin
   * 온라인 쿠버네티스 구성 도구
   * 웹 인터페이스를 통해 클러스터 옵션을 설정할 수 있다.





### 3-6. 구입할 것이냐, 구축할 것이냐



* 적게 실행하는 소프트웨어 철학
  * 표준 기술 이용하기
  * 필요할 땐 아웃소싱
  * 지속적인 경쟁 우위 창출하기



* 클라우드 기반이 아닌, 쿠버네티스 자체를 대상으로 애플리케이션과 자동화를 고려하여 설계한다면 업체 종속에서 자유로울 수 있다.



* 구축하게 되었을 때
  * 표준 쿠버네티스 설치 도구를 이용하는 것이 좋다
  * 반드시 퍼블릭 클라우드 업체에 인프라를 적용할 필요는 없다.





### 3-7. 클러스터가 없는 컨테이너 서비스

* 클러스터리스
  * 컨테이너 워크로드 실행의 오버헤드를 최소화
  * 내부적으로 클러스터가 동작하지만, kubectl과 같은 도구로 접근할 수 없다.
  * 몇 가지 매개변수를 지정하면, 서비스가 나머지 작업을 수행
    * 매개변수 : 컨테이너 이미지, 애플리케이션의 CPU, 메모리 요구사항 ...
    * 서비스 : 애저 컨테이너 인스턴스, 아마존 파게이트



1. 아마존 파게이트
   * VM 대신 컨테이너를 제공
   * 아마존의 ECS와 달리, 클러스터 노드를 프로비저닝하고 컨트롤 플레인에 연결하지 않아도 된다.
     * 컨테이너 이미지를 실행하기 위한 명령어만 정의, 실행하면 된다.
   * 적합한 용도
     * 단순하고 독립적이며, 장기간 실행되는 계산 작업
     * 다른 서비스와의 통합이 필요하지 않은 배치 작접
     * 수명이 짧은 컨테이너를 빌드할 때
     * 워커 노드 관리의 오버헤드가 클 때



2. 애저 컨테이너 인스턴스
   * 파게이트와 비슷
   * 애저 쿠버네티스 서비스와의 통합도 제공
     * 예 :  트래픽이 급증한 경우, 임시로 Pod를 프로비저닝 하여 제공







## 4. 쿠버네티스 오브젝트 다루기



### 4-1. Deployment



* 관리와 스케줄링

  * 디플로이먼트 오브젝트

    * 각 프로그램을 관리하기 위해 생성

    * 프로그램에 대한 정보를 기록

      * 컨테이너 이미지 이름, 실행할 레플리카 수, 컨테이너 실행에 필요한 정보

    * 컨트롤러에 의해 관리된다.

      * 디플로이먼트 리소스에 정의된 'Spec' 내용에 맞게 리소스가 작동하는지 확인
      * 지정된 레플리카 수를 기준으로, 레플리카 수가 그 이상이거나 이하면 즉시 정해진 기준 숫자에 맞게 리소스를 관리한다.
        * deployment를 정의하는 YAML 파일에 레플리카 수가 정의되어 있다면,
        * 레플리카를 종료하거나 제거하더라도 계속 재시작을 하게 된다.
        * =>  쿠버네티스는 *재시작*을 기본 작동으로 수행

    * 관리 순서도 : 

      * `컨트롤러 => 디플로이먼트 오브젝트 => 레플리카셋( ReplicaSet ) => 레플리카 객체( Pod )`

      

* 디플로이먼트 조회하기
  * `kubectl get deployments`
    * 현재 네임스페이스에서 활성화된 모든 deployment 를 확인할 수 있다.
  * `kubectl describe deploymets/<deployment의 NAME>`
    * 특정 deployment의 상세 정보를 확인하기 위한 명령어
    * deployment에는 Pod를 생성하기 위한 Pod Template 정보가 함께 담겨있다.





### 4-2. Pod



* Pod란?
  * 하나 이상의 컨테이너 그룹으로 구성된 쿠버네티스 오브젝트
  * Pod의 Spec에는 컨테이너 목록이 존재



* Deployment에서 컨테이너를 개별적으로 관리하지 않는 이유?
  * 컨테이너 집합으로 스케줄링 되는 경우가 있기 때문
    * 이때, 동일 노드에서 실행되며, 자원 공유가 필요
  * Pod를 실행할 때,
    * Pod 스펙을 참고하여, Pod는 특정 컨테이너와 함께 실행되어야 함을 **선언**





### 4-3. ReplicaSet



* ReplicaSet

  * 동일한 Pod 집합이나 레플리카들을 관리하는 오브젝트
  * 관리의 기준은 Spec에 상세된 레플리카 수를 기준으로 한다.

  * 디플로이먼트 오브젝트에 의해 컨트롤된다.
    * 사용자가 직접 replicaset을 다루는 경우는 드물다.







### 4-4. 조정 루프



* 조정 루프
  * 쿠버네티스 컨트롤러는 각 리소스에서 지정한 "상태( Spec )"와 지속적으로 비교, 동기화하기 위한 작업을 수행
  * 의도한 상태( Spec )과 실제 상태를 일치하기 위한 조정 작업을 영원히 반복



* 작동 예시 
  * 디플로이먼트에 Pod가 "항상 실행"되어야 한다고 정의했다면, 
  * => Pod를 직접 종료, 제거하더라도 이것을 운영자의 실수라고 판단하고 새로운 Pod를 실행한다.
  * => 실제로 이것을 감지, 교체 작업을 진행하는 것은 노드 내의 kubelet
    * 종료된 Pod가 재 스케줄링되는 것





### 4-5. 쿠버네티스 스케줄러



* 스케줄러란?
  * 디플로이먼트가 Pod를 생성하고, 쿠버네티스가 요청된 Pod를 실행하는 **과정을 책임지는 컴포넌트**



* 역할 : 
  * 대기열에서 스케줄링 되지 않은 Pod를 탐색
  * 탐색된 Pod를 배치 및 실행하기 적합한 Node를 탐색
  * => 실행은 언제..?
    * Pod가 노드에 스케줄링되어 할당되면, 노드 내의 kubelet이 컨테이너를 실행





### 4-6. YAML 형식의 리소스 매니페스트



* 쿠버네티스는 근본적으로 **선언형** 시스템
  * Spec을 선언 후, 수정사항이 생겼을 때 수정하면 쿠버네티스가 변경 사항을 처리





* 리소스는 데이터다
  * 쿠버네티스의 리소스는 모두 내부 데이터 베이스에 기록
  * => 조정 루프는 이러한 데이터 베이스 기록 내의 변경사항을 감시, 이에 따라 동작
  * 명령어 이용보다, 리소스 **매니페스트**를 생성, 수정을 권장
    * 매니페스트? 
      * 리소스에 대해 '의도한 상태'의 스펙
    * kubectl apply 명령어를 이용하여 매니페스트를 적용하게 됨
      * apply : 선언형 명령어
        * 쿠버네티스는 선언형 인프라 코드 시스템이므로 보다 활용하기 좋은 명령어



* 디플로이먼트 매니페스트
  * 일반적으로 YAML 파일로 정의
    
    * JSON 형식도 이용 가능
    
  * 예시 : 
  
    ```yaml
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
    	name: demo
    	labels:
    		app: demo
    	spec:
    		replicas: 1
    		selector: 
    			matchLabels:
    				app: demo
    			template:
    				metadata:
    					labels:
    						app: demo
    				spec:
    					containers:
    						- name: demo
    						  image: cloudnatived/demo:hello
    			
    ```





* **Service( 서비스 리소스 )**

  * 서비스 리소스란?

    * 네트워크 연결이 필요한 Pod에 영구적인 IP 주소와 DNS 주소를 제공
    * 트래픽 요청을 받으면, 백엔드 Pod Set에 요청을 전달
    * 웹포트와 YAML 파일의 Spec에 명시된 포트로 트래픽 전달 
      * label과 selector를 이용하여 label 값이 일치하는 Pod 중 무작위로 선택한 Pod로 각 요청을 전달
      * 전통적인 로드 밸런서와 유사
    * *즉, Service는 Pod에게 요청을 전달하는 단일 엔트리 포인트를 제공*

  * 서비스 매니페스트 예시 : 

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
    	name:demo
    	labels:
    		app: demo
    spec:
    	ports:
    	-	port: 9999
    		protocol: TCP
    		targetPort:8888
    	selector:
    		app:demo
    	type: ClusterIP
    ```





* kubectl로 클러스터 조회하기
  * 리소스 관리 도구
    * 리소스 설정 적용
    * 리소스 생성, 수정, 제거
    * 리소스 상태에 대해 조회
  * `kubectl get all`
    * 모든 타입의 리소스를 확인
  * `kubectl describe <리소스 종류>/<리소스 이름>`
    * 개별 리소스에 대한 포괄적인 정보 확인





### 4-7. Helm ( 헬름 ) : 쿠버네티스 패키지 매니저



* 헬름이란?
  * 쿠버네티스 패키지 매니저
  * 헬름 CLI로 애플리케이션 설치 및 설정 가능
    * Helm chart
      * 패키지
      * 애플리케이션을 실행하는데 필요한 리소스, 의존성, 구성 가능한 변수를 지정 가능
      * 다른 바이너리 패키지와의 차이점 : 
        * 실제 실행되는 컨테이너 이미지 자체는 포함하지 않음
        * 이미지를 찾을 수 있는 메타 데이터만 포함
  * 클라우드 네이티브 컴퓨팅 파운데이션(CNCF)  프로젝트의 일부





* 헬름 설치
  * https://helm.sh/ko/docs/intro/using_helm/
  * helm install은 *헬름 릴리스*라는 쿠버네티스 오브젝트를 생성하여 이를 수행





* chart, repository, relelase
  * chart: 
    * 쿠버네티스에서 애플리케이션을 실행하는 데 필요한 모든 리소스를 정의하여 포함
  * repository :
    * 차트가 모여 공유할 수 있는 공간
  * release:
    * 쿠버네티스 클러스터에서 실행되는 차트의 특정 인스턴스
    * `-name` 플래그로 이름을 지정할 수 있다.
      * 지정 안하면 임의로 지어진다.





* `helm list`
  * 실행 중인 릴리스 확인
  * `helm status <릴리즈 이름>`
    * 특정 릴리스의 정확한 상태 확인







## 5. 리소스 관리하기



### 5-1. 리소스 이해하기

> 쿠버네티스 스케줄러 관점에서 리소스를 가장 효율적으로 사용할 수 있는 방법을 생각해본다.
>
> 각 Pod마다 리소스 요청 / 상한이 존재한다.
>
> CPU와 메모리 두 리소스를 어떻게 관리하면 좋을까?





* 리소스 단위
  * Pod의 CPU 사용량
    * CPU 단위로 표시
    * 1CPU = 일반적인 CPU 단위
    * 대부분의 Pod는 CPU 전체를 필요로 하지 않는다.
      * 요청과 상한은 millicpus로 표시
      * 메모리는 바이트 / mebibyte( MiB )로 측정



* 리소스 요청
  * Pod를 실행하기 위한 최소 리소스 양을 지정
  * 쿠버네티스 스케줄러는 Pod template에 요구된 리소스를 기준으로 충분한 리소스 용량을 가진 노드를 탐색
    * 요구된 리소스보다 적은 리소스를 가진 노드에는 Pod가 할당되지 않는다.



* 리소스 상한
  * Pod가 사용할 수 있는 최대 리소스 양을 지정
    * 만일, Pod가 지정된 상한을 초과하여 CPU를 사용하려고 하면, 해당 Pod는 곧바로 제지된다.
    * 상한을 초과한 Pod는 종료 후 재 스케줄링된다.
    * 상한을 지정함으로써, Pod가 클러스터 용량을 과다하게 점유하는 것을 막을 수 있다.
  * 오버커밋( overcommit )
    * 노드 내 컨테이너의 모든 리소스 상한의 합계가 해당 노드의 전체 리소스 양을 초과할 수 있음을 의미
      * 컨테이너 모든 리소스 상한의 합 > 노드 전체 리소스 양
    * 스케줄러는.. 이런 상황이 발생하지 않을 것이라고 가정하고 오버 커밋을 허용
    * 발생 시
      * 리소스 부족으로 인해 컨테이너를 종료
        * 상한 리소스를 초과하지 않은 컨테이너도 종료될 수 있다.
      * 리소스 요청을 가장 많이 한 Pod부터 종료한다. 





* 컨테이너를 작게 유지하라
  * 컨테이너 이미지는 가능한 작은 것이 좋다.
    * 더 빠른 빌드
    * 저장공간 덜 차지
    * 더 빠른 풀링
    * 더 적은 보안 취약점





### 5-2. 컨테이너 생명 주기 관리하기



* Stuck state
  * 프로세스가 실행 중이지만 요청을 처리하지 못하는 상태
  * 쿠버네티스는 이러한 상황을 감지하고 재시작하여 문제 해결이 가능해야 한다.





* 활성 프로브

  * 컨테이너의 작동여부를 확인하는 헬스 체크

    * 컨테이너 스펙으로 지정할 수 있다.

    * Spec 작성 예시

      ```yaml
      livenessProbe:
      	httpGet:
      		path: /healthz
      		port: 8888
      	initialDelaySeconds: 3
      	periodSeconds: 3
      ```

    * 작동 설명

      * httpGet 프로브가 지정한 URI와 포트에 HTTP 요청
        * /healthz의 8888포트로 요청
        * `/healthz` : 일반적으로 헬스 체크를 위한 엔드포인트로 쓰인다.
      * 애플리케이션이 `2xx`, `3xx` 상태 코드로 HTTP 응답 시, 활성 상태로 판단
        * 다른 값으로 응답 시, 컨테이너 죽은 것으로 판단, 재실행



* 프로브 딜레이와 주기
  * 쿠버네티스는 언제 활성 프로브를 검사해야 하는가?
    * `initalDelaySeconds`
      * 첫 번째 활성 프로브를 실행하기 전에 얼마나 기다려야하는지 값을 설정
      * 시작하자마자 활성 프로브를 검사하는 경우를 막아준다.
    * `periodSeconds`
      * 활성 프로브를 검사하는 주기를 지정
      * 지나치게 많은 헬스 체크 요청은 좋지 않다.



* 그 외의 프로브

  * HTTP 외에 tcpSocket도 이용할 수 있다.

  * 작성 예시

    ```yaml
    livenessProbe:
    	tcpSocket:
    	port: 8888
    ```

  * exec 프로브 작성 예시

    ```yaml
    redinessProbe:
    	exec:
    		command:
    		- cat
    		- /tmp/healthy
    ```

    * exec 프로브
      * 컨테이너에 지정된 명령어를 실행한다.
      * 명령어가 실행 후, 제로 상태 코드로 종료하면 프로브는 성공
      * 일반적으로 준비성 프로브로 유용하게 사용



* gPRC 프로브
  * **마이크로서비스**에서 가장 인기있는 네트워크 프로토콜
  * 효율적이고 간편한 바이너리 네트워크 프로토콜
  * 구글이 개발
  * httpGet 프로브는 작동하지 않으므로, tcpSocket 프로브를 사용
    * 서버 연결 불가능
    * 소켓 연결 가능
  * `grpc-health-probe`
    * 쿠버네티스 활성 프로브로 헬스체크를 활용하기 위한 도구
    * 컨테이너에 도구 추가 시, exec 프로브를 사용하여 상태 확인 가능



* 준비성 프로브 ( rediness probe )

  * 애플리케이션이 일시적으로 요청을 처리할 수 없는 상태일때, 쿠버네티스에 신호를 보내는 기능

  * 작성 예시 

    ```yaml
    readinessProbe:
    	httpGet:
    		path: /healthz
    		port: 8888
    	initialDelaySeconds: 3
    	periodSeconds: 3
    ```

    * 애플리케이션이 준비를 완료할 때까지 HTTP를 수신하지 않는 경우의 작성 예시

  * 준비성 프로브가 준비를 실패하면, Pod는 제거된 뒤, 준비를 성공할 때까지 재시작된다.

    * => 준비되지 않은 컨테이너 때문에 발생하는 사용자 에러 예방
    * 준비 완료 반환 값은 오로지 `HTTP 200 ok` 
      * => 쿠버네티스는 200~ 300 사이의 HTTP 응답값을 모두 "준비" 상태로 판단하지만, 클라우드 로드 밸런서는 300 이상의 코드를 비정상으로 판단할 수도 있기 때문.



* 파일 기반 준비성 프로브
  * exec 준비성 프로브로 컨테이너 내에 파일 존재 여부를 알 수 있다.
  * 컨테이너 내 프로그램 디버그 시 유용하다.
    * 준비성 프로브 기준을 컨테이너 내 파일로 설정한 뒤, 
    * 디버그할 컨테이너에 접속하여 기준 파일을 삭제한다.
    * 접속한 컨테이너는 준비성 프로브에서 실패하였으므로 서비스에서 제외된다.
    * 서비스 중단 상태에서 디버그를 진행한다.



* `minReadySeconds`
  * 컨테이너가 준비 완료 상태이지만, 안정성을 좀 더 확인하고자 할 때 이용하는 필드값
  * 준비성 프로브 성공 후, minReadySeconds 지정 값이 지날 때까지 컨테이너는 준비 상태로 판단되지 않는다.



* PodDisruptionBudgets

  * 주어진 시간에 제거할 수 있는 Pod의 양을 제한하는 리소스

  * `minAvailable`

    * 최소한 실행해야 하는 Pod의 갯수를 지정하는 필드값

    * 작성 예시

      ```yaml
      apiVersion: policy/v1beta1
      kind: PodDisruptionBudget
      metadata:
      	name: demo-pdb
      spec:
      	minAvailable: 3
      	selector:
      		matchLabels:
      			app: demo
      ```

      * `app: demo` 레이블과 일치하는 Pod가 적어도 3개는 실행 중이어야 한다는 것을 의미

  * `maxUnavailable`

    * 한 번에 퇴출시킬 수 있는 Pod의 총 갯수 / 비율을 제한하는 필드값

    * 작성 예시

      ```yaml
      apiVersion: policy/v1beta1
      kind: PodDisruptionBudget
      metadata:
      	name: demo-pdb
      spec:
      	maxUnavailable: 10%
      	selector:
      		matchLabels:
      			app: demo
      ```

      * `app: demo` 레이블과 일치하는 Pod를 10% 이상 한번에 퇴출시킬 수 없음을 의미
      * 자발적 퇴출의 경우에만 제한값이 적용
        * 자발적 퇴출?
          * 쿠버네티스가 인위적으로 Pod를 종료하는 경우를 뜻함
      * 하드웨어 장애 및 비자발적 퇴출로 인한 것은 제한 값에 포함되지 않고 별도로 취급된다.

  * 쿠버네티스는 Pod를 분산시키려는 경향이 있다.

  * 분산 운영 시, Pod 축출이 과하게 이뤄졌을 경우 문제가 발생할수 있으므로 위와 같은 옵션값을 리소스에서 잘 지정하자!

    * 서비스에 영향이 가지 않도록 충분한 수의 레플리카를 유지할 수 있도록 하자는 뜻!



### 5-3. 네임스페이스 사용하기

> 리소스 관리의 또다른 방법 중에는 **네임스페이스**를 활용하는 것이 있다.
>
> * 예 : 상용 애플리케이션을 위한 prod 네임스페이스 / 테스트를 위한 test 네임스페이스
> * 네임스페이스 안의 Name은 다른 네임스페이스와 격리
> * 컴퓨터 하드디스크의 폴더와 비슷하다고 볼 수 있다.
>   * 즉, 관련된 리소스를 그룹화하여 작업하기 쉽게 만들어준다.
>   * 단, 폴더와는 다르게 네임스페이스는 중첩될 수 없다.



* 네임스페이스 다루기
  * 네임스페이스를 지정하지 않으면 `default` 기본 네임스페이스에 명령이 실행
    * `--namespace` 플래그로 네임스페이스를 지정
    * `kubectl get pods --namespace <namespace 이름>`
      * 네임스페이스에 해당하는 모든 Pod를 나열
  * `kube-system` 네임 스페이스
    * 내부 시스템 컴포넌트가 실행되는 공간
    * 애플리케이션과는 격리되어있다.



* 어떤 네임스페이스를 사용해야 하는가?

  * 직관적인 방법

    * 하나의 애플리케이션 당 하나의 네임스페이스
    * 예 : demo 애플리케이션을 demo 네임스페이스에서 실행

  * 네임스페이스 생성

    * 작성 예시

      ```yaml
      apiVersion: v1
      kind: Namespace
      metadata:
      	name: demo
      ```

    * 네임스페이스를 삭제하면 네임스페이스 내 모든 리소스가 삭제된다.

  * 쿠버네티스 네트워크 정책

    * 특정 네임스페이스의 모든 트래픽을 차단하는 경우에 사용



* 서비스 주소
  * 서비스의 DNS 양식
    * `SERVICE.NAMESPACE.svc.cluster.local`
    * `svc.cluster.local`은 선택 사항
    * `NAMESPACE`도 선택사항
    * 작성 예시 :  prod 네임스페이스에 있는 demo 서비스와 통신하고 싶다
      * `demo.prod`
      * 선택사항을 입력하여 원하는 서비스를 정확하게 지정 가능



* 리소스 쿼터

  * 네임스페이스도 리소스 제한이 가능하다.

  * 리소스 종류 : `ResourceQuota`

  * 작성 예시

    ```yaml
    apiVersion: v1
    kind: ResourceQuota
    metadata:
    	name: demo-resourcequota
    spec:
    	hard:
    		pods: "100"
    ```

    * 위의 매니페스트가 적용된 네임스페이스에서는 실행 가능한 Pod의 갯수가 100개로 제한

  * 적용 예시

    `kubectl apply --namespace <namespace 이름> -f <작성한 매니페스트 이름>.yaml`

  * 장점

    * 특정 네임스페이스 내 애플리케이션이 과도한 리소스를 독점하는 것을 막을 수 있다.

  * 네임 스페이스의 CPU와 메모리 사용량 제한은 **권장하지 않는다**

    * 낮게 설정 시
      * 워크로드가 한계에 가까워졌을 때, 예상 밖의 문제가 발생할 가능성이 크다
    * 높게 설정 시
      * 제한 설정의 의미가 없다.

  * `kubectl get resourcequotas`

    * 특정 네임스페이스에서 리소스 쿼터가 활성화인지 체크



* 기본 리소스 요청과 상한

  * `LimitRange`

    * 네임스페이스 내 **모든** 컨테이너의 기본 리소스 요청과 상한 설정 가능

  * 리소스 매니페스트 작성 예시

    ```yaml
    apiVersion: v1
    	kind: LimitRange
    	metadata:
    		name: demo-limitrange
    	spec:
    		limits:
    		-	default:
    				cpu: "500m"
    				memory: "256Mi"
    			defaultRequest:
    				cpu: "200m"
    				memory: "128Mi"
    			type: Container
    ```

    * 리소스 요청과 상한을 지정하지 않은 네임스페이스의 모든 컨테이너는 LimitRange에서 기본값 상속

  * LimitRange 리소스 이용시, 개별 컨테이너의 요청과 상한 지정을 신경쓰지 않아도 된다.

    * **하지만 권장하지 않는다**
    * 이 리소스 적용 여부와 상관없이, 컨테이너 Spec을 보고 요청과 상한을 알 수 있어야 함
    * 컨테이너 요청과 상한 지정을 놓쳤을 때를 대비한 *안전장치*로만 사용할 것





### 5-4. 클러스터 비용 최적화하기



* deployment 최적화
  * 레플리카의 갯수는 많을 수록 좋을까?
    * 롤링 업그레이드 중에도 서비스 품질이 저하되지 않는다.
    * 더 많은 트래픽 처리 가능
    * => **그러나** 클러스터는 한정된 리소스를 갖고있으므로, Pod는 최대의 효율성을 추구하는 만큼의 갯수만 실행되어야한다.
    * => 무중단 서비스를 제공하는데 있어서, 생각보다 많은 수의 레플리카가 필요하진 않다.
    * 결론 : deployment에서 Pod의 갯수는 최소한으로 두는 것이 좋다.





* Pod 최적화
  * 컨테이너 리소스 상한 설정
    * 워크로드 유형에 따라 상한 설정 방법이 달라진다.
    * 실제 작업보다 약간 높게 설정하되, 지나치게 높은 상한은 설정하지 않는다.
    * 필요성 : 
      * 컨테이너 메모리 누수를 방지한다.
      * 다른 컨테이너의 리소스 침해를 방지한다.



* Vertical Pod Autoscaler
  * 쿠버네티스 Addon
  * 리소스 요청에 대한 이상적인 값 설정을 돕는다.
  * 디플로이먼트를 관찰, 실제 사용하는 양에 따라 자동으로 Pod의 리소스 요청으로 조정
    * dry-run-mode
      * 실행 중인 Pod를 수정하지 않고 제안만 하는 모드



* Node 최적화
  * 모든 노드에는 운영체제가 존재
    * 디스크, 메모리, CPU 자원을 소모
    * 노드 크기가 작을 때 
      * 오버헤드가 차지하는 전체 리소스 비율이 더 높아진다.
      * 버려지는 리소스( stranded resources ) 비율이 높다
        * 사용 가능한 메모리 공간과 CPU 시간이 너무 작아서 Pod에 할당되지 않기 때문
    * 노드 크기가 클 때
      * 워크로드에 사용할 수 있는 리소스 비율이 더 높다.
      * 비용 면에서 효율적
      * 단점 : 개별 노드에 장애가 발생할 경우 클러스터 가용 용량에 미치는 영향이 더 크다.
  * 노드 권장 설정값
    * 노드의 크기
      * 보통의 Pod를 적어도 5개 실행 가능하게 한다.
      * 버려지는 리소스의 비율을 10% 이하로 유지한다.
        * 만약 Pod를 10개 이상으로 실행할 경우, 버려지는 리소스는 5%미만으로 유지해야한다.
    * 노드 당 기본 Pod 상한값은 110개
      * `--max-pods`로 Pod 상한을 조정할 수 있지만, 관리형 서비스마다 지원여부가 다르다.
      * 특별한 사유가 없다면 변경을 권장하지 않는다.
      * 클라우드 업체에서 가장 큰 인스턴스를 사용할 경우 이점이 없다.
        * 이런 경우, 작은 크기의 노드를 다수 운영하는 것이 효율적
  * `kubectl top nodes`
    * 각 노드의 리소스 사용률을 확인한느 명령어
    * CPU 사용률이 높을수록 잘 활용되고 있는 것
    * 큰 노드의 사용률이 높으면 작은 노드를 제거하고 큰 노드로 교체하는 것이 좋다.





* 스토리지 최적화
  * 디스크 스토리지는 간과하기 쉬운 클라우드 비용 중 하나!
  * 다른 리소스들과 달리, 조정이 불가능한 리소스
  * 실제 사용하는 처리량과 공간을 기준으로 할당할 디스크 볼륨의 크기를 정해야 한다.





* 사용하지 않는 리소스 정리

  * 정기적으로 리소스를 정리하여서 비용과 리소스를 효율적으로 처리하는 것이 좋다.

  * 정리 방법

    1. 소유자 메타데이터 이용

       * 각 리소스에 소유자 정보를 태그하여 관리

         * 매니페스트 파일의 annotation 필드값으로 관리

         * 필드값 작성 예시

           ```yaml
           apiVersion: extendsions/v1beta1
           kind: Deployment
           metadata:
           	name: my-brilliant-app
           	annotations:
           		example.com/owner: "Customer Apps Team"
           		...
           		
           ```

    2. 활용도가 낮은 리소스 파악

       * Pod의 활용도를 **메트릭**을 이용하여 파악한다.
         * 모든 Pod는 수신한 요청의 수를 메트릭으로 기록
       * 웹 콘솔에서 각 Pod의 CPU와 메모리 사용류률을 확인
       * 활용도가 낮지만 유지가 필요한 Pod는 annotation 필드값에 추가하여 식별할 수 있도록 해주는 것이 좋다.

    3. 완료된 잡 정리하기

       * 쿠버네티스 Job?
         * 한번만 실행되고 완료된 이후에는 다시 시작하지 않는 Pod
         * 완료된 잡은 삭제되지 않고 쿠버네티스 데이터베이스에 잔류
         * => 누적되면 API 성능에 영향을 줄 수 있다.
       * `kube-job-cleaner`
         * 완료된 잡을 정리하는 도구





* 여유 용량 파악하기

















