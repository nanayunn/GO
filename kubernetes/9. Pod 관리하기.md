

[TOC]



## 9. Pod 관리하기



### 9-1. 레이블



* 레이블이란?
  * Pod와 같은 오브젝트에 붙이는 key-value 쌍
    * 사용자가 생성한 특정한 키-값 쌍
    * 사용자가 Pod를 식별하기 쉽게끔 도와줌
  * 코어 시스템과 직접적으로 관련은 없다.



* 셀렉터

  * 레이블을 찾는 표현식

  * 셀렉터를 이용하면, 같은 레이블을 지닌 객체들을 상대로 리소스 그룹을 형성할 수 있다.

    * 예 : 요청을 전달한 pod를 식별하기 위해 셀렉터를 이용하는 서비스 리소스

      ```yaml
      apiVersion: v1
      kind: Service
      ...
      spec:
      	selector:
      		app: demo
      ```

    * 매니페스트에 셀렉터가 고를 레이블이 정의된 서비스는, 매니페스트에 적힌 레이블 값과 같은 값을 가진 Pod만을 대상으로 서비스에 연결시킨다.

  * `--selector`( `-l` )

    * kubectl get 명령어를 이용할 때, 레이블 값으로 필터링하여 리소스를 가져올 수 있다.

  * `kubectl get ~ --show-labels`

    * 리소스에 정의된 레이블 목록을 확인하는 명령어



* 고급 셀렉터

  * `,` 콤마로 여러 레이블에 해당하는 리소스를 필터링, 추출할 수 있다.

  * 예 : `kubectl get pods -l app=demo,environment=production`

    * 콤마로 연결된 레이블을 모두 갖고 있는 Pod만 리턴한다.

  * key-value 값의 조건을 명시할 때, 부등호를 지정할 수 있다.

    * 예 : app=demo / app!=demo

  * 레이블 값의 집합으로 리소스를 조회할 수 있다.

    * 예 : `kubectl get pods -l "environment in ( staging, production )"`

    * YAML 매니페스트 작성 예제

      ```yaml
      selector:
      	matchExpressions:
      	-	{key: environment, operator: In, values: [staging, production]}
      ```

      

    * 해당하지 않는 레이블을 조회할 경우

      * 예 : `kubectl get pods -l "environment notin (production)"`

      * YAML 매니페스트 작성 예제

        ```yaml
        selector:
        	matchExpressions:
        	-	{key: environment, operator: NotIn, values: [production]}
        ```

        

* 레이블의 다른 용도?

  * 다양한 키-값 쌍의 레이블을 이용

    * 리소스를 여러 방법으로 그룹, 나눌 수 있다.

    * 여러 개의 레이블을 이용한 매니페스트 예시

      ```yaml
      metadata:
      	labels:
      		app: demo
      		tier: frontend
      		environment: production
      		environment: test
      		version: v1.12.0
      		role: primary
      ```

  * 카나리아 배포에 활용

    * 활용 예시 : 애플리케이션을 일부 Pod만 새롭게 롤아웃 하고싶다면?
    * `track: stable`
    * `track: canary`
    * 두 개의 레이블을 이용해 디플로이먼트 분할
      * stable과 canary 레이블이 포함된 Pod 모두에 트래픽 전달
      * canary Pod의 비율을 점진적으로 높이며 디플로이먼트에서도 레플리카 갯수 변경
      * 실행 중인 Pod가 모두 canary 레이블을 가진 pod로 변경되면, 레이블도 stable로 변경해준다.
      * 다음 버전이 나오면 똑같은 방법으로 롤아웃 실행





* 레이블과 어노테이션
  * label과 annotation
  * 공통점
    * 둘 다 리소스에 대한 메타 데이터를 제공하는 키-값 쌍의 집합
  * 차이점
    * 레이블은 리소스를 식별
      * 관련된 리소스 그룹을 지정할 때 사용
      * 내부 쿼리에서 종종 사용
        * 쿠버네티스 성능에 영향을 미칠 수 있으므로, 엄격한 기준으로 유효성 검사
    * 어노테이션은 비식별 정보를 저장
      * 쿠버네티스 외부의 도구나 서비스에서 사용





### 9-2. Node affinity

* 노드 어피니티를 이용하면 특정 노드에 Pod를 우선적으로 스케줄링할 수 있다.

* 대부분의 경우에는 쿠버네티스가 잘 판단하여 Pod를 스케줄링
  * 선점형 노드는 경고없이 중지될 수 있다
  * 재시작 비용이 큰 Pod가 선점형 노드에 스케줄링 되었다가 삭제가 되면, 비효율적
  * 이러한 비효율성을 막기위한 옵션이 **affinity**
    * requiredDuringSchedulingIgnoredDuringExecution ( 강제 )
      * Pod가 스케줄링 되려면 반드시 규칙을 만족해야함
    * preferredDuringSchedulingIgnoredDuringExecution ( 반강제 )
      * 규칙을 만족하면 좋고, 아니어도 상관없음
      * 규칙 만족이 강제사항이 아님



* 강제 어피니티

  * 매니페스트 작성 예제

    ```yaml
    apiVersion: v1
    kind: Pod
    ...
    spec:
    	affinity:
    		nodeAffinity:
    			requiredDuringSchedulingIgnoredDuringExeucution:
    				nodeSelectorTerms:
    				-	matchExpressions:
    					-	key: "failure-domain.beta.kubernetes.io/zone"
    						operator: In
    						values: ["us-central1-a"]
    ```



* 반강제 어피니티

  * 매니페스트 작성 예제

    ```yaml
    preferredDuringSchedulingIgnoredDuringExecution:
    -	weight: 10
    	preference:
    		matchExpressions:
    		-	key: "failure-domain.beta.kubernetes.io/zone"
    			operator: In
    			values: ["us-central1-a"]
    - 	weight: 100
    	preference:
    		matchExpressions:
    		-	key: "failure-domain.beta.kubernetes.io/zone"
    			operator: In
    			values: ["us-central1-b"]
    ```

    * `weight`  필드값
      * 숫자가 클수록 더 높은 우선순위



### 9-3. Pod affinity와 Anti-affinity



* Pod끼리 레이블을 참고하여 함께 실행되는 옵션은 없을까?
* Pod 어피티니를 활용하면 가능
  * 노드 어피니티와 비슷한 옵션값으로 설정





* Pod를 함께 배치하기

  * server와 cache Pod가 같은 노드에 배치되기 위한 매니페스트 작성 예시

    ```
    apiVersion: v1
    kind: Pod
    metadata:
    	name: server
    	labels:
    		app: server
    ...
    spec:
    	affinity:
    		podAffinity:
    			requiredDuringSchedulingIgnoredDuringExecution:
    				labelSeletor:
    				-	key: app
    					operator: In
    					values: ["cache"]
    				topolocyKey: kubernetes.io/hostname
    ```

    * 위의 예제 매니페스트는 **강제** 어피니티 옵션을 준 Pod이다.
    * 그러나, 강제 옵션을 준 경우, 노드에 리소스가 부족하거나, 만족하는 노드가 없는 경우 Pod 실행 자체가 실패한다.
    * 따라서, 위와 같이 함께 배치하고 싶은 Pod가 있다면, 애초에 하나의 Pod로 만들거나 반강제 어피니티를 이용하여 선호도 정도만 주도록 하는 것이 좋다.



* Pod를 분산하여 배치하기

  * `podAntiAffinity`

    * Pod를 분산하여 배치하기 위해 쓰는 어피니티

    * 작성 예시

      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
      	name: server
      	labels:
      		app: server
      ...
      spec:
      	affinity:
      		podAntiAffinity:
      			requiredDuringSchedulingIgnoredDuriginExecution:
      				labelSelector:
      				-	matchExpressions:
      					-	key: app
      						operator: In
      						values: ["server"]
      				topologyKey: kuberntes.io/hostname
      ```

    * 안티 어피니티는 규칙을 만족하는 노드에 Pod를 스케줄링 **하지 않도록** 스케줄러에게 요청

      * 즉, app: server 레이블이 붙은 Pod가 이미 app: server Pod를 실행중인 노드에 스케줄링되지 않도록 한다.





* 반강제 안티 어피니티
  * 쿠버네티스는 Pod의 레플리카의 균등 분산보다 충분한 갯수의 레플리카 운용을 더 중요하게 생각
    * 강제 안티 어피니티는 상용 환경에서 적합하지는 않다.
    * 노드 어피니티 설정과 마찬가지로 *반강제 안티 어피니티*로 설정을 하는 것이 좋다.



* Pod 어피니티는 언제 사용해야하는가?
  * 어피니티 옵션값은 스케줄러의 자유를 제한
    * 스케줄러는 이미 최적의 성능과 가용성을 고려하여 클러스터에 Pod를 배치하기 때문
  * 따라서 어떤 문제점에 대한 해결 방안으로만 Pod 어피니티를 사용해야 한다.





### 9-4. 테인트와 톨러레이션



* taint

  * 특정 노드에 스케줄러가 Pod를 배치하지 않도록 하기 위한 옵션값

  * taint를 활용하여 특정 종류의 Pod만 수용하는 전용 노드를 구성하도록 할 수 있다.

  * 명령어 사용 예시

    * `kubectl taint nodes docker-for-desktop dedicated=true:NoSchedule`
    * 톨러레이션을 하지 않으면, 이제 해당 노드에는 Pod가 스케줄링되지 않는다.

  * taint 설정 제거 방법

    * taint 이름 뒤에 마이너스 기호( `-` ) 추가
    * 예시 : 
      * `kubectl taint nodes docker-for-desktop dedicated=true:NoSchedule-`

  * toleration

    * Pod의 특성

    * taint와 함께 작동

    * 매니페스트 작성 예시

      ```yaml
      apiVersion: v1
      kind: Pod
      ...
      spec:
      	tolerations:
      	-	key: "dedicated"
      		operator: "Equal"
      		value: "true"
      		effect: "NoSchedule"
      ```

      * 매니페스트가 의미하는 것
        * 이 Pod는 `dedicated=true` taint가 NoSchedule로 설정된 노드에서 실행할 수 있다.

    * toleration 필드가 작성되지 않은 Pod는 taint가 설정된 노드에 스케줄링되지 않는다.

      * 만약 모든 노드에 taint가 설정되어 있다면?
        * Pod가 스케줄링 되지 못하고 `<Pending>` 상태에 머무른다.



​	

### 9-5. Pod 컨트롤러



* 컨트롤러
  * 효율적인 컨테이너 실행을 위한 쿠버네티스 도구



* 데몬셋

  * 데몬이란?

    * 서버에서 백그라운드로 장기간 실행되는 프로세스를 의미
    * 로깅과 같은 것을 처리

  * 쿠버네티스에서 데몬셋이란?

    * 클러스터의 각 노드에 데몬 컨테이너를 실행하는 것과 비슷
    * 로깅 서비스의 연결을 관리하고, 로그 메세지를 전달하는 로깅 에이전트를 각 노드에 배치

  * 매니페스트 작성 예시

    ```yaml
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
    	name: fluentd-elasticsearch
    	...
    spec:
    	...
    	template:
    	...
    		spec:
    			containers:
    			-	name: fluentd-elasticsearch
    				...
    ```

    * 데몬셋 리소스 사용 용도
      * 클러스터의 각 노드마다 *단일* 레플리카 Pod의 실행이 필요할 때





* StatefulSet

  * pod 컨트롤러의 유형 중 하나

  * 특정 순서에 따라 Pod를 시작, 중지하는 기능

  * StatefulSet에 적합한 서비스 유형

    * Redis, 몽고DB, 아파치 Cassandra
    * 클러스터를 구성하는 분산형 애플리케이션
    * 클러스터 리더를 식별하기 위해 예측 가능한 이름이 필요할 경우
    * 예시 : redis 스테이트풀셋 생성
      * redis-0 Pod가 준비 완료 된 후, redis-1 Pod를 실행

  * 작동 순서

    * 시작
      * 스테이트 풀 셋 => 첫번째 레플리카 실행 => 두번째 레플리카 실행 ...
    * 종료
      * 마지막 레플리카 종료 => 마지막 전 레플리카 종료... 첫번째 레플리카 종료 => 스테이트풀 셋 종료

  * 매니페스트 작성 예시

    ```yaml
    apiVersion: apps/v1
    kind: StatefulSet
    metadata:
    	name: redis
    spec:
    	selector:
    		matchLabels:
    			app: redis
    	serviceName: "redis"
    	replicas: 3
    	template:
    	...
    ```

  * 헤드리스 서비스

    * 매니페스트 필드값 중, `ClusterIp: None`으로 설정 시

  * 비헤드리스 서비스  사용

    * redis와 같은 단일 DNS 엔트리에서 전체 백엔드 Pod로 로드밸런스

  * 스테이트 풀셋 특징

    * PersistentVolumeClaim을 자동으로 생성하는 VolumeClainTemplate 오브젝트를 사용
      * Pod의 디스크 스토리지 관리 가능



* Job

  * 다른 컨트롤러와는 다른 유형의 쿠버네티스 Pod 컨트롤러

  * 지정한 횟수만큼만 Pod를 실행

    * 실행 후에는 작업이 완료된 것으로 판단

  * 활용 예시 : 

    * 배치 작업
    * 큐 워커 Pod

  * Job과 관련된 필드값

    * `completions`
      * 작업이 완료되었다고 판단하기 전에, 지정된 Pod가 성공적으로 실행되어야 하는 횟수를 지정
      * 기본값은 1
        * Pod가 1번 실행됨을 의미
    * `parallism`
      * 한 번에 몇개의 Pod를 실행할지 지정
      * 기본값은 1
        * 한 번에 하나의 Pod만 실행함을 의미

  * 매니페스트 파일 작성 예시

    ```yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
    	name: queue-worker
    spec:
    	completions: 1
    	parallelism: 10
    	tempate:
    		metadata:
    			name: queue-worker
    		spec:
    			containers:
    			...
    ```

  * Job 실행에 실패할 경우?

    * 디플로이먼트와 같이, 실패할 경우 다시 실행
    * 성공 상태로 종료되었을 경우에만 completion에 반영한다.

  * Job을 실행하는 방법

    * kubectl이나, helm을 이용, Job 매니페스트를 작성하여 실행
    * 지속적 배포 파이프 라인과 함께 자동화하여 실행
    * 크론잡을 이용하여 실행 ( 가장 일반적 )
      * 그렇다면 크론잡이란..?



* 크론잡

  * 유닉스 환경에서 작동하는 기존의 크론잡과 동일한 개념, 역할

  * 크론잡 매니페스트 예시

    ```yaml
    apiVersion: batch/v1beta1
    kind: CronJob
    metadata:
    	name: demo-cron
    spec:
    	schedule: "*/1 * * * *"
    	jobTemplate:
    		spec:
    		...
    ```

    * 핵심이 되는 필드
      * `spec.schedule`	
        * 유닉스 cron 유틸리티와 동일한 형식으로 작업을 실행할 시점을 지정
      * `spec.jobTemplate`
        * 실행할 Job 템플릿을 지정
        * 일반적인 Job 매니페스트 작성과 동일



* Horizontal Pod AutoScaler

  * 트래픽에 따라 **자동**으로 레플리카 갯수를 조정할 수 있도록 돕는다.

    * 서비스 레플리카의 갯수를 조정

  * 즉, 특정 디플로이먼트를 관찰,

    * 지정한 메트릭을 지속적으로 모니터링하며, 레플리카의 갯수를 필요에 따라 증감

  * 오토스케일링 메트릭 기준

    * CPU 사용률 

      * 가장 일반적인 메트릭

      * CPU 사용률 값에 미달 시
        * 레플리카 갯수를 줄여 다운 스케일링
      * CPU 사용률 값 초과 시
        * 레플리카 갯수를 증가시킴
      * 즉, HPA는 대상 레플리카의 목푯값 대비 실제 메트릭 값의 비율에 따라 레플리카 갯수를 조정

  * HPA 매니페스트 작성 예제

    ```yaml
    apiVersion: autoscaling/v2beta1
    kind: HorizontalPodAutoscaler
    metadata:
    	name: demo-hpa
    	namespace: default
    spec:
    	scaleTargetRef:  #스케일링할 디플로이먼트 지정
    		apiVersion: extensions/v1beta1
    		kind: Deployment
    		name: demo
    	minReplicas: 1     # 스케일링 상한 지정
    	maxReplicas: 10
    	metrics:           # 스케일링에 활용할 메트릭 지정
    	-	type: Resource
    		resource: 
    			name: spu
    			targetAverageUtilization: 80
    ```

  * 이외에 사용 가능한 메트릭

    * 시스템 메트릭
      * CPU, 메모리 사용률과 같이 기본으로 제공되는 메트릭
    * 서비스 메트릭
      * 애플리케이션에서 정의하고 가져오는 메트릭





* Pod Preset

  * 기능

    * 쿠버네티스 알파 단계의 기능
    * Pod를 생성할 때, 특정 정보를 Pod에 주입하도록 하는 기능
      * 예시 : 주어진 레이블과 일치하는 모든 Pod에 볼륨을 마운트 하도록 한다.

  * 유형

    * Admission Controller 객체의 한 유형
      * Admission Controller?
        * Pod가 생성되는 것을 감시, 
        * 셀렉터와 일치하는 Pod가 생성될 때 특정 작업을 수행

  * 활용 예시

    ```yaml
    apiVersion: setting.k8s.io/v1alpha1
    kind: PodPreset
    metadata:
    	name: add-cache
    spec:
    	selector:
    		matchLables:
    			role: frontend
    	volumeMounts:
    		-	mountPath: /cache
    			name: cache-volume
    	volumes:
    		-	name: cache-volume
    			emptyDir: {}
    ```

    * PodPreset을 사용, `role: frontend` 셀렉터와 일치하는 모든 Pod에 볼륨을 추가하는 예제

  * 주의사항

    * **Pod Preset에 정의한 설정은 Pod 자체의 설정을 덮어쓸수 없다**
    * 두 설정이 충돌하면 pod 생성 자체를 실패
    * `podpreset.admission.kubernetes.io/exclude: "true"`
      * Pod의 설정이 Pod Preset에 수정되지 않도록 제외하는 필드값 옵션



* 오퍼레이터와 커스텀 리소스 정의( CRDs )
  * 커스텀 리소스 정의
    * StatefulSet이 제공하는 것보다 더 복잡한 관리가 필요한 애플리케이션의 경우
    * 예 : Velero 백업 도구
      * Config와 Backup 커스텀 쿠버네티스 객체를 생성하여 사용
  * 컨트롤러 역할을 하는 커스텀 리소스도 생성할 수 있다.
    * 생성 순서 예시
      1. 커스텀 컨트롤러 객체를 위한 CRD 생성
      2. 생성된 CRD를 활용하기 위해 쿠버네티스 API와 통신할 프로그램 필요!
         * Operator 프로그램 작성 필요





### 9-6. Ingress Resource

* Ingress란?

  * 서비스 앞단에 있는 로드 밸런서의 개념

  * 클라이언트의 요청을 받아 서비스를 전달

    * 서비스는 전달받은 요청을 레이블 셀렉터를 기반으로 이와 맞는 Pod에 전달
    * ![스크린샷, 2020-09-18 14-46-30](https://user-images.githubusercontent.com/58680504/93560757-d2439100-f9bd-11ea-829d-e7d4e33157ef.png)

  * Ingress 매니페스트 작성 예제

    ```yaml
    apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
    	name: demo-ingress
    spec:
    	backend:
    		serviceName: demo-service
    		servicePort: 80
    ```

    * 예제의 Ingress는 demo-service의 80 포트로 요청을 전달





* 인그레스 규칙

  * 서비스와의 차이

    * 서비스 : 클러스너 *내부*의 트래픽을 라우팅
    * 인그레스 : 클러스터로 유입된 외부의 트래픽을 라우팅

  * 사용자가 지정한 특정 규칙에 따라 트래픽을 전달한다.

  * 가장 일반적인 사용 방법

    * Fanout

      * 요청 URL에 따라 트래픽을 라우팅

      * 매니페스트 작성 예시

        ```yaml
        apiVersion: extensions/v1beta1
        kind: Ingress
        metadata:
        	name: fanout-ingress
        spec:
        	rules:
        	-	http:
        			paths:
        			-	path: /hello
        				backend:
        					serviceName: hello
        					servicePort: 80
        			-	path: /goodbye
        				backend:
        					serviceName: goodbye
        					servicePort: 80
        ```

    * 이름 기반 가상 호스팅

      * HTTP host 헤더를 사용한 라우팅





* 인그레스에서 TLS 종료 활용하기

  * TLS ( 과거 SSL 프로토콜 ) 을 사용한 보안 연결 기능 제공

  * 매니페스트 작성 예제

    ```yaml
    apiVersion: extentions/v1beta1
    kind: Ingress
    metadata:
    	name: demo-ingress
    spec:
    	tls:
    	-	secreteName: demo-tls-secret
    	backend:
    		serviceName: demo-service
    		servicePort: 80
    ```

    *  spec에서 tls 섹션을 추가, 
    * 클라이언트와 트래픽을 암호화하는데 필요한 TLS 인증서를 인그레스에게 알림
    * 인증서는 쿠버네티스 시크릿 리소스에 저장된다.

  * 기존의 TLS 인증서를 추가하여 사용하기

    * 시크릿 오브젝트를 생성하여 쓰면 된다!

    * 매니페스트 작성 예시

      ```yaml
      apiVersion: v1
      kind: Secret
      type: kubernetes.io/tls
      metadata:
      	name: demo-tls-secret
      data:
      	tls.crt: <인증서 내용>
      	tls.key: <인증서 key>
      ```

      * 주의사항
        * 인증서와 키 데이터를 매니페스트에 추가하기 전에, base64로 인코딩하는 과정 필요

  * cert-manager로 LetsEncrypt 인증서 자동화

    * LetsEncrypt 인증서를 사용하여 TLS 인증서 요청과 갱신을 자동화
    * 클러스터에서 cert-manager 실행
      * 인증서가 없는 TLS 인그레스를 자동 탐색
      * 지정된 기관에 요청
    * TLS 연결이 처리되는 방식은 인그레스 컨트롤러에 따라 다름



* 인그레스 컨트롤러

  * 클러스터의 인그레스 리소스를 관리

  * 인그레스 컨트롤러는 인식할 수 있는 특정 어노테이션을 추가하여 인그레스의 작동을 정의

  * 구글과 AWS에서도 인그레스 기능을 서포트

    * 구글 GKE에서 클러스터 운영 시
      * 인그레스를 위한 구글 컴퓨터 로드 밸런서 사용 가능
    * AWS
      * 애플리케이션 로드 밸런서 제공

  * 이외의 자체 운영중인 클러스터에서 컨트롤러를 구축, 실행할 수 있는 방법

    * nginx-ingress

      * NGINX의 다양한 기능을 사용할 수 있는 로드 밸런서
      * 공식 제공 도구
        * nginx-ingress

    * Contour

      * 헵티오에서 관리하는 쿠버네티스 도구
      * 클라이언트와 Pod간의 요청을 프록시
      * 내부적으로 엔보이 사용
      * 엔보이란?
        * L7 프록시로, 네트웍과 애플리케이션 문제를 보다 쉽게 해결할 수 있도록 돕는 도구
        * docker를 이용해 컨테이너로 띄울 수 있다.

    * Traefik

      * 인그레스의 TLS 인증서를 자동으로 관리할 수 있는 가벼운 프록시 도구

      





### 9-7. 이스티오

* 이스티오란?
  * 서비스 간 네트워크 트래픽 라우팅 및 암호화 처리
  * 메트릭, 로그 , 로드 밸런서와 같은 중요한 기능 추가
  * 용도
    * 서로 통신하는 애플리케이션과 서비스가 여러 개인 경우 유용
    * 쿠버네티스 서비스에서, 애드온 컴포넌트로 추가할 수 있다.
  * 설치 방법
    * 이스티오 헬름 차트를 이용하여 설치
  * https://istio.io/docs/concepts/what-is-istio



### 9-8. 엔보이



* 엔보이
  * 보다 정교한 로드밸런싱 구현 가능
    * 기본 로드밸런싱 알고리즘
      * random으로 각 연결을 무작위 선택하여 백엔드에 전달
  * 고성능의 C++ 분산 프록시
    * 단일 서비스와 애플리케이션을 위해 설계된 기능









## 10. 구성과 시크릿



* 구성?
  * 애플리케이션의 값이나 설정
  * 일반적인 환경 설정, 서드파티 서비스의 DNS 주소, 인증 자격 증명
  * 쿠버네티스에서 구성을 관리하기 위해 제공하는 기능
    * Pod Spec의 환경 변수를 통해 애플리케이션에 값을 전달
    * ConfigMap과 시크릿 객체를 사용하여 설정 데이터를 쿠버네티스에 직접 보관



### 10-1. ConfigMap



* configmap?
  * 쿠버네티스 기본 객체
  * 구성 데이터를 저장하는 key-value 쌍의 집합
  * 애플리케이션에 데이터를 제공할 수 있다.
    * Pod에 파일을 생성하는 방식
    * Pod의 환경 변수에 추가하는 방식



* Configmap 생성하기

  * ```
    autoSaveInterval: 60
    batchSize: 128
    protocols:
    	- http
    	- https
    ```

    * 위와 같은 내용을 포함한 YAML 구성 파일이 필요할 때, configmap 리소스로 전환할 수 있는 방법은?

      

  1. configmap 매니페스트에 리터럴 YAML 값으로 데이터를 지정

  * 매니페스트 예제

    ```yaml
    apiVersion: v1
    data:
    	config.yaml : |
    		autoSaveInterval: 60
    		batchSize: 128
    		protocols:
    			- http
    			- https
    kind: ConfigMap
    metadata:
    	name: demo-config
    	namespace: demo
    ```

    * config.yaml의 data 섹션에 값을 추가하여 매니페스트를 처음부터 작성한다.

  2. kubectl 명령어를 활용하여 지정한 YAML 파일을 참조하는 ConfigMap 객체 생성
     * 명령어 이용 예시
       * `kubectl create configmap demo-config --namespace=demo --from-file=config.yaml`
     * 객체 생성 후, YAML 파일로 내보내기 하기 위한 명령어
       * `kubectl get configmap/demo-config --namespace=demo --export -o yaml >demo-config.yaml`





* configmap을 사용해 환경변수 설정

  * 생성된 Configmap을 바탕으로 컨테이너가 실행되도록 하는 실습 예제

    * HTTP 요청을 받을 시, 응답값을 환경변수 GREETING에서 읽어오도록 하는 예제

    * GREETING 환경변수에 값을 설정하는 Configmap 예제

      ```yaml
      apiVersion: v1
      kind: ConfigMap
      metadata:
      	name: demo-config
      data:
      	greeting: Hola
      ```

      * greeting 데이터를 컨테이너의 환경변수로 사용하기 위해, 디플로이먼트의 Spec도 함께 수정한다.

      ```yaml
      # deployment.yaml
      spec: 
      	containers:
      	-	name: demo
      		image: cloudnatived/demo:hello-config-env
      		ports:
      			- containgerPort: 8888  # port, targetport로 나뉘어있지 않으므로 객체 생성 후 port-forwarding 명령어 추가 실행 필요
      		env:
      			- name: GREETING
      			  valueFrom:
      			  	configMapKeyRef:
      			  		name: demo-config
      			  		key: greeting
      ```

      * `valueFrom` 필드값
        * name에 대한 리터럴 값을 다른 곳에서 찾아서 가져온다!
      * `configMapKeyRef`
        * configmap에 지정된 key를 참조한다.
          * 즉, 여기서는 demo-config ( => 앞서 작성한 configmap 예제 ) 파일의 key-value의 한쌍으로 이루어져있는 `greeting: Hola` 데이터를 참조하여 값을 가져오는 것이다.
          * 결론적으로 `GREETING: Hola` 의 key-value 쌍이 되는 것

      



* configmap에서 전체 환경변수 가져오기
  * 디플로이먼트 생성 시, 다음과 같이 `env` 필드를 `envFrom`으로 작성하면 참조하는 configmap에 있는 **모든 설정**이 컨테이너의 환경 변수가 된다.
    * 대소문자도 구분되어 환경 변수가 된다.
  * 환경 변수 지정 시, `env`와 `envFrom` 필드를 동시에 사용할 수 있다.
    * 대신, `env` 필드가 `envFrom`보다 우선순위를 갖는다.
      * 예 : 두 가지 필드에서 각각 참조한 configmap 파일이 있을 때, 파일 내 변수 이름이 동일하면, 변수 값을 무조건 `env` 필드에서 참조한 configmap 파일에서 가져온다.



* 명령줄 인자로 환경변수 사용하기

  * 쿠버네티스 특수 문법 `$(VARIABLE)`를 사용하여 환경 변수를 명령줄 인자로 가져올 수 있다.

  * 디플로이먼트 작성 예시

    ```yaml
    spec:
    	containers:
    	-	name: demo
    		image: cloudnatived/demo:hello-config-args
    		args:
    			- "-greeting"
    			- "$(GREETING)"
    		ports:
    			- containerPort: 8888
    		env:
    			- name: GREETING
    			  valueFrom:
    			  	configMapKeyRef:
    			  		name: demo-config
    			  		key: greeting
    ```

    * `args` 필드 추가
      * 사용자 지정 인수를 컨테이너의 기본 엔트리 포인트 `/bin/demo`로 전달
      * $(VARIABLE) 형식으로 된 모든 값은 VARIABLE 값으로 변경된다.



* configmap으로 설정 파일 생성

  * 단일 key 대신 완전한 YAML 파일을 저장하도록 하는 예제

    ```yaml
    apiVersion:L v1
    kind: ConfigMap
    metadata:
    	name: demo-config
    data:
    	config: |
    		greeting: Buongiorno
    ```

    * `|` 기호 : 
      * 데이터 블록을 의미
      * YAML, JSON, TOML, 평문 등등 다양한 형식으로 지정 가능
      * 파일 형식과 관계없이, 전체 데이터 블록은 컨테이너에 파일로 생성됨

  * 위의 configmap을 활용할 디플로이먼트

    ```yaml
    # deployment.yaml
    spec:
    	containers:
    		- name: demo
    		  image: cloudnatived/demo:hello-config-file
    		  ports:
    		  	- containerPort: 8888
    		  volumeMounts:
    		  -	mountPath: /config/
    		  	name: demo-config-volume
    		  	readOnly: true
    	volumes:
    	-	name: demo-config-volume
    		configMap:
    			name: demo-config
    			items:
    			-	key: cofig
    				path: demo.yaml
    ```

    * 작동 설명
      * demo-config configmap으로 demo-config-volume 이름의 볼륨 생성
      * 생성된 볼륨을 mountPath: /config/ 에 마운트
      * config key를 선택 후, demo.yaml 파일로 생성
        * 즉, 디플로이먼트 실행 시 demo-config 데이터 파일이 각 컨테이너의 /config/demo.yaml 경로에 생성된다.
      * 컨테이너 실행 시, 데이터 파일에서 설정값을 가져오게 된다.

  * `kubectl describe configmap/<configmap 이름>`

    * 클러스터의 configmap 데이터를 확인하는 명령어

  * 변경된 configmap 값은 자동 업데이트 된다.

    * 자동 반영 되지 않는다면, 애플리케이션을 재배포하면 된다.



* 설정 변경 시 Pod 업데이트 하기

  * helm 차트

    * 디플로이먼트가 실행 중인 상태에서 configmap의 일부 값을 변경하고자 할 때 사용

      * 자동으로 설정 변경 감지, Pod가 읽어들이도록 함
      * 디플로이먼트에 annotaion을 추가하는 방식

    * annotaion 추가 예시

      ```yaml
      checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
      ```

      * 디플로이먼트 템플릿
        * configmap 매니페스트의 hash 합을 포함하도록 설정
          * 설정이 수정되면 hash 합이 변경되므로, 이를 탐지하여 Pod를 재시작하도록 하는 것
          * `helm upgrade` 명령어 실행 필요



### 10-2. 쿠버네티스 시크릿



* 시크릿 오브젝트
  * 데이터 보안을 위한 객체
  * 패스워드, API 키와 같이 보안이 필요한 구성 데이터를 저장
  * 적용 방법
    * configmap과 같이 컨테이너에서 읽을 수 있는 환경 변수에 추가
    * 컨테이너의 파일 시스템에 파일로 마운트



* 데모 애플리케이션을 통해 알아보는 시크릿 활용 예제

  * 시크릿 매니페스트 작성 예제

    ```yaml
    apiVersion: v1
    kind: Secret
    metadata:
    	name: demo-secret
    stringData:
    	magicWord: xyzzy
    ```

    * configmap과 같이 여러개의 key: value 쌍을 추가할 수 있다.



* 시크릿을 사용하여 환경변수 설정하기

  * 디플로이먼트 작성 예제

    ```yaml
    spec:
    	containers:
    		- name: demo
    		  image: cloudnatived/demo:hello-secret-env
    		  ports:
    		  	- containerPorts: 8888
    		  env:
    		  	- name: GREETING
    		  	  valueFrom:
    		  	  	secretKeyRef:
    		  	  		name: demo-secret
    		  	  		key: magicWord
    ```

    * configmap과 설정 방법이 동일
      * `configMapKeyRef` 필드가 `secretKeyRef`로 바뀐 것 빼고는 딱히 변경된 부분이 없다.
      * 앞선 configmap 예제와 같이, apply 후 port-forwarding을 해주면 `magicWord` 키값에 해당하는 value 값이 출력



* 시크릿을 파일로 작성하기

  * 시크릿을 컨테이너의 파일로 마운트 한다.

  * 디플로이먼트 작성 예제

    ```yaml
    spec:
    	containers:
    		- name: demo
    		  image: cloudnatived/demo:hello-secret-env
    		  ports:
    		  	- containerPorts: 8888
    		  volumeMounts:
    		  	- name: demo-secret-volume
    		  	  mountPath: "/secrets/"
    		  	  readOnly: true
    	volumes:
    		- name: demo-secret-volume
    		  secret:
    		  	secretName: demo-secret
    ```

    * demo-secret-volume 이름의 볼륨을 생성
    * 컨테이너 spec의 volumeMounts 경로 `/secrets`로 마운트 한다. 
    * 시크릿 키-값 쌍은 해당 디렉터리에 파일로 생성된다. ( 읽기 전용, 암호화된 데이터 )





* 시크릿 읽기
  * 시크릿 타입
    * Opaque 타입
      * 명령어 겨로가, 로그, 터미널에 출력되지 않는 값
    * `kubectl describe` 명령어 이용
      * 조회할 수 없다.
    * `kubectl get secret/<secret 이름> -o yaml` 명령어 이용
      * 시크릿 값의 난독화된 버전이 조회됨
      * base64로 인코딩된 값
        * 임의의 이진 데이터를 문자열로 인코딩하기 위한 방식
    * `echo "base 64로 인코딩 된 값" | base64 --decode` 명령어 이용
      * 디코딩된 시크릿 값 확인 가능





* 시크릿 접근하기
  * 시크릿 접근 권한
    * RBAC로 제어
      * 쿠버네티스 접근 제어 매커니즘
      * 11장에서 소개,,,
    * 해당 설정을 클러스터가 지원하지 않거나, 활성화되어있지 않으면 시크릿은 공개적인 정보가 된다.
      * 상용 클러스터에서 절대로 운영해서는 안되는 방식!







* 저장 데이터 암호화
  * 시크릿 데이터를 etcd 데이터베이스에 암호화하여 저장
    * 이 데이터를 해석할 수 있는 키는 쿠버네티스 API 서버에게만 있다.
  * 저장 데이터 암호화의 활성화 여부를 알 수 있는 명령어
    * `kuectl describe pod kube-system -l component=kube-apiserver | grep encryption`
      * => `experimental-encryption-provider-config`
      * 플래그가 출력되면 저장 데이터 암호화가 활성화 상태라는 뜻
      * 구글 쿠버네티스 엔진과 같은 관리형 쿠버네티스 서비스 사용 시, 데이터가 다른 방식으로 암호화
        * 플래그 확인 불가



* 시크릿 보존하기

  * helm annotation

    * 특정 리소스가 삭제되지 않도록 한다.

  * ```yaml
    kind: Secret
    metadata:
    	annotations:
    		"helm.sh/resource-policy": keep
    ```

  



### 10-3. 시크릿 관리 전략

> 시크릿 데이터는 매니페스트 파일에 평문으로 저장
>
> => 소스코드 커밋으로 파일을 노출시켜서는 안된다..!
>
> 시크릿 데이터 파일을 안전하게 보관, 관리하는 방법은?



* 소스 제어에서 시크릿 암호화하기
  * 소스 제어 저장소에 있는 코드에 시크릿을 직접 저장
    * 암호화된 형식으로 저장, 배포 시에 복호화
    * 즉, 평문으로 소스 코드에 작성하되, 암호화된 평문을 작성해 넣는다.
  * 소스 코드에 시크릿을 작성한다면, 암호화, 복호화 방식을 제외하고 다른 소스코드와 동일하게 다루어질 수 있어야 한다.
    * 애플리케이션 코드 변경 사항과 똑같이 시크릿의 변경사항도 검토되고, 추적해야한다.
    * 고가용성도 보장되어야 한다.
    * 변경사항이 생기면 커밋이 이루어져야 한다.
  * 잠재적인 문제점?
    * 동일한 시크릿을 여러 애플리케이션에서 사용할 경우, 복제본을 다룰 때 실수의 가능성
    * 암호 키에 대한 접근 노출을 조심해야함



* 원격으로 시크릿 저장하기

  * AWS S3 버킷이나, 구글 클라우드 스토리지와 같이 안전한 파일 저장소에 원격 저장

    * 배포 시에는 파일을 다운로드 한 뒤, 복호화하여 애플리케이션에 전달

  * 장점

    * 소스 코드에 저장하는 방법에서, 중복된 시크릿을 이용하여 생기는 잠재적 문제점을 해결

  * 단점

    * 추가 엔지니어링이 필요
    * 배포 시점에 관련 시크릿 파일을 풀링해야 한다.

    * 시크릿 변경 사항을 감지, 수정할 수 있는 변경 제어 과정이 필요



* 전용 시크릿 관리 도구 사용하기
  * 시크릿 관리 도구의 종류
    * Hashicorp Vault
    * Square keywhiz
    * AWS secret manager
    * Azure key vault
  * 관리 도구 이용시 특징
    * 애플리케이션의 모든 시크릿을 중앙에 보관, 
    * 고가용성 보장
    * 사용자와 서비스 계정에 시크릿에 대한 권한 설정 가능
  * 시크릿 관리도구 데이터 활용 방법
    * 시크릿 보관소에 대해 읽기 전용 권한을 가진 서비스 계정을 사용하는 것
  * 단점
    * 인프라가 더 복잡해진다
      * 시크릿 저장소 구축 및 관리 필요..
      * 시크릿을 사용하는 다양한 서비스에 시크릿 관리 도구와 연동할 수 있도록 도구나 미들웨어 추가 필요..
      * 소스코드나 원격으로 넣는 방법에 비해 접근을 위해서 리팩토링 등, 더 큰 시간과 비용 필요





* 추천전략
  * 실제로 관리가 필요한 시크릿이 그렇게 많지 않으므로, 시크릿 관리 시스템 이용 보다는 직접 암호화하여 이용하는 것을 권장





### 10-4. SOPS로 시크릿 암호화 하기

> Mozilla 프로젝트의 SOPS
>
> * secrets operations의 줄임말
> * YAML, JSON, 이진 파일을 암호, 복호화하는 도구
> * 다양한 암호화 백엔드를 지원





* SOPS 소개

  * 전체 파일을 암호화하는 것이 아니라, 개별 시크릿 값만 암호화

    * 따라서 Pull 요청 과정에서 데이터 복호화 과정이 없어도, 코드 수정 및 리뷰가 간편

    * 예시 : 

      * ```yaml
        password: foo
        
        password: ENC[ASE1350_GCM, data:p335==....]
        ```

  * helm-secrets 플러그인으로 SOPS를 사용 가능

    * helm upgrade, helm install 실행 시 
      * 시크릿 배포 과정에서 복호화



* SOPS로 파일 암호화하기
  * 실제 암호화를 진행하지는 않는다!
    * => GnuPG( PGP의 오픈소스 버젼 )와 같은 백엔드에 이를 전달
  * PGP?
    * 공개키 암호 시스템 ( SSH, TLS.. )
    * 공개키, 개인키 한 쌍
    * https://gnupg.org/download
    * 신규 키 생성
      * `gpg --gen-key`
      * Key fingerprint 개별 저장
    * SOPS 설치
      * https://github.com/mozilla/sops/release
      * 혹은 `go get -u go.mozilla.org/sops/cmd/sops`
    * SOPS 버전 확인
      * `sops -v`
    * SOPS를 이용하여 암호화
      * `sops --encrypt --in-place --pgp <key fingerprint>`
    * 복호화
      * `sops --decrypt test.yaml`



* KMS 백엔드 사용하기
  * 클라우드 키 관리 도구
  * KMS를 SOPS와 함께 사용할 수 있다.
  * 작동은 PGP와 함께 작동하는 방식과 동일하다.







## 11. 보안과 백업



### 11-1. 접근 제어와 권한



* 클러스터별 접근 관리
  * 클러스터 접근 사용자
    * 클러스터 운영자
    * 애플리케이션 개발자
    * => 업무에 따라 서로 다른 권한이 필요





* 역할 기반 접근 제어 ( RBAC )
  * role-based access control
  * 특정 작업을 수행할 사용자를 제어
    * 즉, 특정 사용자 / 서비스 계정에 특정 권한을 부여
  * 쿠버네티스 1.6 버전부터 옵션으로 제공되는 기능
    * 활성화 여부는 클라우드 서비스 업체와 쿠버네티스 설치 프로그램에 따라 달라진다.





* 롤 이해하기

  * RBAC의 가장 중요한 개념

    * 롤

      * 특정한 권한들의 집합

      * 미리 정의된 롤을 기본으로 제공

        * cluster-admin
          * 슈퍼유저, 최고 관리자를 위한 권한
          * 클러스터 전체 리소스를 조회, 변경 가능
        * view
          * 주어진 네임스페이스의 오브젝트 목록을 조회, 확인할 수 있는 권한
          * 오브젝트를 수정할 수 없다.

      * 롤 오브젝트

        * 네임스페이스, 클러스터 단위로 롤을 정의할 수 있다.

        * 특정한 네임스페이스 전체에 secret-reader권한을 부여하는 예제

          ```yaml
          kind: ClusterRole
          apiVersion: rbac.authorization.k8s.io/v1
          metadata:
          	name: secret-reader
          rules:
          -	apiGroups: [""]
          	resources: ["secrets"]
          	verbs: ["get", "watch", "list"]
          ```



* 사용자에게 역할 바인딩하기

  * 롤 바인딩

    * 사용자와 롤을 연결한다.

    * RoleBinding 오브젝트를 활용

      * 네임스페이스 단위
      * ClusterRoleBinding은 클러스터 단위

    * daisy라는 사용자에게 demo 네임스페이스에 대한 edit 롤을 할당하는 RoleBinding 매니페스트 예제

      ```yaml
      kind: RoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
      	name: daisy-edit
      	namespace: demo
      subjects:
      -	kind: User
      	name: daisy
      	apiGroup: rbac.authorization.k8s.io
      roleRef:
      	kind: ClusterRole
      	name: edit
      	apiGroup: rbac.authorization.k8s.io
      ```

  * 쿠버네티스 권한 할당 특징

    * **가산적**
      * 아무 권한이 없는 상태에서 시작하여 권한을 추가하는 방식
    * **이미 권한을 갖고있는 사용자의 권한을 제거할 수 없다**



* 롤의 권한을 확인하는 방법

  *  `kubectl describe` 명령어 이용

    * 예시 : kubectl get clusterrole

      ```
      NAME                                                                   AGE
      admin                                                                  24d
      ca-cr-actor                                                            3d23h
      cloud-provider                                                         24d
      cluster-admin                                                          24d
      cluster-autoscaler                                                     3d23h
      edit                                                                   24d
      external-metrics-reader                                                6d23h
      gce:beta:kubelet-certificate-bootstrap                                 24d
      gce:beta:kubelet-certificate-rotation                                  24d
      gce:cloud-provider                                                     24d
      gke-metrics-agent                                                      6d23h
      kubelet-api-admin                                                      24d
      read-updateinfo                                                        24d
      stackdriver:fluentd-gcp                                                24d
      stackdriver:fluentd-gke                                                6d23h
      stackdriver:metadata-agent                                             24d
      storage-version-migration-crd-creator                                  24d
      storage-version-migration-initializer                                  24d
      storage-version-migration-migrator                                     24d
      storage-version-migration-trigger                                      24d
      system:aggregate-to-admin                                              24d
      system:aggregate-to-edit                                               24d
      system:aggregate-to-view                                               24d
      system:auth-delegator                                                  24d
      system:basic-user                                                      24d
      system:certificates.k8s.io:certificatesigningrequests:nodeclient       24d
      system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   24d
      system:clustermetrics                                                  24d
      system:controller:attachdetach-controller                              24d
      system:controller:certificate-controller                               24d
      ...
      ```

    * kubectl describe clusterrole/admin

      ```
      Name:         admin
      Labels:       kubernetes.io/bootstrapping=rbac-defaults
      Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
      PolicyRule:
        Resources                                       Non-Resource URLs  Resource Names  Verbs
        ---------                                       -----------------  --------------  -----
        rolebindings.rbac.authorization.k8s.io          []                 []              [create delete deletecollection get list patch update watch]
        roles.rbac.authorization.k8s.io                 []                 []              [create delete deletecollection get list patch update watch]
        configmaps                                      []                 []              [create delete deletecollection patch update get list watch]
        endpoints                                       []                 []              [create delete deletecollection patch update get list watch]
        persistentvolumeclaims                          []                 []              [create delete deletecollection patch update get list watch]
        pods                                            []                 []              [create delete deletecollection patch update get list watch]
        replicationcontrollers/scale                    []                 []              [create delete deletecollection patch update get list watch]
        replicationcontrollers                          []                 []              [create delete deletecollection patch update get list watch]
        services                                        []                 []              [create delete deletecollection patch update get list watch]
        daemonsets.apps                                 []                 []              [create delete deletecollection patch update get list watch]
        deployments.apps/scale                          []                 []              [create delete deletecollection patch update get list watch]
        deployments.apps                                []                 []              [create delete deletecollection patch update get list watch]
        replicasets.apps/scale                          []                 []              [create delete deletecollection patch update get list watch]
        replicasets.apps                                []                 []              [create delete deletecollection patch update get list watch]
      ```

      



* Cluster-Admin 접근 제어
  * 유닉스 시스템의 root와 같은 역할
  * cluster-admin 롤을 할당하여 모든 문제를 해결하려 하지 말것
    * 클러스터 보안 검사를 모두 생략, 클러스터를 외부 공격에 노출시키는 위험을 감수하는 방법



* 애플리케이션 배포

  * 실행 중인 애플리케이션은 대체로 RBAC 권한이 필요하지 않다.
  * 특별한 권한 설정을 하지 않을 시
    * => 모든 Pod는 실행되는 네임스페이스 내에서, 롤이 할당되지 않은 default 서비스 계정으로 실행
  * 활동에 따라 부여할 권한이 달라진다.
    * 예 : 애플리케이션을 배포할 때?
      * 배포 도구에 배포 권한만을 부여
      * 예 : edit 롤
        * 리소스 생성 및 제거가 가능
        * 새로운 롤 생성, 권한 부여는 불가능

  



* RBAC 문제 해결
  * 권한이 없는 작업에 대해 API 요청 시
    * HTTP status 403 응답
    * RBAC DENY 응답
    * API 서버의 log를 확인하여 알아본다.



### 11-2. 보안 스캐닝

​	

* 클러스터에서 서드 파티 소프트웨어를 실행할 때,
  * 보안 문제, 악성 소프트웨어 검사



* 클레어
  * CoreOS 프로젝트에서 개발한 오픈소스 컨테이너 스캐너
  * 컨테이너 실행 전, 컨테이너 이미지를 정적으로 분석
  * 컨테이너 레지스트리에 연결, 푸시된 이미지도 스캔 가능
  * https://github.com/coreos/clair



* 아쿠아
  * 컨테이너 보안 플랫폼
  * 여러 오케스트레이션 시스템과 통합 가능
  * 마이크로 스캐너 제공
    * 컨테이너 이미지에 추가한 후 빌드, 보안 취약점 검색
      * 취약점은 JSON 형태로 제공
  * kube-hunter 제공
    * 클러스터 외부에서 컨테이너를 실행
    * 클러스터 자체의 보안 이슈를 검색하기 위하여 설계



* 앵커 엔진
  * 컨테이너 이미지를 스캔하는 오픈소ㅅ스 도구
    * 컨테이너에 존재하는 모든 것을 검사
      * 보안 취약점
      * 라이브러리
      * 구성 파일
      * 파일 권한..
  * 직접 빌드한 경우에도 검사하는 것을 권장





### 11-3. 백업

​	

* 모든 쿠버네티스 리소스는 etcd 데이터 베이스에 저장
  * 일부러 Pod가 삭제되어도 디플로이먼트에 따라 재생성이 가능



* 쿠버네티스에도 백업이 필요한가?

  * 예.

  * 예시 : 

    * 퍼시스턴트 볼륨에 저장된 데이터는 장애에 취약하다.
    * 고가용성은 백업이 아니다.
    * 데이터 복제도 백업이 될 수 없다.
      * 애플리케이션의 오작동, 운영자의 실수가 데이터를 덮어쓰는 것을 막지 못한다.

    

* etcd 백업하기
  * etcd의 손실을 막는 방법
    * 1. 관리형 서비스로 알아서 백업이 되게끔 한다.
    * 2. 정기적으로 데이터 스냅샷을 저장한다.



* 리소스 상태 백업하기
  * 개별 리소스의 상태를 저장하는 방법?
    * 쿠버네티스 리소스를 YAML 매니페스트 파일로 관리한다.
    * 이후 소스 제어 저장소에 저장하여, 리소스 손실이 발생했을 때 재생성



* 클러스터 상태 백업하기
  * 소스 코드에 저장된 값 그대로 클러스터가 실행 중인 경우는 없다.
    * 실행하면서 명령형 커맨드를 이용했을 수도 있고,
    * 소스로는 남아있지만 더이상 서비스하지 않는 리소스도 있을 수 있다.
  * 그렇다면 클러스터 삭제 후, 재생성을 하려 할 때 무엇을 참조해야하는가?
    * 실행 중인 클러스터의 스냅샷을 생성하여 후에 비교하며 참조



* 크고 작은 장애
  * 대체적으로 클러스터의 구성요소에 장애가 생기게 되는 원인
    * 사용자의 실수
    * => 백업 도구를 사용하여 장애 상황에 항상 대비해두는 것이 좋다.



* **Velero**
  * 무료 오픈 소스 도구
  * 클러스터의 상태와 퍼시스턴트 데이터를 백업, 복구
  * https://velero.io



* Velero 설정하기

  * BackupStorageLocation 객체를 사용

    * 백업을 저장할 공간을 지정

    * 매니페스트 작성 예시

      ```yaml
      apiVersion: velero.io/v1
      kind: BackupStorageLocation
      metadata:
      	name: default
      	namespace: velero
      spec:
      	provider: aws
      	objectStorage:
      		bucket: demo-backup
      	config:
      		regin: asia-central-c
      ```

  * VolumeSnapshotLocation

    * persistent 볼륨 데이터의 백업도 지원한다.

      ```yaml
      apiVersion: velero.io/v1
      kind: VolumeSnapshotLocation
      metadata:
      	name: aws-default
      	namespace: velero
      spec:
      	provider: aws
      	config;
      		region: asia-central-c
      ```



* Velero 백업 생성하기

  * `velero backup`

    * velero 백업 생성 명령어
    * Velero 서버가 쿠버네티스 API에 지정한 셀렉터에 해당하는 리소스를 조회, 백업 진행

  * 백업된 모든 리소스

    * BackupStorageLocation에 설정된 클라우드 스토리지에 저장
    * kube-system과 같은 특정 네임스페이스를 제외한 클러스터 내 모든 리소스 백업 가능
    * 자동 백업 예약 가능

  * 백업 방법

    * 전체 백업

      * 증분 백업이 아니므로, 가장 최근의 백업파일만 있다면 클러스터 복구 가능

        > 증분 백업?
        >
        > * incremental backup
        > * 일자별로 변경사항만 백업하는 것



* 데이터 복구하기
  * `velero backup get`
    *  백업된 목록 확인 명령어
  * `velero backup download`
    * 특정 백업을 다운로드
    * 파일 다운로드 형식
      * tar.gz 아카이브 파일
      * 압축 해제 후, 개별 매니페스트 파일을 apply 명령어로 복구 가능
  * `velero restore`
    *  백업된 전체 데이터를 복구
    * 지정한 스냅샷을 기준으로, 기존에 존재하는 리소스와 볼륨을 제외한 모든 리소스와 볼륨을 재생성
      * 만약 존재하는 리소스 중, 스냅샷과 다른 부분이 있다면 덮어쓰지 않고 사용자에게 알림



* 복구 과정과 테스트
  * 장애는 언제 발생할지 모르므로, 관리자 외의 사람도 장애 발생 시 복구를 진행할 수 있어야 한다.
  * 단계별 절차서를 작성 및 공유
  * 복구 과정을 함께 테스트



* velero 백업 예약하기
  * `velero schedule create`
    * 크론잡과 같이 veleor 백업을 예약하여 사용할 수 있다.
    * 예시 : 
      * `velero schedule create demo-schedule --schedule="0 1 * * *" --inclued-namespaces demo`
  * `velero schedule get`
    * 예약된 백업 확인
    * 백업 표시 항목 중
      * `BACKUP TTL`
        * 백업을 유지할 기간



* Velero의 다른 활용법
  * 주 용도
    * 대형 장애 복구
  * 다른 용도
    * 리프트 앤 시프트
      * 클러스터 간 리소스와 데이터 이전
  * 모니터링 용도
    * 정기적으로 백업 시, 시간에 따른 클러스터 변경 사항 확인 가능
    * 1개월, 6개월, 1년 전의 상태와 비교





### 11-4. 클러스터 상태 모니터링



* 쿠버네티스 클러스터의 자체 모니터링 기능
  * 클러스터 상태
  * 개별 노드의 상태
  * 클러스터와 워크로드 프로세스의 사용률



* kubectl
  * 클러스터 컴포넌트 상태에 대한 유용한 정보 
  * 컨트롤 플레인 상태
    * `kubectl get componetstatuses`
    * 컴포넌트의 health 정보 출력
      * 스케줄러, 컨트롤러 매니저, etcd ..
  * 노드 상태
    * `kubectl get nodes`
    * 관리형 쿠버네티스 엔진에서는 마스터 노드에 접근이 불가능하므로, 워커 노드의 상태만 파악할 수 있다.
      * 예 : GKE
  * 워크로드
    * `kubectl get pods --all-namespace`
      * 클러스터의 전체 Pod 조회





* CPU와 메모리 사용률
  * `kubectl top <리소스 종류>`
    * 각 노드의 CPU와 메모리 용량, 현재 사용량을 확인 가능
    * 예 : `kubectl top nodes`, `kubectl top pods`



* 클라우드 서비스 업체 콘솔
  * GKE와 같은 관리형 쿠버네티스 서비스 사용 시
    * 웹 기반 콘솔에 접속 가능



* 쿠버네티스 대시보드
  * 쿠버네티스 클러스터 관리를 위한 웹 기반의 사용자 인터페이스
  * kubectl로 확인할 수 있는 리소스 상태를 그래픽 인터페이스로 확인 가능
  * 리소스 생성 및 제거 가능
  * ConfigMap과 Secret 정보 조회가 가능하므로, 엄격한 관리 필요



* 위브 스코프
  * 클러스터 모니터링 도구
  * 노드, 컨테이너, 프로세스의 상태를 실시간 맵으로 시각화
  * 메트릭과 메타데이터 확인 가능
  * 컨테이너 시작, 중지 가능



* kube-ops-view
  * 모니터링 보조 툴
  * 클러스터의 상태를 시각화
    * 노드 현황
    * 각 노드의 CPU 및 메모리 사용률
    * 노드별 실행중인 Pod 갯수
    * 각 Pod의 상태



* node-problem-detector
  * 쿠버네티스 애드온
  * 노드 수준의 이슈를 검출하여 알림
    * 예 : 
      * CPU와 메모리 고장
      * 파일 시스템 손상
      * 하드웨어, 컨테이너 런타임 문제





## 12. 쿠버네티스 애플리케이션 배포



### 12-1. 헬름으로 매니페스트 빌드하기

* 매니페스트 파일을 이용하여 클러스터를 빌드한다면?
  * 애플리케이션 사용자의 조정이 필요한 특정 설정과 변수를 분리
  * 누구나 다운로드 하여 클러스터에 설치할 수 있게 된다!



* 헬름 차트의 구성 요소

  * 모든 헬름 차트는 표준 구조를 가진다.

    * 구조의 예시

      ```
      demo
      |-- Chart.yaml
      |-- production-values.yaml
      |-- staging-values.yaml
      |-- templates
      |   |-- deployment.yaml
      |   |-- service.yaml
      |-- values.yaml
      ```

  * **Chart.yaml**

    * 차트의 이름과 버전을 지정

    * 매니페스트 작성 예제

      ```yaml
      name: demo
      sources:
      	- https://github.com/cloudnativedevops/demo
      version: 1.0.1
      ```

      * 작성 시 필수 항목
        * 이름과 버전
      * 추가 항목
        * 프로젝트 소스 코드 링크

  * **values.yaml**

    * 사용자가 수정 가능한 설정을 포함

    * 자유 형식의 YAML 파일

      * 사전에 정의된 스키마가 없다.
      * 사용자가 사용할 변수, 이름, 값을 지정
      * 작성된 변수와 값은 모든 차트에서 참조할 수 있는 값이 된다.

    * 매니페스트 작성 예제

      ```yaml
      environment: development
      container:
      	name: demo
      	port: 8888
      	image: cloudnatived/demo
      	tag: hello
      replicas: 1
      ```

      

* 헬름 템플릿

  * vaules.yaml에서 변수를 참조하는 곳

    * 변수를 참조할 수 있도록 플레이스 홀더를 포함

  * deployment 템플릿 예제

    ```yaml
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
    	name: {{ .Values.container.name }}-{{ .Values.environment }}
    spec:
    	replicas: {{ .Values.replicas }}
    	selector:
    		matchLabels:
    			app: {{ .Values.container.name }}
    	template:
    		metadata:
    			labels:
    				app: {{ .Values.container.name }}
    				environment: {{ .Values.environment }}
    		spec:
    			containers:
    			-	name: {{ .Values.container.name }}
    				image: {{ .Values.container.image }}:{{ .Values.container.tag }}
    				ports:
    					- containerPort: {{ .Values.container,port }}
    				env:
    					- name: ENVIRONMENT
    					  value: {{ .Values.environment }}
    ```

    * `{}` 중괄호 : 헬름이 대체할 변수의 값을 나타냄
      * Go 템플릿 문법의 일부분



* 변수 삽입하기

  * 중괄호를 포함하는 모든 텍스트는 values.yaml 파일에서 설정한 값으로 교체된다.

  * 같은 값을 여러번 참조할 수 있어서 유용

  * service 템플릿도 마찬가지.

    * 매니페스트 작성 예제

      ```yaml
      apiVersion: v1
      kind: Service
      metadata:
      	name: {{ .Values.container.name }}-service-{{ .Values.environment }}
      	labels:
      		app: {{ .Values.container.name }}
      spec:
      	ports:
      	-	port: {{ .Values.container.port }}
      		protocol: TCP
      		targetPort: {{ .Values.container.port }}
      	selector:
      		app: {{ .Values.container.name }}
      	type: ClusterIP
      ```

      

* 템플릿에서 인용 부호 처리하기
  * `quote` 함수 
    * 템플릿의 인용 부호값을 처리
    * `name: {{ .Values.MyName | quote }}`
    * 인용 부호는 문자열 값에만 붙여야한다.
      * 숫자형 값에 사용하면 안됨
      * 예 : 포트 번호



* 의존성 지정하기

  * requirements.yaml

  * 애플리케이션 차트 간 의존성 문제 해결

  * 매니페스트 작성 예제

    ```yaml
    dependencies:
    	- name: redis
    	  version: 1.2.3
    	- name: nginx
    	  version: 3.2.1
    ```

  * `helm dependency update`

    * 매니페스트에 작성된 의존성을 다운로드





### 12-2. 헬름 차트 배포하기

* 헬름의 가장 중요한 특징
  * 구성 설정을 지정, 변경, 업데이트, 재정의 할 수 있다.



* 변수 설정하기

  * `helm install`

    * 명령줄에 변수파일을 추가 지정하여, values.yaml 파일의 기본값을 덮어쓸 수 있다.

  * **환경 변수 생성하기**

    * 변경하고 싶은 `변수:값`을 포함한 YAML 파일을 생성

      ```yaml
      # staging-values.yaml 파일
      environment: staging
      ```



* 특정 값을 가진 헬름 릴리즈
  * `helm install --values=<새로운 yaml 파일>`
  * `--values` 플래그
    * 새로운 변수와 값을 포함한 파일을 지정
    * 플래그로 지정한 파일은 가본값 파일( values.yaml )과 결합
    * 변경된 변수값만 기본값 파일에 덮어쓰여진다.
    * 헬름 차트 변경 시 권장되는 방법
  * `--setflag`
    * helm  install 명령줄에서 직접 값을 지정
    * => 코드형 인프라 철학에 맞지 않으므로 권장하지 않는다. 
  * `helm inspect values`
    * 차트에서 설정할 수 있는 값의 목록을 확인 가능
    * 예: 
      * `helm inspect values stable/prometheus`



* 헬름으로 애플리케이션 업데이트하기
  * 실행 중인 애플리케이션의 일부 값만 변경을 원한다면?
    * `helm upgrade`
    * 변경사항이 적용된 리소스를 신규로 생성하는 대신, 기존의 리소스에 수정사항을 적용



* 이전 버전으로 롤백하기

  * `helm rollback`
    * 이전 릴리스의 특정 번호를 지정하여 그 릴리스로 롤백
    * `helm history`
      * 릴리스 번호 확인 명령어
  * 헬름-모니터로 자동 롤백하기
    * `helm-monitor`
      * 5분동안 메트릭을 감시, 문제가 발생하면 릴리스를 롤백
        * 메트릭을 기반으로 자동 롤백 가능
        * 원하는 메트릭을 프로메테우스 서버에 정의할 수 있다.
      * 예 :
        * 모니터링 시스템에서 기록한 에러 횟수가 일정 양을 넘을 시
          * 이전 버전의 릴리스로 자동 롤백

  

* 헬름 차트 저장소 생성하기
  * 일반적으로 애플리케이션의 헬름차트는 애플리케이션 자체 저장소에 저장
  * HTTP 통신을 통해 이용가능
  * 저장소 메타데이터를 포함한 index.yaml 파일 생성 방법
    * 모든 차트를 단일 디렉터리에 저장 후,
    * `helm repo index` 명령어 실행
  * 개인 저장소의 차트를 이용하는 방법
    * 헬름 목록에 저장소를 추가
    * 예시 : 
      * `helm repo add myrepo http://myrepo.example.com`
        * 저장소 추가 후,
      * `helm install myrepo/myapp`
        * 차트 설치



* SOPS로 헬름 차트의 시크릿 관리하기

  * 헬름을 사용하여 애플리케이션을 배포할 때, 시크릿 값을 SOPS를 이용해 암호화할 수 있다.

    * 관리가 필요한 시크릿이 두 개 이상 있다면, 모든 시크릿을 하나의 파일에 관리하는 것이 더 간단

  * SOPS를 활용한 예제

    * 디렉토리 tree

      ```
      |-- k8s
      |    |-- demo
      |			|-- Chart.yaml
      |			|-- production-secrets.yaml
      |			|-- production-values.yaml
      |			|-- staging-secrets.yaml
      |			|-- staging-values.yaml
      |			|-- templates
      |			|		|-- deployment.yaml
      |			|		|-- secrets.yaml
      |			|-- values.yaml
      |-- temp.yaml
      ```

      * secrets.yaml

        ```yaml
        apiVersion: v1
        kind: Secret
        metadata:
        	name: {{ .Values.container.name }}-secret
        type: Opaque
        data:
        	{{ $environment := .Values.environment }}
        	app_secrets.yaml: {{ .Files.Get (nospace (cat $environment "-secrets.yaml")) | b64enc }} # 시크릿 데이터의 평문을 base64로 디코딩
        	# => values.yaml 파일에 정의된 environment 값에 따라 
        	# production-secrets.yaml 이나 staging-secrets.yaml에서 시크릿을 읽어오는 
        	# Go 템플릿
        ```

      * SOPS를 활용하는 방식

        1. SOPS를 사용하여 파일을 임시 복호화

        2. 쿠버네티스 클러스터에 변경 사항을 적용

           * 스테이징 시크릿으로 데모 애플리케이션의 스테이징 버전을 배포하는 파이프 라인 명령어

             ```
             sops -d k8s/demo/staging-secrets.yaml > temp-staging-secrets.yaml && 
             # SOPS가 staging-secrets 파일을 복호화하여 temp-staging-secrets에 복호화된 결과를 작성한다.
             
             helm upgrade --install staging-demo --values staging-values.yaml \
             --values temp-staging-secrets.yaml ./k8s/demo && rm temp-staging-secrets.yaml
             # 헬름이 staging-values와 temp-staging-secrets에 저장된 값을 사용하여 demo 차트를 설치한다.
             # temp-staging-secrests 파일을 삭제한다.
             ```



### 12-3. 헬름 파일로 여러 개 차트 관리하기

* 헬름의 한계
  * 한 번에 한 개의 차트만 설치할 수 있다는 것...
  * 관련된 유용한 도구
    * 헬름 파일
    * https://github.com/roboll/helmfile
    * 클러스터에 설치할 모든 애플리케이션 차트를 단일 명령어로 배포가능



* 헬름파일의 구성 요소

  * hemlfile.yaml 파일 예제

    ```yaml
    # 참조할 헬름 차트 저장소를 지정
    repositories:
    	-  name: stable
    	   url: https://kubernetes-charts.storage.googleapis.com/
    		
    releases:
    	- name: demo
    	  namespace: demo
    	  chart: ../hello-helm/k8s/demo
    	  values:
    		  -	"../hello-helm/k8s/demo/production-values.yaml"
    			
    - name: kube-state-metrics
      namespace: kube-state-metrics
      chart: stable/kube-state-metrics
    
    - name: prometheus
      namespace: prometheus
      chart: stable/prometheus
      set:
      	- name: rbac.create
      	  value: true
    ```

    * 클러스터에 배포할 애플리케이션
      * release 필드에 정의
        * name 필드
          * 배포할 헬름 차트
        * namespace 필드
          * 배포할 네임 스페이스
        * chart 필드
          * 차트의 URL이나 파일 경로
        * values 필드
          * 배포에 사용할  values.yaml 파일을 지정
        * set 필드
          * values 파일에 추가할 값을 지정



* 헬름 파일 적용하기
  * `helmfile sync`
    * 헬름 파일을 적용하기 위한 명령어
    * helm 차트에 각각 `helm install && helm upgrade` 명령어를 실행한 것과 동일한 결과
    * 모든 차트는 선언적으로 관리될 수 있다.
      * 매니페스트 소스 코드로 관리
      * 변경 후 적용을 통해서 쉽게 수정, 혹은 롤아웃을 할 수 있다.
    * 헬름파일과 동일한 작업을 수행하는 다른 도구
      * Landscaper
      * Helmsman



### 12-4. 고급 매니페스트 관리 도구

* 쿠버네티스 매니페스트를 더 쉽게 작업하기 위한 도구들



* ksonnet
  * JSON 확장 버전인 Jsonnet을 이용하여 매니페스트를 작성
    * JSON + 변수, 반복문, 산술문, 조건문, 오류 처리 등의 기능을 제공
  * 프로토 타입의 개념 사용
    * 프로토 타입 = 매니페스트 패턴을 찍어낼 수 있는 쿠버네티스 리소스의 사전 조합
    * 예 : deployed-service
      * 특정 컨테이너에 대한 디플로이먼트와 트래픽을 라우팅 하는 서비스를 생성



* kapitan
  * Jsonnet 기반 매니페스트 도구
  * 여러 개의 애플리케이션, 클러스터 간 구성 값을 공유하는 데에 초점
  * **인벤토리** 개념
    * 환경이나 애플리케이션에 따라 다른 값을 연결
      * 매니페스트 패턴을 재사용!
      * 구성값의 계층적 데이터 베이스 



* kustomize
  * 일반 YAML 형식을 이용
  * **오버레이**
    * 다른 환경이나 구성에 대한 매니페스트를 패치
    * 기본 파일 + 오버레이 = 최종 매니페스트



* kompose
  * docker-compose.yml 파일을 쿠버네티스 매니페스트로 변환하는 도구
  * 도커 컴포즈
    * 함께 실행할 컨테이너를 하나의 집합으로 정의하고 배포할 수 있다.
    * 즉, 여러 컨테이너의 구성 정보를 정의



* 앤서블
  * 인프라 자동화 도구
  * 확장 모듈을 사용하여 쿠버네티스 리소스를 관리
    * k8s 모듈
      * 설치, 구성, 리소스 관리
  * Jinja
    * 표준 템플릿 엔진
    * 쿠버네티스 매니페스트 템플릿 생성
    * 계층 시스템을 사용하여 정교한 변수 관리 가능
      * 예 : 애플리케이션의 그룹이나 배포 환경에 대해 공통 값 설정 
  * 쿠버네티스 기반의 인프라와 혼합형 인프라에 모두, 하나의 도구로 관리할 수 있다는 점에서 유용하게 쓰일 수 있다.



* kubeval
  * 매니페스트를 검증하는 도구
    * 예 : 쿠버네티스 각 버전끼리 스키마가 일치하는지 확인
  * kubectl
    * 적용 시점에 매니페스트를 검사하여, 명령어 실행 시 오류를 검출
  * kubeval
    * 적용 전에 매니페스트를 검사
    * 클러스터에 접근이 필요 없다.





## 13. 개발 워크플로



### 13-1. 개발 도구

* Skaffold
  * 구글에서 설계한 오픈 소스 도구
  * 로컬 개발 워크플로 제공
    * 로컬에서 개발한 컨테이너를 자동으로 리빌드
    * 변경 사항을 로컬이나 원격 클러스터에 배포
    * 로컬 디렉터리의 파일 변경사항을 skaffold가 항상 감지한다.
  * skaffold.yaml 파일에 원하는 워크플로를 정의한다.



* Draft
  * Azure에서 관리하는 오픈소스 도구
  * Draft 팩 제공
    * 애플리케이션이 사용하는 개발 언어에 따라 미리 작성된 도커파일과 헬름 차트를 제공
  * `draft init && draft create`
    * 로컬 애플리케이션 디렉터리 내의 파일을 검사,
    * 어떤 개발 언어의 코드인지 판단
    * 해당 언어의 도커파일과 헬름차트 생성
  * `draft up`
    * 생성한 도커 파일을 이용하여 로컬 도커 컨테이너를 빌드
    * 쿠버네티스 클러스터에 배포



* Telepresence
  * 로컬 쿠버네티스 클러스터가 필요한 도구
  * Telepresence의 Pod
    * 실제 클러스터에서 애플리케이션을 대신하여 실행
    * 트래픽을 로컬 머신의 컨테이너로 라우팅
  * 즉, 개발자의 로컬머신을 원격 클러스터 내에 포함 시킨다는 것!
    * => 따라서, 개발자가 개발을 진행하며 애플리케이션 코드를 수정하면, 이러한 업데이트 사항이 실제 클러스터에 반영



* Knative
  * 모든 종류의 워크로드를 쿠버네티스에 배포하기 위한 표준 매커니즘을 제공
    * 컨테이너화된 애플리케이션부터, 서버리스 스타일의 함수까지,,
  * Knative + 이스티오
    * 애플리케이션/함수 배포 플랫폼
    * 빌드 프로세스 설정, 자동화 배포, 표준화된 메시징, 큐잉 시스템을 사용한 이벤트 처리를 지원
      * 큐잉 시스템? : Pub/Sub, KafKa, RabbitMQ



### 13-2. 배포 전략

* 무중단 배포
  * 여러 개의 레플리카 각각을 순서대로 업그레이드하여 서비스 중단이 발생하지 않게 하는 것



* 업데이트 전략

  * RollingUpdate

    * 서비스 무중단을 위해 Pod를 차례대로 업그레이드
    * 쿠버네티스 디플로이먼트에서 기본값

  * Recreate

    * 속도를 위해 모든 Pod를 한 번에 업그레이드

    * 매니페스트에 정의 필요

      ```yaml
      apiVersion: extensions/v1beta1
      kind: Deployment
      spec:
      	replicas: 1
      	strategy:
      		type: Recreate
      ```

      

* 롤링 업데이트
  * 모든 레플리카가 업그레이드 될 때까지, 한 번에 하나의 Pod만 업그레이드
    * v1으로 실행 중인 3 개의 레플리카에 업그레이드를 진행한다면?
      1. v1 Pod 중 하나를 제거
      2. 새로운 v2 Pod를 생성 
      3. 새로운 v2 Pod에는 준비되지 않은 상태를 의미하는 플래그를 붙이고, 트래픽 중단
      4. 남은 v1 Pod 2개는 계속해서 트래픽을 처리한다.
      5. .v2 Pod가 준비되면, 중단되었던 트래픽을 다시 받는다.
      6. 나머지 v1 Pod 2개도 동일한 방식으로 v2 Pod로 교체된다.
  * 장점
    * 무중단 서비스
  * 단점
    * 데이터 베이스 마이그레이션이 포함된 업데이트는 롤링 업데이트가 불가능



* Recreate
  * 실행 중인 모든 레플리카를 한 번에 제거, 새로운 레플리카를 생성
  * 직접적으로 요청을 처리하지 않는 애플리케이션의 경우 사용하기 적절
  * 장점
    * 두 가지 다른 버전의 애플리케이션이 동시 실행되는 상황을 피할 수 있다.



* maxSurge & maxUnavailable

  * maxSurge
    * 최대 초과 Pod 수
      * 예 : 10개의 Pod가 있을 때, maxSurge 값을 30%로 설정한다면 최대 13개의 Pod까지만 한번에 실행이 가능
    * 값이 클수록 롤아웃이 빨라진다
    * 값이 클수록 클러스터 리소스에 추가 부하가 발생한다.
    * 값이 작을수록 추가 부하는 적어지지만, 롤아웃에 시간이 많이 소요된다.

  * maxUnavailable

    * 사용 불가능 상태의 최대 Pod 수
      * 예 ; 10개의 Pod가 있고, maxUnavailable 값을 20%로 설정하면, 사용 가능한 상태의 Pod는 8개 이하로 떨어지지 않는다.
    * 값이 클수록 롤아웃이 빨라진다
    * 값이 클수록 애플리케이션 용량이 일시적으로 줄어든다
    * 값이 작을수록 롤아웃에 시간이 많이 소요된다.

  * 두 개의 필드값은 모두 정수 / 백분률 의 단위로 설정할 수 있다.

  * 매니페스트 작성 예시

    ```yaml
    apiVersion: extensions/v1beta1
    kind: Deployment
    spec:
    	replicas: 10
    	stratgy:
    		type: RollingUpdate
    		rollingUpdate:
    			maxSurge: 20%
    			maxUnavailable: 3
    ```

    

* 블루/그린 배포

  * 완전히 새로운 디플로이먼트를 생성하여, v2 버전을 실행하는 Pod가 v1 디플로이먼트와 함께 실행

  * 장점

    * 기존 버전과 새로운 버전의 애플리케이션이 요청을 동시에 처리하지 않는다.

  * 단점

    * 클러스터는 애플리케이션에서 요구하는 레플리카 수의 두 배를 실행할만큼의 용량이 항상 있어야 한다.
    * 업데이트를 할 때를 제외하곤, 사용하지 않는 용량이 대부분 유후 자원으로 존재한다.

  * 구현 방식

    * 레이블을 이용

    * 기존과 새로운 Pod 각각에게 서로 다른 레이블을 설정

    * 다음과 같이 서비스 매니페스트를 수정하여 레이블이 `deployment: blue` 인 Pod에만 트래픽을 전달하도록 한다.

      ```yaml
      apiVersion: v1
      kind: Service
      metadata:
      	name: demo
      spec:
      	ports:
      	-	port: 8080
      		protocol: TCP
      		targetPort: 8080
      	selector:
      		app: demo
      		development: blue # 새로운 버전은 deployment: green
      	type: ClusterIP
      ```

      * 다른 레이블이 붙었으므로, 트래픽은 기존 버전에만 라우팅된다.
      * 이후, green v2 버전이 완료되면, 서비스의 매니페스트만 변경하여 적용하면 된다.



* 레인보우 배포
  * Pod에 웹소켓과 같은 장시간 지속되는 연결이 있을 경우, 블루/그린 배포가 부적합
    * => 이런 경우, 동시에 3개 이상의 애플리케이션 버전을 유지해야 한다.
  * 업데이트를 배포할 때마다 새로운 색상의  Pod 집합이 생성
  * 가장 오래된 세트의 Pod에서 연결이 종료되면 Pod를 종료할 수 있다.



* 카나리아 배포
  * 블루/그린 배포의 단점
    * 업데이트를 위한 용량이 너무 많이 필요..
  * 블루/그린 배포의 단점을 보완!
    * 새로운 버전의 Pod를 소규모로 상용환경에 배포
    * 이후 문제가 없다면 롤아웃을 진행
    * 문제가 발생하더라도 소수의 Pod만 영향을 받는다는 장점이 있다.
  * 블루/그린 배포와 같이 레이블을 활용한다.
  * 이스티오와 함께 사용할 수 있다.
    * A/B 테스트
      * 두 개의 A와 B 버전을 비교하여 어떤 버전이 효과적인지 판단하는 방법



### 13-3. 헬름으로 마이그레이션 처리하기

* 데이터 베이스가 관련되면, 롤아웃 중간 과정에서 마이그레이션 작업이 필요
  * 작업 수행 방법
    * 쿠버네티스 Job 리소스를 이용
    * kubectl과 스크립트를 이용
    * 헬름의 내장 기능 hook을 사용



* 헬름 훅

  * 배포 과정에서 발생하는 작업의 순서를 제어할 수 있다.

  * 문제 발생 시 작업 중단도 쉽다.

  * 헬름으로 배포한 레일즈 애플리케이션의 데이터베이스를 이전하는 예제

    ```yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
    	name: {{ .Values.appName }}-db-migrate
    	annotations:
    		"helm.sh/hook": pre-upgrade # 업그레이드 수행 전, Job 매니페스트 적용하도록 지시
    		"helm.sh/hook-delete-policy": hook-succeeded # Job이 성공적으로 완료되면, 헬름이 Job을 삭제하도록 함
    spec:
    	activedDeadlineSeconds: 60
    	template:
    			name: {{ .values.appName }}-db-migrate
    		spec:
    			restartPolicy: Never
    			containers:
    			- name: {{ .Values.appName }}-migration-job
    			  image: {{ .values.image.repository }}:{{ .Values.image.tag }}
    			  command:
    			  	- bundle
    			  	- exec
    			  	- rails
    			  	- db:migrate
    ```

    

* 실패된 훅 처리
  * 헬름 훅이 실패하면?
    * Job을 실패 상태에 유지, 디버깅할 수 있도록 한다.
  * `kubectl get pods -a`
    * 업그레이드에 실패한 Pod를 조회, 
    * 로그와 자세한 정보를 알 수 있다.
  * 이슈 해결 후에는 실패한 Job을 삭제 후 재시도
    * `kubectl delete job <job name>`



* 다른 훅
  * pre-install
    * 템플릿이 렌더링된 후, 리소스가 생성되기 전에 실행
  * post-install
    * 모든 리소스가 설치된 후 실행
  * pre-delete
    * 리소스를 삭제하기 전, 삭제 요청 시점에 실행
  * post-delete
    * 삭제 요청으로 모든 릴리스 리소스를 삭제한 후 실행
  * pre-upgrade
    * 템플릿이 렌더링된 후, 업그레이드 요청에 대해 리소스를 불러오지 않은 시점에 실행
    * 예 : kubectl apply 작동 전
  * post-upgrade
    * 모든 리소스를 업그레이드한 후 실행
  * pre-rollback
    * 템플릿이 렌더링된 후 롤백 요청에서 리소스가 롤백되기 전에 실행
  * post-rollback
    * 롤백을 실행하고 모든 리소스를 수정한 후 실행



* 훅 체이닝

  * `helm.sh/hook-weight`

    * 특정 순서대로 훅을 함께 묶을 수 있다.
    * 낮은 값 -> 높은 값 순서로 실행
    * 즉, 값이 낮을수록 우선순위가 높다.

  * 매니페스트 작성 예제

    ```yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
    	name: {{ .Values.appName }}-stage-0
    	annotations:
    		"helm.sh/hook": pre-upgrade
    		"helm.sh/hook-delete-policy": hook-succeeded
    		"helm.sh/hook-weight": 0
    ```

    



## 14. 쿠버네티스에서 지속적 배포하기



### 14-1. 지속적 배포란 무엇인가?

* 지속적 배포 ( Continuouds deployment - CD )
  * 완성된 빌드를 상용 환경에 자동으로 배포하는 것
  * 중앙에서 관리, 자동화
  * 지속적 통합 ( Continuouds integration - CI )와 관련있다.
    * 메인 브랜치에 대한 개발자의 변경 사항을 자동으로 통합, 테스트
    * 빌드를 깨뜨릴 수 있는 브랜치를 변경한다면, 개발자에게 알림이 감
  * 지속적 통합과 지속적 배포의 조합 = CI/CD
  * CD는 일종의 파이프 라인
    * 파이프 라인?
      * 일련의 테스트, 승인 단계를 거쳐 개발자의 워크 스테이션에서 상용 환경으로 코들르 가져오는 자동화된 작업
      * 일반적인 컨테이너화 애플리케이션의 파이프 라인
        1. 개발자는 변경한 코드를 깃에 푸시
        2. 빌드 시스템이 변경된 코드를 자동 빌드, 테스트
        3. 모든 빌드와 테스트를 통과하면 컨테이너 이미지를 중앙 레지스트리에 푸시
        4. 새롭게 빌드된 컨테이너가 자동으로 스테이징 환경에 배포
        5. 스테이징 환경에서 테스트 수행
        6. 검증된 이미지가 상용환경에 배포
      * 소스코드가 아닌, 컨테이너가 테스트, 배포된다는 것에 유의
      * 이미 스테이징 환경에서 테스트 한 후 상용에 배포되므로 안정성이 보장된다는 장점이 있다.



### 14-2. 어떤 CD 도구를 사용해야 할까?



* 젠킨스

  * CD 워크플로에서 사용할 수 있는 거의 모든 플러그인을 제공
    * 도커, kubectl, 헬름

  * 젠킨스 X
    * 쿠버네티스에서 젠킨스를 실행하기 위한 사이드 프로젝트



* 드론
  * 컨테이너를 위해 만들어진 CD 도구
  * 단일 YAML파일로 정의된 파이프라인
    * 각 빌드 단계가 컨테이너 실행으로 구성



* 구글 클라우드 빌드
  * YAML 구성 파일로 컨테이너를 실행
  * 개발자가 할 일
    * 클라우드 빌드가 깃 리포지터리를 감시하도록 설정
      * 미리 설정한 조건( 특정 브랜치나 태그를 푸시하는 것.. )을 트리거로 걸어둔다.
      * 트리거가 발생하면, 특정 파이프 라인을 실행하여 새로운 컨테이너를 빌드, 테스트, 게시, 배포



* Concourse
  * Go로 작성된 오픈소스 CD 도구
  * YAML 파일 형태
  * 쿠버네티스에 배포할 수 있는 안정 버전의 헬름 차트 이용
    * 빠르고 쉬운 컨테이너화 파이프라인 구축 가능



* 스피네이커
  * 넷플릭스가 개발
  * 블루/그린 배포와 같은 대규모, 복잡한 배포에 적합
  * 강력하고 유연한 특성



* Gitlab CI
  * 깃허브의 대안으로 인기있는 도구
  * 지속적 배포 파이프라인을 구현



* 코드프레시
  * 쿠버네티스 애플리케이션을 테스트, 배포하기 위한 관리형 CD 서비스
  * 특징
    * 모든 기능 브랜치를 위한 임시 스테이징 환경을 배포할 수 있다
  * 컨테이너를 사용하여 온디맨드 환경을 빌드, 테스트, 배포 가능
  * 클러스터의 다양한 환경에 컨테이너를 배포하는 방법 설정



* 애저 파이프라인
  * 애저 데브옵스 서비스는 지속적 전달 파이프라인 도구인 애저 파이프라인을 제공



### 14-3. CD 컴포넌트



* 도커 허브
  * 코드 수정 시 새로운 컨테이너를 자동으로 빌드할 수 있는 가장 간단한 방법
  * 깃허브나 비트 버킷에 대한 트리거를 생성
  * 새로운 컨테이너를 자동으로 빌드, 도커 허브에 게시



* Gitkubbe
  * 쿠버네티스에서 실행하는 자체 호스팅 도구
  * 깃 저장소를 감시
  * 트리거 실행 시 새로운 컨테이너를 자동 빌드, 푸시



* Flux
  * 깃 옵스 ( GitOps )
    * 깃 브랜치나 태그에서 CD 파이프라인을 트리거하는 패턴
  * Flux는 이러한 GitOps를 확장한 개념
    * 깃 저장소 대신, 컨테이너 레지스트리를 감시



* Keel
  * Flux와 비슷
  * 새로운 컨테이너 이미지를 레지스터에서 클러스터로 배포하는 것에 중점
  * 웹훅에 응답,
  * 슬랙 메세지 통신 가능



### 14-4. 클라우드 빌드를 사용한 CD 파이프라인

* CI/CD를 위해 사용하는 환경
  * GCP, GKE, 구글 클라우드 빌드

* 이후 이어지는 실습은 
  * 쿠버네티스 데브옵스 -실습 에 정리





## 15.   관측 가능성과 모니터링



### 15-1. 관측가능성이란?



* 모니터링
  * 자동화된 모니터링을 의미
    * 프로그래밍 방식
    * 웹사이트, 서비스의 가용성 / 작동 현황을 주기적으로 확인 한다.
      * 문제가 생길 시 엔지니어에게 자동으로 알림



* 블랙박스 모니터링
  * 시스템 내부의 상황을 관찰하지 않고, 시스템 외부의 작동만 관찰하는 모니터링 기법
  * 웹사이트 모니터링의 가장 간단한 방법
    * HTTP 상태 코드 확인
    * 웹서버 구성에 오류가 있다면?
      * HTTP 상태 코드만 정상이고, 실제로 문제가 생겨있을 수도 있다.
  * 정적 페이지 이상의 정보를 모니터링할 필요가 있다.
  * 블랙박스 모니터링의 한계
    * 예측가능한 장애만 탐지할 수 있다.
    * 외부에 노출되는 시스템의 일부 기능만 검사할 수 있다.
    * 수동적이고 반응적
      * 문제 발생 후에야 문제를 감지할 수 있다.
    * 원인에 대해 알 수 없다.



* 업/다운 테스트
  * 서비스의 작동 상태를 기계적으로 작동한다/작동하지 않는다 로 평가하는 테스트
    * 업 :  서비스가 작동, 실행하는 상태
    * 다운 :  장애가 발생해 작동하지 않는 상태
  * 운영에서 가동 시간이란?
    * 애플리케이션의 탄력성, 가용성을 백분율로 측정한 것
    * 예 : 
      * 99.9%
        * 쓰리 나인
        * 1년에 서비스 다운 타임이 약 9시간
      * 99.99%
        * 포 나인
        * 1년에 서비스 다운 타임이 약 1시간 미만
      * 99.999%
        * 파이브 나인
        * 1년에 서비스 다운 타임 시간이 약 5분
  * 클라우드 네이티브 애플리케이션과 같은 분산 시스템
    * 절대 '업'이 될 수 없다.
    * 분산 시스템은 일부가 늘 저하된 상태로 존재
    * 그레이 실패
      * 관측 시스템에서는 정상
      * 애플리케이션 내부에 이슈가 있는 상태
        * 성능 저하, 랜덤 패킷 손상, 메모리 누수..



* 로깅
  * 로그란?
    * 일련의 기록
    * 일반적으로 작성 시간과 순서를 나타내기 위한 타임스탬프를 포함
    * 예 :  웹서버의 로그 기록 데이터
      * 요청된 URI
      * 클라이언트의 IP 주소
      * 응답 값의 HTTP 상태
  * 로깅의 한계
    * 로깅 여부는 애플리케이션을 작성한 개발자가 결정
      * => 즉, 로그는 개발자가 결정한 범위의, 미리 예측한 문제만 감지할 수 있다.
    * 로그의 양
      * 장애 상황 판단을 위해 기록되는 로그의 양은 너무 방대하다.
      * 그렇다고 로그의 양을 줄이기에는
        * 오류 상황만 로그로 기록하면 정상 상태를 파악하기 어렵다.
  * 로그는 확장하기 어렵다
    * 로그는 트래픽에 따라 확장되지 않는다.
      * 따라서 모든 사용자의 요청을 로그로 남길 경우, 로그 수집기에서 병목 현상 발생
    * 많은 양의 로그를 데이터로 저장한다 하더라도
      * 비용적으로 비효율적일 수 있다.
  * 쿠버네티스에서 로깅은 유용한가?
    * 일반 텍스트보다는 JSON 형식으로
      * 자동으로 구분 분석될 수 있는 구조화된 데이터 형식
    * 메트릭



* 메트릭 소개
  * 메트릭이란?
    * 수치로 표현한 값
  * 애플리케이션에 따른 메트릭
    * 현재 처리 중인 요청 수
    * 분당 처리된 요청 수
    * 요청을 처리할 때 발생한 오류 수
    * 요청을 처리하는 데 걸린 평균 시간
  * 인프라에 대한 메트릭
    * 컨테이너나 개별 프로세스의 CPU 사용률
    * 노드 및 서버의 디스트 I/O
    * 머신, 클러스터, 로드 밸런서의 인바운드, 아웃바운드 네트워크 트래픽
  * 메트릭의 장점
    * 원인을 알 수 있다.
      * 상황에 대한 수치 정보를 제공
        * 예 : 높은 지연 시간으로 사용자가 불편함을 겪음
          * 지연 시간이 높은 시점에, 특정 머신이나 구성 요소의 CPU 사용률과 비례함을 확인
          * 어떤 부분에서 문제가 일어나는지 단서를 제공
      * 다양한 활용 방식
        * 그래프, 통계 작성, 미리 정의된 임곗값에 대한 알림
        * 예 : 일정 기간동안 애플리케이션의 오류 발생률이 10%를 초과할 시, 모니터링 시스템이 알림 생성
    * 문제 예측에 도움이 된다.
      * 문제를 인식하기 전에, 사용자에게 문제가 발생할 수 있음을 알릴 수 있다.
      * 예 : 서버의 디스크 사용률
        * 사용률 한계점에 도달하기 전에, 알림을 생성하여 사용자에게 알림
    * 애플리케이션 내부를 모니터링할 수 있다.
      * 시스템의 실제 작동을 기반으로, 시스템의 내부에 대한 주요 정보를 내보낼 수 있다.
      * 다양한 도구
        * 프로메테우스, statsd, 그라파이트 ..



* 추적

  * 분산 시스템에서 중요한 기술
  * 메트릭과 로그
    * 시스템 각 개별 구성 요소에서 발생하는 상황을 알림
  * 추적
    * 전체 수명 주기 동안 단일 사용자의 요청을 추적
    * 예 : 
      * 지연이 발생하는 사용자의 요청을 처음부터 끝까지 추적

  * 유명한 분산 추적 도구
    * Zipkin, Jaeger, LightStep



* 관측 가능성
  * 시스템을 얼마나 잘 계측하고, 내부 상태를 얼마나 쉽게 찾을 수 있는 지 측정하는 것
  * 모니터링
    * 시스템의 작동 여부 알림
  * 관측가능성
    * 시스템이 작동하지 않는 이유
      * 즉, 시스템의 기능과 작동 방식의 이해가 필요
    * 데이터
      * 생성할 데이터와 수집할 데이터를 알아야 한다.
      * 분석할 수 있어야 함
  * 소프트웨어는 이해하기 어렵다.
    * 머신에 대해서는 많은 데이터 존재
    * 소프트웨어는 데이터가 많지 않으므로 소프트웨어 **자체**를 계측해야 함



### 15-2. 관측가능성 파이프라인



* 관측 가능성
  * 일반적으로 여러 개의 데이터 소스를 다양한 데이터 저장소에 연결
  * 로그 => ELK 서버로 전달
  * 메트릭 => 3 ~ 4개의 관리형 서비스에 전달
  * 모니터링 => 또 다른 서비스로 보고
  * 위와 같은 방식의 한계
    * 확장의 어려움
      * 데이터 소스와 저장소로 인해 많은 트래픽 발생
  * 해결 방안
    * 관측 가능성 파이프라인 이용

* 관측 가능성 파이프라인
  * 장점
    * 데이터를 버퍼에 보관
      * 데이터가 유실되지 않는다.
    * 쉬운 데이터 소비
    * 쉬운 데이터 필터링과 데이터 전달
    * 유연한 데이터 이용이 가능
      * 파이프 라인에 연결만 하면, 새로운 데이터 소스를 추가 가능
  * 사용 요구 사항
    * 표준 메트릭 형식
    * JSON, 직렬화된 데이터 형식을 사용하는 애플리케이션 로깅





### 15-3. 쿠버네티스 모니터링



* 외부 블랙박스 검사
  * 블랙박스 모니터링
    * 애플리케이션 작동 중지 상태를 탐지할 수 없다는 점을 제외하고는, 유용한 정보를 제공하는 모니터링
  * 서비스형 모니터링 ( Maas )
    * 서비스의 문제
      * 외부적인 문제
        * 블랙박스 모니터링으로 응답값 확인,
        * 작동 시 정적 페이지 출력값이 정상 작동 시 출력되는 값과 일치하는지 비교 필요
      * 내부적인 문제
        * 사용자와 인프라 사이 다양한 문제로 발생
          * 네트워크 파티션.. 패킷 손실..
        * 내부는 블랙박스 검사로 만족될 수 없다.
          * 따라서 외부에서 관측할 시, **서비스의 가용성**을 중심으로 모니터링을 진행해야 한다.
    * 관련 도구
      * 업타임 로봇
      * 핑돔
      * 웜리
* 모니터링 인프라를 직접 구축하지 말자
  * 비용, 시간 효율적인 측면에서 고려 후, 다양한 상품들을 알아보자
  * 외부 모니터링 업체를 선택할 경우 고려해야할 사항
    * HTTP/HTTPS 검사
    * TLS 인증서가 유효하지 않거나 만료되었는지 탐지
    * 키워드 일치
    * API로 자동 검사를 생성, 업데이트
    * 이메일, SMS, 웹훅 또는 기타 간단한 방식을 사용한 알림 생성



* 내부 헬스 체크
  * 복원력이 뛰어날수록, 블랙박스 모니터링으로 장애를 감지하기 어렵다
    * => 애플리케이션의 자체 헬스 체크 필요
  * 활성 / 준비성 프로브
    * 애플리케이션 준비 상태에 대한 매커니즘 제공
    * 내부 모니터링을 하기에 좋다.
  * 서비스와 서킷 브레이커
    * 컨테이너 활성 검사 실패 시
      * expotential backoff 방식으로 컨테이너 자동 재시작
        * expotential backoff?
          * 임의 지연 시간을 계산하는 알고리즘
          * 근거리망에서 충돌 발생 시, 재전송을 위한 지연 시간을 계산하는 알고리즘
        * 컨테이너 외의 문제( 예 : 의존성 )와 같은 문제는 파악할 수 없다.
    * 컨테이너 준비성 검사 실패 시
      * 컨테이너는 백엔드 모든 서비스에서 제외
      * 사용자 요청 전달도 중단
      * 의존성 문제를 다루기에 더 좋은 방법.
    * 서킷 브레이커 패턴이란?
      * MSA 구축 시, 서비스 간의 연결 장애로 인핸 지연시간을 줄이기 위한 방법
      * 예 :  넷플릭스 - Hystrix
    * degrade gracefully
      * 서비스에서 부분적인 장애가 발생하더라도, 일부가 서비스가 가능하도록 설정이 필요하다.











